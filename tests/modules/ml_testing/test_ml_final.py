#!/usr/bin/env python3
"""
Final test script for MLModelTester implementation.
Tests both subtask 3.1 and 3.2 functionality.
"""

import asyncio
import sys
import numpy as np
from pathlib import Path

# Add the tests directory to the path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from config.test_config import get_test_config
from modules.ml_testing.ml_model_tester import MLModelTester


async def test_complete_ml_module():
    """Test complete ML module functionality."""
    print("=== Testing Complete ML Module (Subtasks 3.1 + 3.2) ===")

    try:
        config = get_test_config()
        ml_tester = MLModelTester(config.ml_config)

        print(f"‚úì MLModelTester initialized")
        print(f"  Module name: {ml_tester.get_module_name()}")
        print(f"  Accuracy thresholds: {ml_tester.accuracy_thresholds}")

        # Run the complete test suite
        result = await ml_tester.run_tests()

        print(f"\n‚úì Complete ML module execution:")
        print(f"  Module name: {result.module_name}")
        print(f"  Status: {result.status.value}")
        print(f"  Tests run: {result.tests_run}")
        print(f"  Tests passed: {result.tests_passed}")
        print(f"  Tests failed: {result.tests_failed}")
        print(f"  Tests skipped: {result.tests_skipped}")
        print(f"  Execution time: {result.execution_time_seconds:.3f}s")

        # Validate that all expected tests are included
        expected_tests = [
            "threshold_validation",  # Subtask 3.1
            "decimal_precision_validation",  # Subtask 3.1
            "synthetic_data_generation",  # Subtask 3.1
            "prediction_consistency",  # Subtask 3.2
            "prediction_format_validation",  # Subtask 3.2
            "performance_degradation",  # Subtask 3.2
            "test_summary",
        ]

        print(f"\n‚úì Test coverage validation:")
        for test_name in expected_tests:
            if test_name in result.detailed_results:
                print(f"  ‚úì {test_name}")
            else:
                print(f"  ‚ùå {test_name} - MISSING")

        # Validate specific subtask 3.2 functionality
        if "prediction_consistency" in result.detailed_results:
            consistency_data = result.detailed_results["prediction_consistency"]
            print(f"\n‚úì Prediction Consistency Test Details:")
            print(f"  Model: {consistency_data['model_name']}")
            print(f"  Runs: {consistency_data['consistency_runs']}")
            print(f"  Mean accuracy: {consistency_data['mean_accuracy']}")
            print(f"  Is consistent: {consistency_data['is_consistent']}")

        if "prediction_format_validation" in result.detailed_results:
            format_data = result.detailed_results["prediction_format_validation"]
            print(f"\n‚úì Prediction Format Validation Details:")
            print(f"  Model: {format_data['model_name']}")
            print(f"  Format valid: {format_data['format_valid']}")
            print(f"  Validation errors: {len(format_data['validation_errors'])}")

        if "performance_degradation" in result.detailed_results:
            degradation_data = result.detailed_results["performance_degradation"]
            print(f"\n‚úì Performance Degradation Detection Details:")
            print(f"  Model: {degradation_data['model_name']}")
            print(f"  Baseline accuracy: {degradation_data['baseline_accuracy']}")
            print(f"  Current accuracy: {degradation_data['current_accuracy']}")
            print(f"  Degradation detected: {degradation_data['degradation_detected']}")

        # Overall validation
        assert result.module_name == "ml_testing"
        assert result.tests_run >= 6  # Should have at least 6 tests
        assert result.tests_passed >= 5  # Most tests should pass
        assert result.execution_time_seconds > 0

        success_rate = (
            result.tests_passed / result.tests_run if result.tests_run > 0 else 0
        )
        print(f"\n‚úì Overall Results:")
        print(f"  Success rate: {success_rate:.1%}")
        print(
            f"  All expected tests present: {all(test in result.detailed_results for test in expected_tests)}"
        )

        if result.error_messages:
            print(f"  Errors encountered: {len(result.error_messages)}")
            for i, error in enumerate(result.error_messages[:2]):
                print(f"    {i+1}. {error}")

        return True

    except Exception as e:
        print(f"‚ùå Complete ML module test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_individual_components():
    """Test individual components separately."""
    print("\n=== Testing Individual Components ===")

    try:
        config = get_test_config()
        ml_tester = MLModelTester(config.ml_config)

        # Test synthetic data generation
        print("Testing synthetic data generation...")
        synthetic_data = ml_tester.generate_synthetic_test_data(100)
        assert synthetic_data.data_size == 100
        assert synthetic_data.features.shape[0] == 100
        print("‚úì Synthetic data generation works")

        # Test ML metrics calculation
        print("Testing ML metrics calculation...")
        precision, recall, f1, accuracy = (
            ml_tester.calculate_ml_metrics_with_decimal_precision(
                synthetic_data.labels, synthetic_data.predictions
            )
        )
        from decimal import Decimal

        assert isinstance(precision, Decimal)
        assert isinstance(recall, Decimal)
        assert isinstance(f1, Decimal)
        assert isinstance(accuracy, Decimal)
        print("‚úì ML metrics calculation with Decimal precision works")

        # Test prediction format validation
        print("Testing prediction format validation...")
        format_result = ml_tester.validate_prediction_format(
            synthetic_data.predictions, "test_model"
        )
        assert format_result.model_name == "test_model"
        assert isinstance(format_result.format_valid, bool)
        print("‚úì Prediction format validation works")

        print("‚úì All individual components work correctly")
        return True

    except Exception as e:
        print(f"‚ùå Individual components test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_orchestrator_registration():
    """Test registration with TestOrchestrator."""
    print("\n=== Testing TestOrchestrator Registration ===")

    try:
        from orchestrator.test_orchestrator import TestOrchestrator

        config = get_test_config()
        ml_tester = MLModelTester(config.ml_config)
        orchestrator = TestOrchestrator(config)

        # Test registration
        ml_tester.register_with_orchestrator(orchestrator)

        # Verify registration
        registered_modules = orchestrator.get_registered_test_modules()
        assert "ml_testing" in registered_modules

        # Test status
        status = orchestrator.get_test_status()
        assert status["registered_modules"]["ml_testing"] == True

        print("‚úì TestOrchestrator registration works correctly")
        return True

    except Exception as e:
        print(f"‚ùå TestOrchestrator registration test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


async def main():
    """Run all final tests."""
    print("Final Testing of MLModelTester Implementation")
    print("Subtask 3.1: ML Model accuracy validation with Decimal precision")
    print("Subtask 3.2: Prediction consistency and format validation")
    print("=" * 70)

    tests = [
        test_individual_components,
        test_complete_ml_module,
        test_orchestrator_registration,
    ]

    passed = 0
    failed = 0

    for test in tests:
        try:
            if asyncio.iscoroutinefunction(test):
                result = await test()
            else:
                result = test()

            if result:
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"‚ùå Test {test.__name__} failed with exception: {e}")
            failed += 1

    print("\n" + "=" * 70)
    print(f"Final Test Results: {passed} passed, {failed} failed")

    if failed == 0:
        print("üéâ ALL TESTS PASSED! ML Model Testing Module is complete!")
        print("\n‚úÖ Subtask 3.1 - COMPLETED:")
        print("  - MLModelTester class with accuracy validation")
        print("  - Precision, recall, F1-score calculations with Decimal precision")
        print("  - Synthetic test data generation for model validation")
        print("  - Model accuracy threshold validation")
        print("  - Comprehensive error handling and logging")

        print("\n‚úÖ Subtask 3.2 - COMPLETED:")
        print("  - Prediction consistency tests across multiple runs")
        print("  - Statistical validation of prediction consistency")
        print("  - Prediction format validation for expected output structure")
        print("  - Model performance degradation detection with baseline comparison")
        print("  - Integration with TestOrchestrator for proper module registration")

        print("\nüîß Technical Features:")
        print("  - All financial calculations use Decimal type (8-digit precision)")
        print("  - ROUND_HALF_UP rounding for all financial operations")
        print("  - Comprehensive error handling and logging")
        print("  - Async/await pattern for I/O operations")
        print("  - Statistical validation using scipy.stats")
        print("  - Proper integration with test orchestrator")

    else:
        print("‚ùå Some tests failed. Please review the implementation.")

    return failed == 0


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
