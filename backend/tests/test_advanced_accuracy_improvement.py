"""
é«˜åº¦ãªç²¾åº¦æ”¹å–„åŠ¹æœãƒ†ã‚¹ãƒˆ

æ–°ã—ã„é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã«ã‚ˆã‚Šã€
40.55%ã‹ã‚‰60%ä»¥ä¸Šã¸ã®ç²¾åº¦å‘ä¸Šã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import pandas as pd
import numpy as np
import logging
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score
from sklearn.preprocessing import RobustScaler
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.ml.feature_engineering.advanced_features import AdvancedFeatureEngineer
from app.services.ml.ensemble.ensemble_trainer import EnsembleTrainer

logger = logging.getLogger(__name__)


class AdvancedAccuracyImprovementTest:
    """é«˜åº¦ãªç²¾åº¦æ”¹å–„åŠ¹æœãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.feature_engineer = AdvancedFeatureEngineer()
        # EnsembleTrainerã®è¨­å®š
        ensemble_config = {
            "method": "stacking",
            "stacking_params": {
                "base_models": ["lightgbm", "random_forest", "xgboost"],
                "meta_model": "lightgbm",
                "cv_folds": 3
            }
        }
        self.ensemble_trainer = EnsembleTrainer(ensemble_config=ensemble_config)

    def create_enhanced_dataset(self, n_samples=1000):
        """æ‹¡å¼µã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        np.random.seed(42)
        
        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        dates = pd.date_range(start='2023-01-01', periods=n_samples, freq='1h')
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆè¤‡æ•°ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        base_price = 50000
        
        # è¤‡æ•°ã®å¸‚å ´ã‚µã‚¤ã‚¯ãƒ«ã‚’æ¨¡æ“¬
        trend_component = np.sin(np.arange(n_samples) * 2 * np.pi / 168) * 0.05  # é€±æ¬¡ã‚µã‚¤ã‚¯ãƒ«
        volatility_component = np.sin(np.arange(n_samples) * 2 * np.pi / 24) * 0.02  # æ—¥æ¬¡ã‚µã‚¤ã‚¯ãƒ«
        random_walk = np.cumsum(np.random.normal(0, 0.01, n_samples))
        
        price_pattern = trend_component + volatility_component + random_walk
        prices = base_price * np.cumprod(1 + price_pattern)
        
        # OHLCV ãƒ‡ãƒ¼ã‚¿
        data = pd.DataFrame({
            'Open': prices * (1 + np.random.normal(0, 0.001, n_samples)),
            'High': prices * (1 + np.abs(np.random.normal(0, 0.003, n_samples))),
            'Low': prices * (1 - np.abs(np.random.normal(0, 0.003, n_samples))),
            'Close': prices,
            'Volume': np.random.lognormal(10, 0.8, n_samples),
        }, index=dates)
        
        # åŸºæœ¬çš„ãªæŠ€è¡“æŒ‡æ¨™ï¼ˆTALibãŒä½¿ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        data['SMA_10'] = data['Close'].rolling(10).mean()
        data['SMA_20'] = data['Close'].rolling(20).mean()
        data['RSI'] = self._calculate_rsi(data['Close'])
        data['MACD'] = data['Close'].ewm(span=12).mean() - data['Close'].ewm(span=26).mean()
        
        # ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¨å»ºç‰æ®‹é«˜
        fr_data = pd.DataFrame({
            'timestamp': dates[::8],  # 8æ™‚é–“é–“éš”
            'funding_rate': np.random.normal(0.0001, 0.0003, len(dates[::8]))
        })
        
        oi_data = pd.DataFrame({
            'timestamp': dates,
            'open_interest': np.random.lognormal(15, 0.3, n_samples)
        })
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆã‚ˆã‚Šäºˆæ¸¬å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ï¼‰
        future_returns = data['Close'].pct_change(12).shift(-12)
        
        # å‹•çš„é–¾å€¤
        rolling_vol = data['Close'].pct_change().rolling(24).std()
        dynamic_threshold = rolling_vol * 0.8
        
        # 3ã‚¯ãƒ©ã‚¹åˆ†é¡
        y = pd.Series(1, index=dates)  # Hold
        y[future_returns > dynamic_threshold] = 2   # Up
        y[future_returns < -dynamic_threshold] = 0  # Down
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        valid_mask = data.notna().all(axis=1) & y.notna() & rolling_vol.notna()
        data = data[valid_mask]
        y = y[valid_mask]
        
        return data, fr_data, oi_data, y

    def _calculate_rsi(self, prices, window=14):
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.fillna(50)

    def test_baseline_performance(self, X, y):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ï¼ˆæ”¹å–„å‰ï¼‰"""
        logger.info("ğŸ”´ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ãƒ†ã‚¹ãƒˆï¼ˆåŸºæœ¬ç‰¹å¾´é‡ + RandomForestï¼‰")
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        split_point = int(len(X) * 0.7)
        X_train = X.iloc[:split_point]
        X_test = X.iloc[split_point:]
        y_train = y.iloc[:split_point]
        y_test = y.iloc[split_point:]
        
        # åŸºæœ¬çš„ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )
        
        # RandomForest
        from sklearn.ensemble import RandomForestClassifier
        model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            class_weight='balanced'
        )
        model.fit(X_train_scaled, y_train)
        
        y_pred = model.predict(X_test_scaled)
        
        results = {
            'method': 'ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆåŸºæœ¬ç‰¹å¾´é‡ + RandomForestï¼‰',
            'accuracy': accuracy_score(y_test, y_pred),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'features': X.shape[1]
        }
        
        logger.info(f"  ç²¾åº¦: {results['accuracy']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {results['balanced_accuracy']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.4f}")
        logger.info(f"  ç‰¹å¾´é‡æ•°: {results['features']}")
        
        return results

    def test_advanced_features_performance(self, ohlcv_data, fr_data, oi_data, y):
        """é«˜åº¦ãªç‰¹å¾´é‡ã§ã®æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸŸ¡ é«˜åº¦ãªç‰¹å¾´é‡æ€§èƒ½ãƒ†ã‚¹ãƒˆï¼ˆé«˜åº¦ç‰¹å¾´é‡ + RandomForestï¼‰")
        
        # é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ
        advanced_features = self.feature_engineer.create_advanced_features(
            ohlcv_data, fr_data, oi_data
        )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        advanced_features = self.feature_engineer.clean_features(advanced_features)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã‚‹
        common_index = advanced_features.index.intersection(y.index)
        X = advanced_features.loc[common_index]
        y_aligned = y.loc[common_index]
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        split_point = int(len(X) * 0.7)
        X_train = X.iloc[:split_point]
        X_test = X.iloc[split_point:]
        y_train = y_aligned.iloc[:split_point]
        y_test = y_aligned.iloc[split_point:]
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆé‡è¦åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        from sklearn.ensemble import RandomForestClassifier
        temp_model = RandomForestClassifier(n_estimators=50, random_state=42)
        temp_model.fit(X_train_scaled, y_train)
        
        feature_importance = pd.Series(
            temp_model.feature_importances_,
            index=X_train_scaled.columns
        ).sort_values(ascending=False)
        
        # ä¸Šä½ç‰¹å¾´é‡ã‚’é¸æŠ
        top_features = feature_importance.head(min(30, len(feature_importance))).index
        X_train_selected = X_train_scaled[top_features]
        X_test_selected = X_test_scaled[top_features]
        
        # RandomForestï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        model = RandomForestClassifier(
            n_estimators=200,
            max_depth=12,
            min_samples_split=3,
            class_weight='balanced',
            random_state=42
        )
        model.fit(X_train_selected, y_train)
        
        y_pred = model.predict(X_test_selected)
        
        results = {
            'method': 'é«˜åº¦ç‰¹å¾´é‡ + æ”¹è‰¯RandomForest',
            'accuracy': accuracy_score(y_test, y_pred),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'features': len(top_features),
            'total_features_generated': X.shape[1]
        }
        
        logger.info(f"  ç²¾åº¦: {results['accuracy']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {results['balanced_accuracy']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.4f}")
        logger.info(f"  é¸æŠç‰¹å¾´é‡æ•°: {results['features']}")
        logger.info(f"  ç”Ÿæˆç‰¹å¾´é‡ç·æ•°: {results['total_features_generated']}")
        
        return results, X_train_selected, X_test_selected, y_train, y_test

    def test_ensemble_performance(self, X_train, X_test, y_train, y_test):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã§ã®æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸŸ¢ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’æ€§èƒ½ãƒ†ã‚¹ãƒˆï¼ˆé«˜åº¦ç‰¹å¾´é‡ + ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»è©•ä¾¡
        training_result = self.ensemble_trainer._train_model_impl(
            X_train, X_test, y_train, y_test
        )
        
        # çµæœã‚’æ•´å½¢ï¼ˆEnsembleTrainerã®çµæœã‚’ensemble_resultsã®å½¢å¼ã«å¤‰æ›ï¼‰
        ensemble_results = {
            "stacking_ensemble": {
                "accuracy": training_result.get("accuracy", 0),
                "balanced_accuracy": training_result.get("balanced_accuracy", 0),
                "f1_score": training_result.get("f1_score", 0)
            }
        }
        
        # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š
        best_method = "stacking_ensemble"
        best_model = ensemble_results["stacking_ensemble"]
        
        logger.info(f"ğŸ† æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_method}")
        logger.info(f"  ç²¾åº¦: {best_model['accuracy']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {best_model['balanced_accuracy']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {best_model['f1_score']:.4f}")
        
        return ensemble_results, best_method, best_model

    def run_comprehensive_accuracy_test(self):
        """åŒ…æ‹¬çš„ãªç²¾åº¦æ”¹å–„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        logger.info("=" * 80)
        logger.info("ğŸš€ é«˜åº¦ãªç²¾åº¦æ”¹å–„åŠ¹æœã®åŒ…æ‹¬çš„æ¤œè¨¼")
        logger.info("=" * 80)
        
        # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        ohlcv_data, fr_data, oi_data, y = self.create_enhanced_dataset(n_samples=1200)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(ohlcv_data)}ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        
        # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½
        baseline_results = self.test_baseline_performance(ohlcv_data, y)
        
        # 2. é«˜åº¦ãªç‰¹å¾´é‡ã§ã®æ€§èƒ½
        advanced_results, X_train, X_test, y_train, y_test = self.test_advanced_features_performance(
            ohlcv_data, fr_data, oi_data, y
        )
        
        # 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã§ã®æ€§èƒ½
        ensemble_results, best_method, best_model = self.test_ensemble_performance(
            X_train, X_test, y_train, y_test
        )
        
        # çµæœåˆ†æ
        self._analyze_comprehensive_results(baseline_results, advanced_results, best_model, ensemble_results)
        
        return baseline_results, advanced_results, ensemble_results

    def _analyze_comprehensive_results(self, baseline, advanced, best_ensemble, all_ensemble):
        """åŒ…æ‹¬çš„çµæœåˆ†æ"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š åŒ…æ‹¬çš„ç²¾åº¦æ”¹å–„åŠ¹æœåˆ†æ")
        logger.info("=" * 80)
        
        # æ®µéšçš„æ”¹å–„åŠ¹æœ
        baseline_acc = baseline['balanced_accuracy']
        advanced_acc = advanced['balanced_accuracy']
        ensemble_acc = best_ensemble['balanced_accuracy']
        
        # æ”¹å–„ç‡è¨ˆç®—
        advanced_improvement = (advanced_acc - baseline_acc) / baseline_acc * 100
        ensemble_improvement = (ensemble_acc - baseline_acc) / baseline_acc * 100
        final_improvement = (ensemble_acc - advanced_acc) / advanced_acc * 100
        
        logger.info("ğŸ¯ æ®µéšçš„æ”¹å–„åŠ¹æœ:")
        logger.info(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline_acc:.4f}")
        logger.info(f"  é«˜åº¦ç‰¹å¾´é‡: {advanced_acc:.4f} ({advanced_improvement:+.1f}%)")
        logger.info(f"  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: {ensemble_acc:.4f} ({ensemble_improvement:+.1f}%)")
        
        logger.info(f"\nğŸ“ˆ æ”¹å–„æ®µéš:")
        logger.info(f"  ç‰¹å¾´é‡æ”¹å–„åŠ¹æœ: {advanced_improvement:+.1f}%")
        logger.info(f"  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœ: {final_improvement:+.1f}%")
        logger.info(f"  ç·åˆæ”¹å–„åŠ¹æœ: {ensemble_improvement:+.1f}%")
        
        # ç‰¹å¾´é‡åŠ¹ç‡æ€§
        baseline_efficiency = baseline['accuracy'] / baseline['features']
        advanced_efficiency = advanced['accuracy'] / advanced['features']
        
        logger.info(f"\nğŸ”§ ç‰¹å¾´é‡åŠ¹ç‡æ€§:")
        logger.info(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline_efficiency:.6f} ({baseline['features']}ç‰¹å¾´é‡)")
        logger.info(f"  é«˜åº¦ç‰¹å¾´é‡: {advanced_efficiency:.6f} ({advanced['features']}ç‰¹å¾´é‡)")
        
        # å…¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ
        logger.info(f"\nğŸ¤– ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ:")
        for method, scores in all_ensemble.items():
            logger.info(f"  {method}: {scores['balanced_accuracy']:.4f}")
        logger.info("  æ³¨: EnsembleTrainerã¯ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ã¿ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™")
        
        # ç›®æ¨™é”æˆåº¦
        target_accuracy = 0.55  # 55%ç›®æ¨™
        current_accuracy = ensemble_acc
        
        logger.info(f"\nğŸ¯ ç›®æ¨™é”æˆåº¦:")
        logger.info(f"  ç›®æ¨™ç²¾åº¦: {target_accuracy:.1%}")
        logger.info(f"  ç¾åœ¨ç²¾åº¦: {current_accuracy:.1%}")
        logger.info(f"  é”æˆç‡: {current_accuracy/target_accuracy:.1%}")
        
        if current_accuracy >= target_accuracy:
            logger.info("  ğŸ‰ ç›®æ¨™ç²¾åº¦ã‚’é”æˆï¼")
        elif current_accuracy >= target_accuracy * 0.9:
            logger.info("  âœ… ç›®æ¨™ã«è¿‘ã„ç²¾åº¦ã‚’é”æˆ")
        else:
            logger.info("  âš ï¸ ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦")
        
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ææ¡ˆ
        logger.info(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ææ¡ˆ:")
        if current_accuracy < target_accuracy:
            logger.info("  1. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆãƒã‚¯ãƒ­çµŒæ¸ˆæŒ‡æ¨™ã€ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆï¼‰")
            logger.info("  2. ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ï¼ˆLSTMã€Transformerï¼‰")
            logger.info("  3. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆOptunaï¼‰")
            logger.info("  4. ã‚ˆã‚Šé•·æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’")
        else:
            logger.info("  1. æœ¬ç•ªç’°å¢ƒã§ã®å®Ÿè¨¼å®Ÿé¨“")
            logger.info("  2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰")
            logger.info("  3. ãƒªã‚¹ã‚¯ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ")


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s: %(message)s'
    )
    
    # åŒ…æ‹¬çš„ç²¾åº¦æ”¹å–„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test = AdvancedAccuracyImprovementTest()
    baseline, advanced, ensemble = test.run_comprehensive_accuracy_test()
    
    logger.info("\nğŸ‰ é«˜åº¦ãªç²¾åº¦æ”¹å–„åŠ¹æœæ¤œè¨¼å®Œäº†")
