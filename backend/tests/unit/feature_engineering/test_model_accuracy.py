"""
AutoMLç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¸¬å®šãƒ†ã‚¹ãƒˆ

3ã¤ã®AutoMLãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆTSFreshã€Featuretoolsã€AutoFeatï¼‰ã§ç”Ÿæˆã•ã‚ŒãŸ
ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ã¦å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ç²¾åº¦ã‚’æ¸¬å®šã—ã¾ã™ã€‚
"""

import pytest
import pandas as pd
import numpy as np
import time
import warnings
from typing import Dict, List, Tuple, Any
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression

# LightGBMã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import lightgbm as lgb

    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("LightGBMãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚pip install lightgbm ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")

from app.services.ml.feature_engineering.enhanced_feature_engineering_service import (
    EnhancedFeatureEngineeringService,
)
from app.services.ml.feature_engineering.automl_features.automl_config import (
    AutoMLConfig,
    TSFreshConfig,
    FeaturetoolsConfig,
    AutoFeatConfig,
)


class TestModelAccuracy:
    """AutoMLç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¸¬å®šãƒ†ã‚¹ãƒˆ"""

    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        # å…¨AutoMLãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æœ‰åŠ¹ã«ã—ãŸè¨­å®š
        tsfresh_config = TSFreshConfig(
            enabled=True,
            feature_selection=False,  # ã‚ˆã‚Šå¤šãã®ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
            feature_count_limit=200,
            parallel_jobs=1,
        )

        featuretools_config = FeaturetoolsConfig(
            enabled=True,
            max_depth=2,
            max_features=50,
        )

        autofeat_config = AutoFeatConfig(
            enabled=True,
            max_features=20,
            feateng_steps=2,
            max_gb=1.0,
        )

        self.automl_config = AutoMLConfig(
            tsfresh_config=tsfresh_config,
            featuretools_config=featuretools_config,
            autofeat_config=autofeat_config,
        )

        self.service = EnhancedFeatureEngineeringService(self.automl_config)

    def _generate_realistic_financial_data(self, n_samples: int = 1000) -> pd.DataFrame:
        """ãƒªã‚¢ãƒ«ãªé‡‘èæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        np.random.seed(42)

        # åŸºæœ¬çš„ãªä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ + ãƒˆãƒ¬ãƒ³ãƒ‰ + ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰
        returns = np.random.normal(0.0005, 0.02, n_samples)  # æ—¥æ¬¡ãƒªã‚¿ãƒ¼ãƒ³

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ
        volatility = np.ones(n_samples) * 0.02
        for i in range(1, n_samples):
            volatility[i] = (
                0.9 * volatility[i - 1]
                + 0.1 * abs(returns[i - 1])
                + np.random.normal(0, 0.001)
            )
            volatility[i] = max(volatility[i], 0.001)  # æœ€å°å€¤ã‚’è¨­å®šã—ã¦è² ã®å€¤ã‚’é˜²ã
            returns[i] = np.random.normal(0.0005, volatility[i])

        # ä¾¡æ ¼ç³»åˆ—ã‚’ç”Ÿæˆ
        prices = 100 * np.exp(np.cumsum(returns))

        # OHLCV ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        data = []
        for i in range(n_samples):
            base_price = prices[i]
            daily_volatility = volatility[i] * base_price

            # æ—¥ä¸­ã®ä¾¡æ ¼å¤‰å‹•ã‚’æ¨¡æ“¬
            high = base_price + np.random.exponential(daily_volatility * 0.5)
            low = base_price - np.random.exponential(daily_volatility * 0.5)

            if i == 0:
                open_price = base_price
            else:
                open_price = prices[i - 1] * (1 + np.random.normal(0, 0.001))

            close_price = base_price
            volume = np.random.lognormal(15, 1)  # å‡ºæ¥é«˜

            data.append(
                {
                    "Open": open_price,
                    "High": max(open_price, high, close_price),
                    "Low": min(open_price, low, close_price),
                    "Close": close_price,
                    "Volume": volume,
                }
            )

        df = pd.DataFrame(data)

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ 
        df.index = pd.date_range(start="2020-01-01", periods=n_samples, freq="D")

        return df

    def _create_prediction_targets(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """è¤‡æ•°ã®äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä½œæˆ"""
        targets = {}

        # 1. æ¬¡æ—¥ã®çµ‚å€¤äºˆæ¸¬ï¼ˆå›å¸°ï¼‰
        targets["next_close"] = df["Close"].shift(-1).dropna()

        # 2. æ¬¡æ—¥ã®ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬ï¼ˆå›å¸°ï¼‰
        targets["next_return"] = df["Close"].pct_change().shift(-1).dropna()

        # 3. 5æ—¥å¾Œã®ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬ï¼ˆå›å¸°ï¼‰
        targets["return_5d"] = df["Close"].pct_change(5).shift(-5).dropna()

        # 4. ä¾¡æ ¼æ–¹å‘äºˆæ¸¬ï¼ˆåˆ†é¡ - ä¸Šæ˜‡/ä¸‹é™ï¼‰
        next_return = df["Close"].pct_change().shift(-1)
        targets["direction"] = (next_return > 0).astype(int).dropna()

        return targets

    def _evaluate_model_performance(
        self, X: pd.DataFrame, y: pd.Series, task_type: str = "regression"
    ) -> Dict[str, Any]:
        """è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½ã‚’è©•ä¾¡"""

        # ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã«åˆ†å‰²ï¼ˆæ™‚ç³»åˆ—ã‚’è€ƒæ…®ï¼‰
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        # ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        results = {}

        if task_type == "regression":
            models = {
                "LinearRegression": LinearRegression(),
                "Ridge": Ridge(alpha=1.0),
                "RandomForest": RandomForestRegressor(
                    n_estimators=100, random_state=42
                ),
                "GradientBoosting": GradientBoostingRegressor(
                    n_estimators=100, random_state=42
                ),
            }

            # LightGBMãƒã‚¤ãƒ†ã‚£ãƒ–APIã‚’ä½¿ç”¨ï¼ˆæ—¢å­˜å®Ÿè£…ã«åˆã‚ã›ã‚‹ï¼‰
            if LIGHTGBM_AVAILABLE:
                models["LightGBM_Native"] = "lightgbm_native"  # ç‰¹åˆ¥ãªãƒãƒ¼ã‚«ãƒ¼

            for name, model in models.items():
                start_time = time.time()

                # LightGBMãƒã‚¤ãƒ†ã‚£ãƒ–APIã®ç‰¹åˆ¥å‡¦ç†
                if name == "LightGBM_Native" and model == "lightgbm_native":
                    # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
                    train_data = lgb.Dataset(X_train_scaled, label=y_train)
                    valid_data = lgb.Dataset(
                        X_test_scaled, label=y_test, reference=train_data
                    )

                    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ—¢å­˜å®Ÿè£…ã«åˆã‚ã›ã‚‹ï¼‰
                    params = {
                        "objective": "regression",
                        "metric": "rmse",
                        "boosting_type": "gbdt",
                        "num_leaves": 31,
                        "learning_rate": 0.1,
                        "feature_fraction": 0.9,
                        "bagging_fraction": 0.8,
                        "bagging_freq": 5,
                        "verbose": -1,
                        "random_state": 42,
                    }

                    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                    lgb_model = lgb.train(
                        params,
                        train_data,
                        valid_sets=[train_data, valid_data],
                        valid_names=["train", "valid"],
                        num_boost_round=100,
                        callbacks=[
                            lgb.early_stopping(stopping_rounds=20),
                            lgb.log_evaluation(0),  # ãƒ­ã‚°ã‚’æŠ‘åˆ¶
                        ],
                    )

                    # äºˆæ¸¬
                    y_pred = lgb_model.predict(
                        X_test_scaled, num_iteration=lgb_model.best_iteration
                    )
                else:
                    # é€šå¸¸ã®scikit-learnãƒ¢ãƒ‡ãƒ«
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_test_scaled)

                # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)

                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ™‚ç³»åˆ—åˆ†å‰²ï¼‰
                tscv = TimeSeriesSplit(n_splits=5)
                if name == "LightGBM_Native":
                    # LightGBMãƒã‚¤ãƒ†ã‚£ãƒ–APIã®å ´åˆã¯æ‰‹å‹•ã§ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                    cv_scores = []
                    for train_idx, val_idx in tscv.split(X_train_scaled):
                        X_cv_train, X_cv_val = (
                            X_train_scaled[train_idx],
                            X_train_scaled[val_idx],
                        )
                        y_cv_train, y_cv_val = (
                            y_train.iloc[train_idx],
                            y_train.iloc[val_idx],
                        )

                        cv_train_data = lgb.Dataset(X_cv_train, label=y_cv_train)
                        cv_val_data = lgb.Dataset(
                            X_cv_val, label=y_cv_val, reference=cv_train_data
                        )

                        cv_model = lgb.train(
                            params,
                            cv_train_data,
                            valid_sets=[cv_val_data],
                            num_boost_round=50,  # CVã§ã¯çŸ­ã
                            callbacks=[lgb.log_evaluation(0)],
                        )

                        cv_pred = cv_model.predict(
                            X_cv_val, num_iteration=cv_model.best_iteration
                        )
                        cv_score = r2_score(y_cv_val, cv_pred)
                        cv_scores.append(cv_score)

                    cv_scores = np.array(cv_scores)
                else:
                    cv_scores = cross_val_score(
                        model, X_train_scaled, y_train, cv=tscv, scoring="r2"
                    )

                training_time = time.time() - start_time

                results[name] = {
                    "mse": mse,
                    "mae": mae,
                    "r2": r2,
                    "cv_r2_mean": cv_scores.mean(),
                    "cv_r2_std": cv_scores.std(),
                    "training_time": training_time,
                }

        else:  # classification
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.linear_model import LogisticRegression
            from sklearn.metrics import (
                accuracy_score,
                precision_score,
                recall_score,
                f1_score,
            )

            models = {
                "LogisticRegression": LogisticRegression(random_state=42),
                "RandomForest": RandomForestClassifier(
                    n_estimators=100, random_state=42
                ),
            }

            # LightGBMãƒã‚¤ãƒ†ã‚£ãƒ–APIã‚’ä½¿ç”¨ï¼ˆæ—¢å­˜å®Ÿè£…ã«åˆã‚ã›ã‚‹ï¼‰
            if LIGHTGBM_AVAILABLE:
                models["LightGBM_Native"] = "lightgbm_native"  # ç‰¹åˆ¥ãªãƒãƒ¼ã‚«ãƒ¼

            for name, model in models.items():
                start_time = time.time()

                # LightGBMãƒã‚¤ãƒ†ã‚£ãƒ–APIã®ç‰¹åˆ¥å‡¦ç†
                if name == "LightGBM_Native" and model == "lightgbm_native":
                    # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
                    train_data = lgb.Dataset(X_train_scaled, label=y_train)
                    valid_data = lgb.Dataset(
                        X_test_scaled, label=y_test, reference=train_data
                    )

                    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ—¢å­˜å®Ÿè£…ã«åˆã‚ã›ã‚‹ï¼‰
                    params = {
                        "objective": "binary",
                        "metric": "binary_logloss",
                        "boosting_type": "gbdt",
                        "num_leaves": 31,
                        "learning_rate": 0.1,
                        "feature_fraction": 0.9,
                        "bagging_fraction": 0.8,
                        "bagging_freq": 5,
                        "verbose": -1,
                        "random_state": 42,
                    }

                    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                    lgb_model = lgb.train(
                        params,
                        train_data,
                        valid_sets=[train_data, valid_data],
                        valid_names=["train", "valid"],
                        num_boost_round=100,
                        callbacks=[
                            lgb.early_stopping(stopping_rounds=20),
                            lgb.log_evaluation(0),  # ãƒ­ã‚°ã‚’æŠ‘åˆ¶
                        ],
                    )

                    # äºˆæ¸¬
                    y_pred_proba = lgb_model.predict(
                        X_test_scaled, num_iteration=lgb_model.best_iteration
                    )
                    y_pred = (y_pred_proba > 0.5).astype(int)
                else:
                    # é€šå¸¸ã®scikit-learnãƒ¢ãƒ‡ãƒ«
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_test_scaled)

                # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average="weighted")
                recall = recall_score(y_test, y_pred, average="weighted")
                f1 = f1_score(y_test, y_pred, average="weighted")

                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                tscv = TimeSeriesSplit(n_splits=5)
                if name == "LightGBM_Native":
                    # LightGBMãƒã‚¤ãƒ†ã‚£ãƒ–APIã®å ´åˆã¯æ‰‹å‹•ã§ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                    cv_scores = []
                    for train_idx, val_idx in tscv.split(X_train_scaled):
                        X_cv_train, X_cv_val = (
                            X_train_scaled[train_idx],
                            X_train_scaled[val_idx],
                        )
                        y_cv_train, y_cv_val = (
                            y_train.iloc[train_idx],
                            y_train.iloc[val_idx],
                        )

                        cv_train_data = lgb.Dataset(X_cv_train, label=y_cv_train)
                        cv_val_data = lgb.Dataset(
                            X_cv_val, label=y_cv_val, reference=cv_train_data
                        )

                        cv_model = lgb.train(
                            params,
                            cv_train_data,
                            valid_sets=[cv_val_data],
                            num_boost_round=50,  # CVã§ã¯çŸ­ã
                            callbacks=[lgb.log_evaluation(0)],
                        )

                        cv_pred_proba = cv_model.predict(
                            X_cv_val, num_iteration=cv_model.best_iteration
                        )
                        cv_pred = (cv_pred_proba > 0.5).astype(int)
                        cv_score = accuracy_score(y_cv_val, cv_pred)
                        cv_scores.append(cv_score)

                    cv_scores = np.array(cv_scores)
                else:
                    cv_scores = cross_val_score(
                        model, X_train_scaled, y_train, cv=tscv, scoring="accuracy"
                    )

                training_time = time.time() - start_time

                results[name] = {
                    "accuracy": accuracy,
                    "precision": precision,
                    "recall": recall,
                    "f1": f1,
                    "cv_accuracy_mean": cv_scores.mean(),
                    "cv_accuracy_std": cv_scores.std(),
                    "training_time": training_time,
                }

        return results

    def test_comprehensive_model_accuracy(self):
        """åŒ…æ‹¬çš„ãªãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¸¬å®šãƒ†ã‚¹ãƒˆ"""
        print("\n" + "=" * 80)
        print("ğŸš€ AutoMLç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¸¬å®šãƒ†ã‚¹ãƒˆ")
        print("=" * 80)

        # ãƒªã‚¢ãƒ«ãªé‡‘èãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        print("ğŸ“Š ãƒªã‚¢ãƒ«ãªé‡‘èæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
        financial_data = self._generate_realistic_financial_data(1000)
        print(f"ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: {financial_data.shape}")

        # äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä½œæˆ
        targets = self._create_prediction_targets(financial_data)
        print(f"äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {list(targets.keys())}")

        # AutoMLç‰¹å¾´é‡ã‚’ç”Ÿæˆ
        print("\nğŸ”§ AutoMLç‰¹å¾´é‡ã‚’ç”Ÿæˆä¸­...")
        start_time = time.time()

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")

            # å…¨AutoMLãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´é‡ç”Ÿæˆ
            enhanced_features = self.service.calculate_enhanced_features(
                ohlcv_data=financial_data,
                target=targets[
                    "next_return"
                ],  # ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬ã‚’ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã¦ä½¿ç”¨
                lookback_periods={"short": 10, "medium": 20, "long": 50},
            )

        feature_generation_time = time.time() - start_time

        print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†:")
        print(f"   - ç·ç‰¹å¾´é‡æ•°: {len(enhanced_features.columns)}å€‹")
        print(f"   - ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°: {len(enhanced_features)}è¡Œ")
        print(f"   - ç”Ÿæˆæ™‚é–“: {feature_generation_time:.2f}ç§’")

        # ç‰¹å¾´é‡ã®å†…è¨³ã‚’è¡¨ç¤º
        feature_types = {
            "Manual": len(
                [
                    col
                    for col in enhanced_features.columns
                    if not any(prefix in col for prefix in ["TSF_", "FT_", "AF_"])
                ]
            ),
            "TSFresh": len([col for col in enhanced_features.columns if "TSF_" in col]),
            "Featuretools": len(
                [col for col in enhanced_features.columns if "FT_" in col]
            ),
            "AutoFeat": len([col for col in enhanced_features.columns if "AF_" in col]),
        }

        print(f"   - ç‰¹å¾´é‡å†…è¨³: {feature_types}")

        # å„äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡
        all_results = {}

        for target_name, target_series in targets.items():
            print(f"\nğŸ“ˆ {target_name} äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã®è©•ä¾¡ä¸­...")

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’çµ±ä¸€ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å•é¡Œã‚’å›é¿ï¼‰
            X_reset = enhanced_features.reset_index(drop=True)
            y_reset = target_series.reset_index(drop=True)

            # ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’åˆã‚ã›ã‚‹
            min_length = min(len(X_reset), len(y_reset))
            X = X_reset.iloc[:min_length]
            y = y_reset.iloc[:min_length]

            # ç„¡é™å€¤ãƒ»NaNå€¤ã‚’é™¤å»
            numeric_cols = X.select_dtypes(include=[np.number]).columns
            mask = np.isfinite(X[numeric_cols]).all(axis=1) & np.isfinite(y)
            X_clean = X[mask]
            y_clean = y[mask]

            print(f"   - ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿: {len(X_clean)}è¡Œ")

            if len(X_clean) < 100:
                print(f"   âš ï¸  ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {len(X_clean)}è¡Œ")
                continue

            # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
            task_type = "classification" if target_name == "direction" else "regression"

            # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡
            results = self._evaluate_model_performance(X_clean, y_clean, task_type)
            all_results[target_name] = results

            # çµæœã‚’è¡¨ç¤º
            print(f"   ğŸ“Š {target_name} äºˆæ¸¬çµæœ:")
            for model_name, metrics in results.items():
                if task_type == "regression":
                    print(f"      {model_name}:")
                    print(f"        - RÂ² Score: {metrics['r2']:.4f}")
                    print(f"        - MSE: {metrics['mse']:.6f}")
                    print(f"        - MAE: {metrics['mae']:.6f}")
                    print(
                        f"        - CV RÂ² (å¹³å‡Â±æ¨™æº–åå·®): {metrics['cv_r2_mean']:.4f}Â±{metrics['cv_r2_std']:.4f}"
                    )
                    print(f"        - è¨“ç·´æ™‚é–“: {metrics['training_time']:.2f}ç§’")
                else:
                    print(f"      {model_name}:")
                    print(f"        - Accuracy: {metrics['accuracy']:.4f}")
                    print(f"        - F1 Score: {metrics['f1']:.4f}")
                    print(
                        f"        - CV Accuracy (å¹³å‡Â±æ¨™æº–åå·®): {metrics['cv_accuracy_mean']:.4f}Â±{metrics['cv_accuracy_std']:.4f}"
                    )
                    print(f"        - è¨“ç·´æ™‚é–“: {metrics['training_time']:.2f}ç§’")

        # ç·åˆçµæœã®ã‚µãƒãƒªãƒ¼
        print(f"\n" + "=" * 80)
        print("ğŸ“‹ ç·åˆçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 80)

        print(f"ğŸ”§ ç‰¹å¾´é‡ç”Ÿæˆ:")
        print(f"   - ç·ç‰¹å¾´é‡æ•°: {len(enhanced_features.columns)}å€‹")
        print(f"   - ç”Ÿæˆæ™‚é–“: {feature_generation_time:.2f}ç§’")
        print(f"   - ç‰¹å¾´é‡å†…è¨³: {feature_types}")

        print(f"\nğŸ¯ æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«:")
        for target_name, results in all_results.items():
            if target_name in ["next_return", "return_5d"]:  # å›å¸°ã‚¿ã‚¹ã‚¯
                best_model = max(results.items(), key=lambda x: x[1]["r2"])
                print(
                    f"   {target_name}: {best_model[0]} (RÂ² = {best_model[1]['r2']:.4f})"
                )
            elif target_name == "direction":  # åˆ†é¡ã‚¿ã‚¹ã‚¯
                best_model = max(results.items(), key=lambda x: x[1]["accuracy"])
                print(
                    f"   {target_name}: {best_model[0]} (Accuracy = {best_model[1]['accuracy']:.4f})"
                )

        # ãƒ†ã‚¹ãƒˆæˆåŠŸã®ç¢ºèª
        assert (
            len(enhanced_features.columns) > 100
        ), f"ç‰¹å¾´é‡æ•°ãŒä¸è¶³: {len(enhanced_features.columns)}å€‹"
        assert len(all_results) > 0, "ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœãŒç©ºã§ã™"

        print(f"\nâœ… ãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¸¬å®šãƒ†ã‚¹ãƒˆå®Œäº†!")
        print("=" * 80)

        return all_results
