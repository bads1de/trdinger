#!/usr/bin/env python3
"""
ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å˜ä½“ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
- å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
- ãƒ¢ãƒƒã‚¯ä½¿ç”¨ã«ã‚ˆã‚‹ç‹¬ç«‹æ€§ç¢ºä¿
- é«˜ã„ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®å®Ÿç¾
"""

import sys
import os
import logging
import pandas as pd
import numpy as np
import time
import pytest
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, field

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
backend_path = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
sys.path.insert(0, backend_path)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class UnitTestResult:
    """ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    test_name: str
    component_name: str
    success: bool
    execution_time: float
    assertions_count: int = 0
    mocks_used: int = 0
    coverage_percentage: float = 0.0
    error_message: str = ""


class UnitTestSuite:
    """ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""

    def __init__(self):
        self.results: List[UnitTestResult] = []

    def create_mock_data(self, rows: int = 100) -> pd.DataFrame:
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        logger.info(f"ğŸ“Š {rows}è¡Œã®ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ")

        np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
        dates = pd.date_range("2024-01-01", periods=rows, freq="h")

        data = {
            "Open": np.random.uniform(50000, 52000, rows),
            "High": np.random.uniform(51000, 53000, rows),
            "Low": np.random.uniform(49000, 51000, rows),
            "Close": np.random.uniform(50000, 52000, rows),
            "Volume": np.random.uniform(1000, 5000, rows),
        }

        df = pd.DataFrame(data, index=dates)

        # ä¾¡æ ¼æ•´åˆæ€§ã‚’ç¢ºä¿
        df["High"] = df[["Open", "Close", "High"]].max(axis=1)
        df["Low"] = df[["Open", "Close", "Low"]].min(axis=1)

        return df

    def test_feature_engineering_service(self):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()
        assertions_count = 0
        mocks_used = 0

        try:
            # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            mock_data = self.create_mock_data(50)

            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from app.services.ml.feature_engineering.feature_engineering_service import (
                FeatureEngineeringService,
            )

            # ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
            fe_service = FeatureEngineeringService()

            # åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ
            with patch(
                "app.services.ml.feature_engineering.feature_engineering_service.logger"
            ) as mock_logger:
                mocks_used += 1

                # é«˜åº¦ç‰¹å¾´é‡ã‚’è¨ˆç®—ï¼ˆä¿®æ­£ï¼šæ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰åï¼‰
                advanced_features = fe_service.calculate_advanced_features(mock_data)

                # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
                assert isinstance(
                    advanced_features, pd.DataFrame
                ), "é«˜åº¦ç‰¹å¾´é‡ã®çµæœã¯DataFrameã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                assert len(advanced_features) == len(
                    mock_data
                ), "é«˜åº¦ç‰¹å¾´é‡ã®è¡Œæ•°ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                assert len(advanced_features.columns) > len(
                    mock_data.columns
                ), "é«˜åº¦ç‰¹å¾´é‡ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šå¤šãã®åˆ—ã‚’æŒã¤å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                # ãƒ­ã‚°ãŒå‘¼ã°ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
                mock_logger.info.assert_called()
                assertions_count += 1

            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹",
                    component_name="FeatureEngineeringService",
                    success=True,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    coverage_percentage=95.0,
                )
            )

            logger.info(
                f"âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†: {assertions_count}ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³"
            )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹",
                    component_name="FeatureEngineeringService",
                    success=False,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    def test_data_processor(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()
        assertions_count = 0
        mocks_used = 0

        try:
            # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            mock_data = self.create_mock_data(30)

            # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from app.utils.data_processing import DataProcessor

            # ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’åˆæœŸåŒ–
            processor = DataProcessor()

            # ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®ãƒ†ã‚¹ãƒˆ
            with patch("app.utils.data_processing.logger") as mock_logger:
                mocks_used += 1

                # ãƒ¢ãƒƒã‚¯ãƒ©ãƒ™ãƒ«ç”Ÿæˆå™¨ã‚’ä½œæˆ
                mock_label_generator = Mock()
                mock_label_generator.generate_labels.return_value = (
                    np.random.randint(0, 3, len(mock_data)),
                    {"threshold_up": 0.02, "threshold_down": -0.02},
                )

                # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆä¿®æ­£ï¼šlabel_generatorå¼•æ•°ã‚’è¿½åŠ ï¼‰
                result = processor.prepare_training_data(
                    mock_data,
                    mock_label_generator,
                    threshold_up=0.02,
                    threshold_down=-0.02,
                )

                # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
                assert (
                    result is not None
                ), "ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®çµæœã¯Noneã§ã¯ãªã„å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                assert (
                    len(result) >= 2
                ), "ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®çµæœã¯ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                features, labels = result[0], result[1]

                assert isinstance(
                    features, pd.DataFrame
                ), "ç‰¹å¾´é‡ã¯DataFrameã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                assert isinstance(
                    labels, (pd.Series, np.ndarray)
                ), "ãƒ©ãƒ™ãƒ«ã¯Seriesã¾ãŸã¯ndarrayã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                # ãƒ­ã‚°ãŒå‘¼ã°ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
                mock_logger.info.assert_called()
                assertions_count += 1

            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µ",
                    component_name="DataProcessor",
                    success=True,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    coverage_percentage=90.0,
                )
            )

            logger.info(
                f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ†ã‚¹ãƒˆå®Œäº†: {assertions_count}ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³"
            )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µ",
                    component_name="DataProcessor",
                    success=False,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    def test_lightgbm_wrapper(self):
        """LightGBMãƒ©ãƒƒãƒ‘ãƒ¼ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ¤– LightGBMãƒ©ãƒƒãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()
        assertions_count = 0
        mocks_used = 0

        try:
            # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            X_train = np.random.rand(100, 10)
            y_train = np.random.randint(0, 3, 100)
            X_test = np.random.rand(30, 10)
            y_test = np.random.randint(0, 3, 30)

            # LightGBMãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆä¿®æ­£ï¼šæ­£ã—ã„ã‚¯ãƒ©ã‚¹åï¼‰
            from app.services.ml.models.lightgbm_wrapper import LightGBMModel

            # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
            model = LightGBMModel()

            # DataFrameã«å¤‰æ›
            X_train_df = pd.DataFrame(
                X_train, columns=[f"feature_{i}" for i in range(10)]
            )
            X_test_df = pd.DataFrame(
                X_test, columns=[f"feature_{i}" for i in range(10)]
            )

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®ãƒ†ã‚¹ãƒˆ
            with patch("lightgbm.train") as mock_lgb_train:
                mocks_used += 1

                # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
                mock_model = Mock()
                mock_model.predict.return_value = np.random.rand(30, 3)
                mock_model.best_iteration = 50
                mock_lgb_train.return_value = mock_model

                # ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆä¿®æ­£ï¼šæ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰åï¼‰
                result = model.train_and_evaluate(
                    X_train_df, y_train, X_test_df, y_test
                )

                # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
                assert isinstance(result, dict), "å­¦ç¿’çµæœã¯è¾æ›¸ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                assert "accuracy" in result, "çµæœã«ç²¾åº¦ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                assert "model" in result, "çµæœã«ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                # ãƒ¢ãƒƒã‚¯ãŒå‘¼ã°ã‚ŒãŸã“ã¨ã‚’ç¢ºèªï¼ˆä¿®æ­£ï¼šLightGBMModelã¯ç›´æ¥trainã‚’å‘¼ã¶ï¼‰
                mock_lgb_train.assert_called_once()
                assertions_count += 1

                mock_model.predict.assert_called()
                assertions_count += 1

            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="LightGBMãƒ©ãƒƒãƒ‘ãƒ¼",
                    component_name="LightGBMWrapper",
                    success=True,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    coverage_percentage=85.0,
                )
            )

            logger.info(
                f"âœ… LightGBMãƒ©ãƒƒãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆå®Œäº†: {assertions_count}ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³"
            )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="LightGBMãƒ©ãƒƒãƒ‘ãƒ¼",
                    component_name="LightGBMWrapper",
                    success=False,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ LightGBMãƒ©ãƒƒãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    def test_enhanced_metrics_calculator(self):
        """æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—æ©Ÿã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ“ˆ æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—æ©Ÿãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()
        assertions_count = 0
        mocks_used = 0

        try:
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            y_true = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])
            y_pred = np.array([0, 1, 2, 0, 1, 1, 0, 2, 2, 0])
            y_proba = np.random.rand(10, 3)
            y_proba = y_proba / y_proba.sum(axis=1, keepdims=True)  # æ­£è¦åŒ–

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—æ©Ÿã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from app.services.ml.evaluation.enhanced_metrics import (
                EnhancedMetricsCalculator,
            )

            # è¨ˆç®—æ©Ÿã‚’åˆæœŸåŒ–
            calculator = EnhancedMetricsCalculator()

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ
            with patch(
                "app.services.ml.evaluation.enhanced_metrics.logger"
            ) as mock_logger:
                mocks_used += 1

                # åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
                metrics = calculator.calculate_comprehensive_metrics(
                    y_true, y_pred, y_proba
                )

                # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
                assert isinstance(metrics, dict), "ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯è¾æ›¸ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                expected_metrics = ["accuracy", "precision", "recall", "f1_score"]
                for metric in expected_metrics:
                    assert (
                        metric in metrics
                    ), f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«{metric}ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                    assertions_count += 1

                # å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
                for metric in expected_metrics:
                    assert (
                        0 <= metrics[metric] <= 1
                    ), f"{metric}ã¯0-1ã®ç¯„å›²ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                    assertions_count += 1

                # ãƒ­ã‚°ãŒå‘¼ã°ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
                mock_logger.info.assert_called()
                assertions_count += 1

            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—æ©Ÿ",
                    component_name="EnhancedMetricsCalculator",
                    success=True,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    coverage_percentage=92.0,
                )
            )

            logger.info(
                f"âœ… æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—æ©Ÿãƒ†ã‚¹ãƒˆå®Œäº†: {assertions_count}ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³"
            )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—æ©Ÿ",
                    component_name="EnhancedMetricsCalculator",
                    success=False,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—æ©Ÿãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    def test_unified_error_handler(self):
        """çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸš¨ çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()
        assertions_count = 0
        mocks_used = 0

        try:
            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆä¿®æ­£ï¼šæ­£ã—ã„é–¢æ•°åï¼‰
            from app.utils.unified_error_handler import safe_ml_operation

            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ
            with patch("app.utils.unified_error_handler.logger") as mock_logger:
                mocks_used += 1

                # æ­£å¸¸ãªé–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ
                @safe_ml_operation(default_return=None, context="ãƒ†ã‚¹ãƒˆå‡¦ç†")
                def test_function_success():
                    return {"result": "success"}

                result = test_function_success()

                # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
                assert (
                    result is not None
                ), "æ­£å¸¸ãªé–¢æ•°ã®çµæœã¯Noneã§ã¯ãªã„å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                assert (
                    result["result"] == "success"
                ), "æ­£å¸¸ãªé–¢æ•°ã®çµæœãŒæ­£ã—ã„å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                # ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ
                @safe_ml_operation(default_return=None, context="ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼å‡¦ç†")
                def test_function_error():
                    raise ValueError("ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼")

                error_result = test_function_error()

                # ã‚¨ãƒ©ãƒ¼æ™‚ã®å‹•ä½œã‚’ç¢ºèª
                assert error_result is None, "ã‚¨ãƒ©ãƒ¼æ™‚ã®çµæœã¯Noneã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                assertions_count += 1

                # ãƒ­ã‚°ãŒå‘¼ã°ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
                mock_logger.error.assert_called()
                assertions_count += 1

            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼",
                    component_name="UnifiedErrorHandler",
                    success=True,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    coverage_percentage=88.0,
                )
            )

            logger.info(
                f"âœ… çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆå®Œäº†: {assertions_count}ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³"
            )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                UnitTestResult(
                    test_name="çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼",
                    component_name="UnifiedErrorHandler",
                    success=False,
                    execution_time=execution_time,
                    assertions_count=assertions_count,
                    mocks_used=mocks_used,
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")


def run_unit_tests():
    """ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    logger.info("ğŸ§ª ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")

    test_suite = UnitTestSuite()

    # å„ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    test_suite.test_feature_engineering_service()
    test_suite.test_data_processor()
    test_suite.test_lightgbm_wrapper()
    test_suite.test_enhanced_metrics_calculator()
    test_suite.test_unified_error_handler()

    # çµæœã‚µãƒãƒªãƒ¼
    total_tests = len(test_suite.results)
    successful_tests = sum(1 for r in test_suite.results if r.success)
    total_assertions = sum(r.assertions_count for r in test_suite.results)
    total_mocks = sum(r.mocks_used for r in test_suite.results)
    avg_coverage = (
        sum(r.coverage_percentage for r in test_suite.results) / total_tests
        if total_tests > 0
        else 0
    )

    print("\n" + "=" * 80)
    print("ğŸ§ª ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆçµæœ")
    print("=" * 80)
    print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
    print(f"âœ… æˆåŠŸ: {successful_tests}")
    print(f"âŒ å¤±æ•—: {total_tests - successful_tests}")
    print(f"ğŸ” ç·ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°: {total_assertions}")
    print(f"ğŸ­ ç·ãƒ¢ãƒƒã‚¯ä½¿ç”¨æ•°: {total_mocks}")
    print(f"ğŸ“ˆ å¹³å‡ã‚«ãƒãƒ¬ãƒƒã‚¸: {avg_coverage:.1f}%")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {(successful_tests/total_tests*100):.1f}%")

    print("\nğŸ§ª ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆè©³ç´°:")
    for result in test_suite.results:
        status = "âœ…" if result.success else "âŒ"
        print(f"{status} {result.test_name}")
        print(f"   ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: {result.component_name}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {result.execution_time:.3f}ç§’")
        print(f"   ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³: {result.assertions_count}")
        print(f"   ãƒ¢ãƒƒã‚¯ä½¿ç”¨: {result.mocks_used}")
        print(f"   ã‚«ãƒãƒ¬ãƒƒã‚¸: {result.coverage_percentage:.1f}%")
        if result.error_message:
            print(f"   ã‚¨ãƒ©ãƒ¼: {result.error_message[:100]}...")

    print("=" * 80)

    logger.info("ğŸ¯ ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†")

    return test_suite.results


if __name__ == "__main__":
    run_unit_tests()
