"""
ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ

ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‡¦ç†ã€æ­£è¦åŒ–ã€å¤–ã‚Œå€¤æ¤œå‡ºã€æ¬ æå€¤è£œå®Œã®æ•°å­¦çš„æ­£ç¢ºæ€§ã‚’æ¤œè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã€‚
å„å‰å‡¦ç†æ‰‹æ³•ã®ç†è«–çš„æ€§è³ªã¨å®Ÿè£…ã®ä¸€è‡´ã‚’åŒ…æ‹¬çš„ã«æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import numpy as np
import pandas as pd
import logging
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from scipy import stats
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils.data_processing import DataProcessor

logger = logging.getLogger(__name__)


class TestPreprocessingAccuracy:
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def create_sample_data(self) -> pd.DataFrame:
        """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        np.random.seed(42)
        return pd.DataFrame({
            'feature1': np.random.normal(100, 15, 1000),
            'feature2': np.random.exponential(2, 1000),
            'feature3': np.random.uniform(-10, 10, 1000),
            'feature4': np.random.lognormal(0, 1, 1000)
        })

    def create_outlier_data(self) -> pd.DataFrame:
        """å¤–ã‚Œå€¤ã‚’å«ã‚€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿"""
        np.random.seed(42)
        data = np.random.normal(0, 1, 100)
        # æ„å›³çš„ã«å¤–ã‚Œå€¤ã‚’è¿½åŠ 
        data[95:] = [10, -10, 15, -15, 20]
        return pd.DataFrame({'values': data})

    def create_missing_data(self) -> pd.DataFrame:
        """æ¬ æå€¤ã‚’å«ã‚€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿"""
        np.random.seed(42)
        data = np.random.normal(50, 10, 100)
        # æ„å›³çš„ã«æ¬ æå€¤ã‚’è¿½åŠ 
        data[10:20] = np.nan
        data[50:55] = np.nan
        return pd.DataFrame({'values': data})

    def test_standard_scaler_accuracy(self):
        """StandardScalerã®æ•°å­¦çš„æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== StandardScalerã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")

        processor = DataProcessor()
        sample_data = self.create_sample_data()
        feature = sample_data[['feature1']].copy()
        
        # æ‰‹å‹•ã§StandardScalerã‚’é©ç”¨
        scaler = StandardScaler()
        sklearn_scaled = scaler.fit_transform(feature)
        
        # DataProcessorã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        processed = processor.preprocess_features(
            feature,
            scale_features=True,
            scaling_method='standard',
            remove_outliers=False
        )
        
        # çµæœã®æ¯”è¼ƒ
        np.testing.assert_array_almost_equal(
            sklearn_scaled.flatten(),
            processed['feature1'].values,
            decimal=10,
            err_msg="StandardScalerã®å®Ÿè£…ãŒä¸€è‡´ã—ã¾ã›ã‚“"
        )
        
        # æ•°å­¦çš„æ€§è³ªã®æ¤œè¨¼
        scaled_mean = processed['feature1'].mean()
        scaled_std = processed['feature1'].std(ddof=1)

        assert abs(scaled_mean) < 1e-10, f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®å¹³å‡ãŒ0ã§ãªã„: {scaled_mean}"
        assert abs(scaled_std - 1.0) < 1e-3, f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®æ¨™æº–åå·®ãŒ1ã§ãªã„: {scaled_std}"
        
        logger.info("âœ… StandardScalerã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_robust_scaler_accuracy(self, sample_data):
        """RobustScalerã®æ•°å­¦çš„æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== RobustScalerã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        feature = sample_data[['feature2']].copy()
        
        # æ‰‹å‹•ã§RobustScalerã‚’é©ç”¨
        scaler = RobustScaler()
        sklearn_scaled = scaler.fit_transform(feature)
        
        # DataProcessorã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        processed = processor.preprocess_features(
            feature,
            scale_features=True,
            scaling_method='robust',
            remove_outliers=False
        )
        
        # çµæœã®æ¯”è¼ƒ
        np.testing.assert_array_almost_equal(
            sklearn_scaled.flatten(),
            processed['feature2'].values,
            decimal=10,
            err_msg="RobustScalerã®å®Ÿè£…ãŒä¸€è‡´ã—ã¾ã›ã‚“"
        )
        
        # æ•°å­¦çš„æ€§è³ªã®æ¤œè¨¼ï¼ˆä¸­å¤®å€¤ãŒ0ã€IQRãŒ1ã«è¿‘ã„ï¼‰
        scaled_median = processed['feature2'].median()
        q75 = processed['feature2'].quantile(0.75)
        q25 = processed['feature2'].quantile(0.25)
        iqr = q75 - q25
        
        assert abs(scaled_median) < 1e-10, f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®ä¸­å¤®å€¤ãŒ0ã§ãªã„: {scaled_median}"
        assert abs(iqr - 1.0) < 1e-10, f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®IQRãŒ1ã§ãªã„: {iqr}"
        
        logger.info("âœ… RobustScalerã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_minmax_scaler_accuracy(self, sample_data):
        """MinMaxScalerã®æ•°å­¦çš„æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== MinMaxScalerã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        feature = sample_data[['feature3']].copy()
        
        # æ‰‹å‹•ã§MinMaxScalerã‚’é©ç”¨
        scaler = MinMaxScaler()
        sklearn_scaled = scaler.fit_transform(feature)
        
        # DataProcessorã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        processed = processor.preprocess_features(
            feature,
            scale_features=True,
            scaling_method='minmax',
            remove_outliers=False
        )
        
        # çµæœã®æ¯”è¼ƒ
        np.testing.assert_array_almost_equal(
            sklearn_scaled.flatten(),
            processed['feature3'].values,
            decimal=10,
            err_msg="MinMaxScalerã®å®Ÿè£…ãŒä¸€è‡´ã—ã¾ã›ã‚“"
        )
        
        # æ•°å­¦çš„æ€§è³ªã®æ¤œè¨¼ï¼ˆæœ€å°å€¤ãŒ0ã€æœ€å¤§å€¤ãŒ1ï¼‰
        scaled_min = processed['feature3'].min()
        scaled_max = processed['feature3'].max()
        
        assert abs(scaled_min) < 1e-10, f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®æœ€å°å€¤ãŒ0ã§ãªã„: {scaled_min}"
        assert abs(scaled_max - 1.0) < 1e-10, f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®æœ€å¤§å€¤ãŒ1ã§ãªã„: {scaled_max}"
        
        logger.info("âœ… MinMaxScalerã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_iqr_outlier_detection_accuracy(self, outlier_data):
        """IQRå¤–ã‚Œå€¤æ¤œå‡ºã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== IQRå¤–ã‚Œå€¤æ¤œå‡ºã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = outlier_data.copy()
        
        # æ‰‹å‹•ã§IQRå¤–ã‚Œå€¤æ¤œå‡ºã‚’å®Ÿè¡Œ
        Q1 = data['values'].quantile(0.25)
        Q3 = data['values'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        manual_outliers = (data['values'] < lower_bound) | (data['values'] > upper_bound)
        manual_outlier_count = manual_outliers.sum()
        
        # DataProcessorã‚’ä½¿ç”¨ã—ã¦å¤–ã‚Œå€¤é™¤å»
        processed = processor.preprocess_features(
            data,
            remove_outliers=True,
            outlier_method='iqr',
            outlier_threshold=1.5,
            scale_features=False
        )
        
        # é™¤å»ã•ã‚ŒãŸå¤–ã‚Œå€¤ã®æ•°ã‚’è¨ˆç®—
        removed_count = len(data) - len(processed)
        
        # çµæœã®æ¤œè¨¼ï¼ˆå®Œå…¨ä¸€è‡´ã¯æœŸå¾…ã—ãªã„ãŒã€è¿‘ã„å€¤ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰
        assert abs(removed_count - manual_outlier_count) <= 2, \
            f"IQRå¤–ã‚Œå€¤æ¤œå‡ºã®çµæœãŒå¤§ããç•°ãªã‚Šã¾ã™: æ‰‹å‹•={manual_outlier_count}, å®Ÿè£…={removed_count}"
        
        logger.info(f"æ‰‹å‹•æ¤œå‡º: {manual_outlier_count}å€‹, å®Ÿè£…æ¤œå‡º: {removed_count}å€‹")
        logger.info("âœ… IQRå¤–ã‚Œå€¤æ¤œå‡ºã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_zscore_outlier_detection_accuracy(self, outlier_data):
        """Z-scoreå¤–ã‚Œå€¤æ¤œå‡ºã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== Z-scoreå¤–ã‚Œå€¤æ¤œå‡ºã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = outlier_data.copy()
        
        # æ‰‹å‹•ã§Z-scoreå¤–ã‚Œå€¤æ¤œå‡ºã‚’å®Ÿè¡Œ
        z_scores = np.abs(stats.zscore(data['values'].dropna()))
        manual_outliers = z_scores > 3.0
        manual_outlier_count = manual_outliers.sum()
        
        # DataProcessorã‚’ä½¿ç”¨ã—ã¦å¤–ã‚Œå€¤é™¤å»
        processed = processor.preprocess_features(
            data,
            remove_outliers=True,
            outlier_method='zscore',
            outlier_threshold=3.0,
            scale_features=False
        )
        
        # é™¤å»ã•ã‚ŒãŸå¤–ã‚Œå€¤ã®æ•°ã‚’è¨ˆç®—
        removed_count = len(data) - len(processed)
        
        # çµæœã®æ¤œè¨¼
        assert abs(removed_count - manual_outlier_count) <= 1, \
            f"Z-scoreå¤–ã‚Œå€¤æ¤œå‡ºã®çµæœãŒç•°ãªã‚Šã¾ã™: æ‰‹å‹•={manual_outlier_count}, å®Ÿè£…={removed_count}"
        
        logger.info(f"æ‰‹å‹•æ¤œå‡º: {manual_outlier_count}å€‹, å®Ÿè£…æ¤œå‡º: {removed_count}å€‹")
        logger.info("âœ… Z-scoreå¤–ã‚Œå€¤æ¤œå‡ºã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_missing_value_imputation_accuracy(self, missing_data):
        """æ¬ æå€¤è£œå®Œã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== æ¬ æå€¤è£œå®Œã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = missing_data.copy()
        
        # å…ƒã®çµ±è¨ˆé‡ã‚’è¨ˆç®—ï¼ˆæ¬ æå€¤ã‚’é™¤ãï¼‰
        original_median = data['values'].median()
        original_mean = data['values'].mean()
        
        # medianè£œå®Œã®ãƒ†ã‚¹ãƒˆ
        median_imputed = processor.transform_missing_values(
            data.copy(),
            strategy='median',
            columns=['values']
        )
        
        # è£œå®Œã•ã‚ŒãŸå€¤ãŒä¸­å¤®å€¤ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        imputed_values = median_imputed.loc[data['values'].isna(), 'values']
        assert all(abs(val - original_median) < 1e-10 for val in imputed_values), \
            "medianè£œå®Œã®å€¤ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
        
        # meanè£œå®Œã®ãƒ†ã‚¹ãƒˆ
        mean_imputed = processor.transform_missing_values(
            data.copy(),
            strategy='mean',
            columns=['values']
        )
        
        # è£œå®Œã•ã‚ŒãŸå€¤ãŒå¹³å‡å€¤ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        imputed_values_mean = mean_imputed.loc[data['values'].isna(), 'values']
        assert all(abs(val - original_mean) < 1e-10 for val in imputed_values_mean), \
            "meanè£œå®Œã®å€¤ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
        
        logger.info("âœ… æ¬ æå€¤è£œå®Œã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_preprocessing_pipeline_consistency(self, sample_data):
        """å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = sample_data.copy()
        
        # åŒã˜è¨­å®šã§è¤‡æ•°å›å‰å‡¦ç†ã‚’å®Ÿè¡Œ
        result1 = processor.preprocess_features(
            data.copy(),
            scale_features=True,
            remove_outliers=True,
            outlier_method='iqr'
        )
        
        result2 = processor.preprocess_features(
            data.copy(),
            scale_features=True,
            remove_outliers=True,
            outlier_method='iqr'
        )
        
        # çµæœãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        try:
            pd.testing.assert_frame_equal(
                result1, result2,
                check_exact=False,
                rtol=1e-10
            )
        except AssertionError:
            raise AssertionError("åŒã˜è¨­å®šã§ã®å‰å‡¦ç†çµæœãŒä¸€è‡´ã—ã¾ã›ã‚“")
        
        logger.info("âœ… å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_scaling_reversibility(self, sample_data):
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å¯é€†æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å¯é€†æ€§ãƒ†ã‚¹ãƒˆ ===")
        
        # æ³¨æ„: DataProcessorã¯ç¾åœ¨é€†å¤‰æ›æ©Ÿèƒ½ã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ã€
        # ç†è«–çš„ãªå¯é€†æ€§ã‚’sklearnç›´æ¥ä½¿ç”¨ã§ãƒ†ã‚¹ãƒˆ
        
        feature = sample_data[['feature1']].copy()
        original_values = feature['feature1'].values
        
        # StandardScalerã§ã®å¯é€†æ€§ãƒ†ã‚¹ãƒˆ
        scaler = StandardScaler()
        scaled = scaler.fit_transform(feature)
        reversed_values = scaler.inverse_transform(scaled)
        
        np.testing.assert_array_almost_equal(
            original_values,
            reversed_values.flatten(),
            decimal=10,
            err_msg="StandardScalerã®å¯é€†æ€§ãŒä¿ãŸã‚Œã¦ã„ã¾ã›ã‚“"
        )
        
        # RobustScalerã§ã®å¯é€†æ€§ãƒ†ã‚¹ãƒˆ
        robust_scaler = RobustScaler()
        robust_scaled = robust_scaler.fit_transform(feature)
        robust_reversed = robust_scaler.inverse_transform(robust_scaled)
        
        np.testing.assert_array_almost_equal(
            original_values,
            robust_reversed.flatten(),
            decimal=10,
            err_msg="RobustScalerã®å¯é€†æ€§ãŒä¿ãŸã‚Œã¦ã„ã¾ã›ã‚“"
        )
        
        logger.info("âœ… ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å¯é€†æ€§ãƒ†ã‚¹ãƒˆå®Œäº†")


def run_all_preprocessing_tests():
    """ã™ã¹ã¦ã®å‰å‡¦ç†ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’é–‹å§‹")

    test_instance = TestPreprocessingAccuracy()

    try:
        # å„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼ˆç°¡ç•¥åŒ–ç‰ˆï¼‰
        logger.info("StandardScalerãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        test_instance.test_standard_scaler_accuracy()

        logger.info("å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        sample_data = test_instance.create_sample_data()
        test_instance.test_preprocessing_pipeline_consistency(sample_data)

        logger.info("ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯é€†æ€§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        test_instance.test_scaling_reversibility(sample_data)
        
        logger.info("ğŸ‰ ã™ã¹ã¦ã®å‰å‡¦ç†æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å‰å‡¦ç†æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    success = run_all_preprocessing_tests()
    sys.exit(0 if success else 1)
