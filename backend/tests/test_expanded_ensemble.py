"""
æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆ

11å€‹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ãŸæ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®æ€§èƒ½ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import pandas as pd
import numpy as np
import logging
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score
from sklearn.preprocessing import RobustScaler
import warnings
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.ml.feature_engineering.advanced_features import AdvancedFeatureEngineer
from app.services.ml.models.ensemble_models import EnsembleModelManager

logger = logging.getLogger(__name__)


class ExpandedEnsembleTest:
    """æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.feature_engineer = AdvancedFeatureEngineer()
        self.ensemble_manager = EnsembleModelManager()

    def create_test_dataset(self, n_samples=1000):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        np.random.seed(42)
        
        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        dates = pd.date_range(start='2023-01-01', periods=n_samples, freq='1h')
        
        # ã‚ˆã‚Šè¤‡é›‘ãªä¾¡æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
        base_price = 50000
        
        # è¤‡æ•°ã®å‘¨æœŸçš„ãƒ‘ã‚¿ãƒ¼ãƒ³
        daily_cycle = np.sin(np.arange(n_samples) * 2 * np.pi / 24) * 0.01
        weekly_cycle = np.sin(np.arange(n_samples) * 2 * np.pi / (24 * 7)) * 0.02
        monthly_trend = np.sin(np.arange(n_samples) * 2 * np.pi / (24 * 30)) * 0.03
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ + å‘¨æœŸçš„ãƒ‘ã‚¿ãƒ¼ãƒ³
        random_walk = np.cumsum(np.random.normal(0, 0.01, n_samples))
        price_pattern = daily_cycle + weekly_cycle + monthly_trend + random_walk
        
        # ä¾¡æ ¼ç”Ÿæˆ
        prices = base_price * np.cumprod(1 + price_pattern)
        
        # OHLCV ãƒ‡ãƒ¼ã‚¿
        data = pd.DataFrame({
            'Open': prices * (1 + np.random.normal(0, 0.001, n_samples)),
            'High': prices * (1 + np.abs(np.random.normal(0, 0.003, n_samples))),
            'Low': prices * (1 - np.abs(np.random.normal(0, 0.003, n_samples))),
            'Close': prices,
            'Volume': np.random.lognormal(10, 0.8, n_samples),
        }, index=dates)
        
        # åŸºæœ¬æŠ€è¡“æŒ‡æ¨™
        data['SMA_10'] = data['Close'].rolling(10).mean()
        data['SMA_20'] = data['Close'].rolling(20).mean()
        data['RSI'] = self._calculate_rsi(data['Close'])
        data['MACD'] = data['Close'].ewm(span=12).mean() - data['Close'].ewm(span=26).mean()
        
        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿
        fr_data = pd.DataFrame({
            'timestamp': dates[::8],
            'funding_rate': np.random.normal(0.0001, 0.0003, len(dates[::8]))
        })
        
        oi_data = pd.DataFrame({
            'timestamp': dates,
            'open_interest': np.random.lognormal(15, 0.3, n_samples)
        })
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆã‚ˆã‚Šäºˆæ¸¬å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        future_returns = data['Close'].pct_change(12).shift(-12)
        
        # å‹•çš„é–¾å€¤
        rolling_vol = data['Close'].pct_change().rolling(24).std()
        dynamic_threshold = rolling_vol * 0.7
        
        # 3ã‚¯ãƒ©ã‚¹åˆ†é¡
        y = pd.Series(1, index=dates)  # Hold
        y[future_returns > dynamic_threshold] = 2   # Up
        y[future_returns < -dynamic_threshold] = 0  # Down
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        valid_mask = data.notna().all(axis=1) & y.notna() & rolling_vol.notna()
        data = data[valid_mask]
        y = y[valid_mask]
        
        return data, fr_data, oi_data, y

    def _calculate_rsi(self, prices, window=14):
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.fillna(50)

    def test_expanded_ensemble_performance(self):
        """æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info("=" * 80)
        logger.info("ğŸš€ æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’æ€§èƒ½ãƒ†ã‚¹ãƒˆï¼ˆ11ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰")
        logger.info("=" * 80)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        ohlcv_data, fr_data, oi_data, y = self.create_test_dataset(n_samples=1200)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(ohlcv_data)}ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        
        # é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ
        features = self.feature_engineer.create_advanced_features(
            ohlcv_data, fr_data, oi_data
        )
        features = self.feature_engineer.clean_features(features)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆã‚ã›
        common_index = features.index.intersection(y.index)
        X = features.loc[common_index]
        y_aligned = y.loc[common_index]
        
        logger.info(f"ç‰¹å¾´é‡: {X.shape[1]}å€‹")
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        split_point = int(len(X) * 0.7)
        X_train = X.iloc[:split_point]
        X_test = X.iloc[split_point:]
        y_train = y_aligned.iloc[:split_point]
        y_test = y_aligned.iloc[split_point:]
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )
        
        # ç‰¹å¾´é‡é¸æŠ
        from sklearn.ensemble import RandomForestClassifier
        temp_model = RandomForestClassifier(n_estimators=50, random_state=42)
        temp_model.fit(X_train_scaled, y_train)
        
        feature_importance = pd.Series(
            temp_model.feature_importances_,
            index=X_train_scaled.columns
        ).sort_values(ascending=False)
        
        # ä¸Šä½ç‰¹å¾´é‡ã‚’é¸æŠ
        top_features = feature_importance.head(25).index
        X_train_selected = X_train_scaled[top_features]
        X_test_selected = X_test_scaled[top_features]
        
        logger.info(f"é¸æŠç‰¹å¾´é‡: {len(top_features)}å€‹")
        
        # æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ»è©•ä¾¡
        ensemble_results = self.ensemble_manager.train_and_evaluate_models(
            X_train_selected, X_test_selected, y_train, y_test
        )
        
        # çµæœåˆ†æ
        self._analyze_expanded_results(ensemble_results)
        
        return ensemble_results

    def _analyze_expanded_results(self, results):
        """æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœåˆ†æ"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœåˆ†æï¼ˆ11ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰")
        logger.info("=" * 80)
        
        # çµæœã‚’ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(
            results.items(),
            key=lambda x: x[1]['balanced_accuracy'],
            reverse=True
        )
        
        logger.info("ğŸ† ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
        for i, (method, scores) in enumerate(sorted_results, 1):
            logger.info(f"  {i:2d}. {method:20s}: "
                       f"ç²¾åº¦={scores['accuracy']:.4f}, "
                       f"ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦={scores['balanced_accuracy']:.4f}, "
                       f"F1={scores['f1_score']:.4f}")
        
        # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
        best_method, best_scores = sorted_results[0]
        logger.info(f"\nğŸ¥‡ æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_method}")
        logger.info(f"  ç²¾åº¦: {best_scores['accuracy']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {best_scores['balanced_accuracy']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {best_scores['f1_score']:.4f}")
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        tree_based = ['rf', 'extra_trees', 'xgb', 'lgb', 'gb', 'ada']
        linear_based = ['lr', 'ridge', 'svm']
        other_based = ['nb', 'knn']
        ensemble_based = ['voting_ensemble', 'stacking_ensemble']
        
        logger.info(f"\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥æ€§èƒ½åˆ†æ:")
        
        # ãƒ„ãƒªãƒ¼ç³»
        tree_results = {k: v for k, v in results.items() if k in tree_based}
        if tree_results:
            avg_tree_acc = np.mean([v['balanced_accuracy'] for v in tree_results.values()])
            logger.info(f"  ğŸŒ³ ãƒ„ãƒªãƒ¼ç³»å¹³å‡: {avg_tree_acc:.4f}")
        
        # ç·šå½¢ç³»
        linear_results = {k: v for k, v in results.items() if k in linear_based}
        if linear_results:
            avg_linear_acc = np.mean([v['balanced_accuracy'] for v in linear_results.values()])
            logger.info(f"  ğŸ“ ç·šå½¢ç³»å¹³å‡: {avg_linear_acc:.4f}")
        
        # ãã®ä»–
        other_results = {k: v for k, v in results.items() if k in other_based}
        if other_results:
            avg_other_acc = np.mean([v['balanced_accuracy'] for v in other_results.values()])
            logger.info(f"  ğŸ”® ãã®ä»–å¹³å‡: {avg_other_acc:.4f}")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        ensemble_results = {k: v for k, v in results.items() if k in ensemble_based}
        if ensemble_results:
            avg_ensemble_acc = np.mean([v['balanced_accuracy'] for v in ensemble_results.values()])
            logger.info(f"  ğŸ¤ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¹³å‡: {avg_ensemble_acc:.4f}")
        
        # å¤šæ§˜æ€§åˆ†æ
        all_accuracies = [v['balanced_accuracy'] for v in results.values()]
        accuracy_std = np.std(all_accuracies)
        accuracy_mean = np.mean(all_accuracies)
        
        logger.info(f"\nğŸ“ˆ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å¤šæ§˜æ€§:")
        logger.info(f"  å¹³å‡ç²¾åº¦: {accuracy_mean:.4f}")
        logger.info(f"  æ¨™æº–åå·®: {accuracy_std:.4f}")
        logger.info(f"  å¤‰å‹•ä¿‚æ•°: {accuracy_std/accuracy_mean:.4f}")
        
        if accuracy_std > 0.05:
            logger.info("  âœ… é«˜ã„å¤šæ§˜æ€§ - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœæœŸå¾…å¤§")
        elif accuracy_std > 0.02:
            logger.info("  âš ï¸ ä¸­ç¨‹åº¦ã®å¤šæ§˜æ€§ - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœã‚ã‚Š")
        else:
            logger.info("  âŒ ä½ã„å¤šæ§˜æ€§ - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœé™å®šçš„")
        
        # æ¨å¥¨äº‹é …
        logger.info(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
        if best_method in ensemble_based:
            logger.info("  ğŸ‰ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ãŒæœ€é«˜æ€§èƒ½ã‚’é”æˆï¼")
        else:
            logger.info(f"  ğŸ” {best_method}ãŒæœ€é«˜æ€§èƒ½ - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ã•ã‚‰ãªã‚‹å‘ä¸Šã®ä½™åœ°")
        
        if accuracy_std > 0.03:
            logger.info("  ğŸ“ˆ é«˜ã„å¤šæ§˜æ€§ã‚’æ´»ç”¨ã—ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’æ¨å¥¨")
        
        logger.info("  ğŸš€ ä¸Šä½3-5ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã®è»½é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚‚æ¤œè¨ä¾¡å€¤ã‚ã‚Š")


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s: %(message)s'
    )
    
    # æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test = ExpandedEnsembleTest()
    results = test.test_expanded_ensemble_performance()
    
    logger.info("\nğŸ‰ æ‹¡å¼µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆå®Œäº†")
