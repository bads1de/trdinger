#!/usr/bin/env python3
"""
ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é¢ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
- å…¥åŠ›æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
- ãƒ‡ãƒ¼ã‚¿ä¿è­·ãƒ†ã‚¹ãƒˆ
- ã‚¨ãƒ©ãƒ¼æƒ…å ±æ¼æ´©é˜²æ­¢ãƒ†ã‚¹ãƒˆ
- æ¨©é™åˆ¶å¾¡ãƒ†ã‚¹ãƒˆ
"""

import sys
import os
import logging
import pandas as pd
import numpy as np
import time
import tempfile
import shutil
from datetime import datetime
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, field

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
backend_path = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
sys.path.insert(0, backend_path)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class SecurityTestResult:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    test_name: str
    security_category: str
    success: bool
    execution_time: float
    vulnerability_detected: bool = False
    security_level: str = "UNKNOWN"  # HIGH, MEDIUM, LOW, CRITICAL
    protection_verified: bool = False
    error_message: str = ""
    security_details: Dict[str, Any] = field(default_factory=dict)


class SecurityTestSuite:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""

    def __init__(self):
        self.results: List[SecurityTestResult] = []

    def create_malicious_data(self, attack_type: str) -> pd.DataFrame:
        """æ‚ªæ„ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        logger.info(f"ğŸ”’ {attack_type}æ”»æ’ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ")

        if attack_type == "sql_injection":
            # SQL ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒã‚’æ¨¡æ“¬
            malicious_data = pd.DataFrame(
                {
                    "Open": [50000, 51000, 52000],
                    "High": [51000, 52000, 53000],
                    "Low": [49000, 50000, 51000],
                    "Close": [50500, 51500, 52500],
                    "Volume": [1000, 1100, 1200],
                    "malicious_column": [
                        "'; DROP TABLE users; --",
                        "1' OR '1'='1",
                        "UNION SELECT * FROM sensitive_data",
                    ],
                }
            )

        elif attack_type == "script_injection":
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒã‚’æ¨¡æ“¬
            malicious_data = pd.DataFrame(
                {
                    "Open": [50000, 51000, 52000],
                    "High": [51000, 52000, 53000],
                    "Low": [49000, 50000, 51000],
                    "Close": [50500, 51500, 52500],
                    "Volume": [1000, 1100, 1200],
                    "script_column": [
                        "<script>alert('XSS')</script>",
                        "javascript:void(0)",
                        "eval('malicious_code')",
                    ],
                }
            )

        elif attack_type == "path_traversal":
            # ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒã‚’æ¨¡æ“¬
            malicious_data = pd.DataFrame(
                {
                    "Open": [50000, 51000, 52000],
                    "High": [51000, 52000, 53000],
                    "Low": [49000, 50000, 51000],
                    "Close": [50500, 51500, 52500],
                    "Volume": [1000, 1100, 1200],
                    "path_column": [
                        "../../../etc/passwd",
                        "..\\..\\windows\\system32\\config\\sam",
                        "../../../../proc/self/environ",
                    ],
                }
            )

        elif attack_type == "buffer_overflow":
            # ãƒãƒƒãƒ•ã‚¡ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼æ”»æ’ƒã‚’æ¨¡æ“¬
            long_string = "A" * 10000
            malicious_data = pd.DataFrame(
                {
                    "Open": [50000, 51000, 52000],
                    "High": [51000, 52000, 53000],
                    "Low": [49000, 50000, 51000],
                    "Close": [50500, 51500, 52500],
                    "Volume": [1000, 1100, 1200],
                    "overflow_column": [long_string, long_string, long_string],
                }
            )

        elif attack_type == "code_injection":
            # ã‚³ãƒ¼ãƒ‰ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒã‚’æ¨¡æ“¬
            malicious_data = pd.DataFrame(
                {
                    "Open": [50000, 51000, 52000],
                    "High": [51000, 52000, 53000],
                    "Low": [49000, 50000, 51000],
                    "Close": [50500, 51500, 52500],
                    "Volume": [1000, 1100, 1200],
                    "code_column": [
                        "__import__('os').system('rm -rf /')",
                        'exec(\'import subprocess; subprocess.call(["ls", "/"])\')',
                        "eval('1+1; import sys; sys.exit()')",
                    ],
                }
            )

        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ­£å¸¸ãƒ‡ãƒ¼ã‚¿
            malicious_data = pd.DataFrame(
                {
                    "Open": [50000, 51000, 52000],
                    "High": [51000, 52000, 53000],
                    "Low": [49000, 50000, 51000],
                    "Close": [50500, 51500, 52500],
                    "Volume": [1000, 1100, 1200],
                }
            )

        return malicious_data

    def test_input_validation_security(self):
        """å…¥åŠ›æ¤œè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”’ å…¥åŠ›æ¤œè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆé–‹å§‹")

        attack_types = [
            "sql_injection",
            "script_injection",
            "path_traversal",
            "buffer_overflow",
            "code_injection",
        ]

        for attack_type in attack_types:
            start_time = time.time()

            try:
                # æ‚ªæ„ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                malicious_data = self.create_malicious_data(attack_type)

                # MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã§å‡¦ç†
                from app.services.ml.single_model.single_model_trainer import (
                    SingleModelTrainer,
                )

                trainer = SingleModelTrainer(model_type="lightgbm")

                # æ‚ªæ„ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã‚’è©¦è¡Œ
                result = trainer.train_model(
                    training_data=malicious_data,
                    save_model=False,
                    threshold_up=0.02,
                    threshold_down=-0.02,
                )

                execution_time = time.time() - start_time

                # çµæœã‚’åˆ†æ
                vulnerability_detected = False
                protection_verified = True
                security_level = "HIGH"

                # æ‚ªæ„ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒå‡¦ç†ã•ã‚ŒãŸå ´åˆã¯è„†å¼±æ€§ã®å¯èƒ½æ€§
                if result and "accuracy" in result:
                    # ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«å‡¦ç†ã•ã‚ŒãŸå ´åˆã€å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†ãªå¯èƒ½æ€§
                    if attack_type in [
                        "sql_injection",
                        "script_injection",
                        "code_injection",
                    ]:
                        vulnerability_detected = True
                        protection_verified = False
                        security_level = (
                            "MEDIUM"  # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã§é™¤å»ã•ã‚ŒãŸå¯èƒ½æ€§
                        )

                self.results.append(
                    SecurityTestResult(
                        test_name=f"å…¥åŠ›æ¤œè¨¼_{attack_type}",
                        security_category="input_validation",
                        success=protection_verified,
                        execution_time=execution_time,
                        vulnerability_detected=vulnerability_detected,
                        security_level=security_level,
                        protection_verified=protection_verified,
                        security_details={
                            "attack_type": attack_type,
                            "data_processed": result is not None,
                            "malicious_columns_detected": len(
                                [
                                    col
                                    for col in malicious_data.columns
                                    if "malicious" in col
                                    or "script" in col
                                    or "path" in col
                                    or "overflow" in col
                                    or "code" in col
                                ]
                            ),
                        },
                    )
                )

                logger.info(
                    f"âœ… {attack_type}æ”»æ’ƒãƒ†ã‚¹ãƒˆå®Œäº†: ä¿è­·ãƒ¬ãƒ™ãƒ«={security_level}"
                )

            except Exception as e:
                execution_time = time.time() - start_time

                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€é©åˆ‡ãªå…¥åŠ›æ¤œè¨¼ãŒæ©Ÿèƒ½ã—ã¦ã„ã‚‹å¯èƒ½æ€§
                protection_verified = True
                vulnerability_detected = False
                security_level = "HIGH"

                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æ©Ÿå¯†æƒ…å ±ãŒæ¼æ´©ã—ã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                error_msg = str(e).lower()
                sensitive_keywords = [
                    "password",
                    "secret",
                    "key",
                    "token",
                    "path",
                    "file",
                    "directory",
                ]
                info_leak = any(keyword in error_msg for keyword in sensitive_keywords)

                if info_leak:
                    vulnerability_detected = True
                    security_level = "MEDIUM"

                self.results.append(
                    SecurityTestResult(
                        test_name=f"å…¥åŠ›æ¤œè¨¼_{attack_type}",
                        security_category="input_validation",
                        success=protection_verified,
                        execution_time=execution_time,
                        vulnerability_detected=vulnerability_detected,
                        security_level=security_level,
                        protection_verified=protection_verified,
                        error_message=str(e),
                        security_details={
                            "attack_type": attack_type,
                            "error_occurred": True,
                            "information_leak_detected": info_leak,
                            "error_handled_securely": not info_leak,
                        },
                    )
                )

                logger.info(f"âœ… {attack_type}æ”»æ’ƒãƒ†ã‚¹ãƒˆå®Œäº†: ã‚¨ãƒ©ãƒ¼ã§é©åˆ‡ã«ãƒ–ãƒ­ãƒƒã‚¯")

    def test_data_protection(self):
        """ãƒ‡ãƒ¼ã‚¿ä¿è­·ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ›¡ï¸ ãƒ‡ãƒ¼ã‚¿ä¿è­·ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            # æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            sensitive_data = pd.DataFrame(
                {
                    "Open": [50000, 51000, 52000, 53000, 54000],
                    "High": [51000, 52000, 53000, 54000, 55000],
                    "Low": [49000, 50000, 51000, 52000, 53000],
                    "Close": [50500, 51500, 52500, 53500, 54500],
                    "Volume": [1000, 1100, 1200, 1300, 1400],
                    "user_id": ["user123", "user456", "user789", "user101", "user202"],
                    "api_key": [
                        "sk-1234567890abcdef",
                        "sk-abcdef1234567890",
                        "sk-fedcba0987654321",
                        "sk-1357924680abcdef",
                        "sk-2468013579fedcba",
                    ],
                    "password_hash": [
                        "$2b$12$abcdef...",
                        "$2b$12$fedcba...",
                        "$2b$12$123456...",
                        "$2b$12$789012...",
                        "$2b$12$345678...",
                    ],
                }
            )

            # MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§å‡¦ç†
            from app.services.ml.single_model.single_model_trainer import (
                SingleModelTrainer,
            )

            trainer = SingleModelTrainer(model_type="lightgbm")

            result = trainer.train_model(
                training_data=sensitive_data,
                save_model=False,
                threshold_up=0.02,
                threshold_down=-0.02,
            )

            execution_time = time.time() - start_time

            # ãƒ‡ãƒ¼ã‚¿ä¿è­·ã®æ¤œè¨¼
            protection_verified = True
            vulnerability_detected = False
            security_level = "HIGH"

            # æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ãŒç‰¹å¾´é‡ã«å«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if result and "feature_names" in result:
                feature_names = result["feature_names"]
                sensitive_features = [
                    name
                    for name in feature_names
                    if any(
                        sensitive in name.lower()
                        for sensitive in ["user_id", "api_key", "password"]
                    )
                ]

                if sensitive_features:
                    vulnerability_detected = True
                    protection_verified = False
                    security_level = "CRITICAL"

            # ãƒ­ã‚°å‡ºåŠ›ã«æ©Ÿå¯†æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            log_protection = True  # å®Ÿéš›ã®ãƒ­ã‚°ç›£è¦–ã¯è¤‡é›‘ãªãŸã‚ç°¡æ˜“å®Ÿè£…

            self.results.append(
                SecurityTestResult(
                    test_name="ãƒ‡ãƒ¼ã‚¿ä¿è­·",
                    security_category="data_protection",
                    success=protection_verified,
                    execution_time=execution_time,
                    vulnerability_detected=vulnerability_detected,
                    security_level=security_level,
                    protection_verified=protection_verified,
                    security_details={
                        "sensitive_data_processed": True,
                        "feature_count": (
                            result.get("feature_count", 0) if result else 0
                        ),
                        "sensitive_features_detected": vulnerability_detected,
                        "log_protection": log_protection,
                        "data_sanitization": not vulnerability_detected,
                    },
                )
            )

            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ä¿è­·ãƒ†ã‚¹ãƒˆå®Œäº†: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«={security_level}")

        except Exception as e:
            execution_time = time.time() - start_time

            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æ©Ÿå¯†æƒ…å ±æ¼æ´©ã‚’ãƒã‚§ãƒƒã‚¯
            error_msg = str(e)
            sensitive_patterns = ["user123", "sk-", "$2b$12$", "api_key", "password"]
            info_leak = any(pattern in error_msg for pattern in sensitive_patterns)

            vulnerability_detected = info_leak
            protection_verified = not info_leak
            security_level = "MEDIUM" if info_leak else "HIGH"

            self.results.append(
                SecurityTestResult(
                    test_name="ãƒ‡ãƒ¼ã‚¿ä¿è­·",
                    security_category="data_protection",
                    success=protection_verified,
                    execution_time=execution_time,
                    vulnerability_detected=vulnerability_detected,
                    security_level=security_level,
                    protection_verified=protection_verified,
                    error_message=str(e),
                    security_details={
                        "error_occurred": True,
                        "information_leak_in_error": info_leak,
                        "error_sanitization": not info_leak,
                    },
                )
            )

            logger.info(
                f"âœ… ãƒ‡ãƒ¼ã‚¿ä¿è­·ãƒ†ã‚¹ãƒˆå®Œäº†: ã‚¨ãƒ©ãƒ¼å‡¦ç†ã§ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«={security_level}"
            )

    def test_file_access_security(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            temp_dir = tempfile.mkdtemp()

            try:
                # æ­£å¸¸ãªãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                normal_data = pd.DataFrame(
                    {
                        "Open": [50000, 51000, 52000],
                        "High": [51000, 52000, 53000],
                        "Low": [49000, 50000, 51000],
                        "Close": [50500, 51500, 52500],
                        "Volume": [1000, 1100, 1200],
                    }
                )

                # MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚’è©¦è¡Œ
                from app.services.ml.single_model.single_model_trainer import (
                    SingleModelTrainer,
                )

                trainer = SingleModelTrainer(model_type="lightgbm")

                result = trainer.train_model(
                    training_data=normal_data,
                    save_model=True,  # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚’æœ‰åŠ¹åŒ–
                    threshold_up=0.02,
                    threshold_down=-0.02,
                )

                execution_time = time.time() - start_time

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®æ¤œè¨¼
                protection_verified = True
                vulnerability_detected = False
                security_level = "HIGH"

                # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æ¨©é™ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                file_security = True
                if result and "model_path" in result:
                    model_path = result["model_path"]
                    if os.path.exists(model_path):
                        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€é©åˆ‡ãªå ´æ‰€ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        if not model_path.startswith(
                            ("models/", "./models/", "backend/models/")
                        ):
                            vulnerability_detected = True
                            protection_verified = False
                            security_level = "MEDIUM"
                            file_security = False

                self.results.append(
                    SecurityTestResult(
                        test_name="ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                        security_category="file_access",
                        success=protection_verified,
                        execution_time=execution_time,
                        vulnerability_detected=vulnerability_detected,
                        security_level=security_level,
                        protection_verified=protection_verified,
                        security_details={
                            "model_saved": (
                                result is not None and "model_path" in result
                                if result
                                else False
                            ),
                            "file_path_secure": file_security,
                            "temp_dir_used": temp_dir is not None,
                            "file_permissions_secure": True,  # ç°¡æ˜“å®Ÿè£…
                        },
                    )
                )

                logger.info(
                    f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Œäº†: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«={security_level}"
                )

            finally:
                # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                shutil.rmtree(temp_dir, ignore_errors=True)

        except Exception as e:
            execution_time = time.time() - start_time

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼ã®åˆ†æ
            error_msg = str(e).lower()
            file_related_error = any(
                keyword in error_msg
                for keyword in ["permission", "access", "file", "directory", "path"]
            )

            protection_verified = file_related_error  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãŒåˆ¶é™ã•ã‚Œã¦ã„ã‚‹
            vulnerability_detected = not file_related_error
            security_level = "HIGH" if file_related_error else "MEDIUM"

            self.results.append(
                SecurityTestResult(
                    test_name="ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                    security_category="file_access",
                    success=protection_verified,
                    execution_time=execution_time,
                    vulnerability_detected=vulnerability_detected,
                    security_level=security_level,
                    protection_verified=protection_verified,
                    error_message=str(e),
                    security_details={
                        "file_access_error": file_related_error,
                        "access_control_working": file_related_error,
                        "error_type": type(e).__name__,
                    },
                )
            )

            logger.info(
                f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Œäº†: ã‚¨ãƒ©ãƒ¼åˆ†æã§ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«={security_level}"
            )

    def test_error_information_disclosure(self):
        """ã‚¨ãƒ©ãƒ¼æƒ…å ±æ¼æ´©ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸš¨ ã‚¨ãƒ©ãƒ¼æƒ…å ±æ¼æ´©ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            # æ„å›³çš„ã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            error_inducing_data = pd.DataFrame(
                {
                    "Open": [np.inf, -np.inf, np.nan],
                    "High": [np.inf, -np.inf, np.nan],
                    "Low": [np.inf, -np.inf, np.nan],
                    "Close": [np.inf, -np.inf, np.nan],
                    "Volume": [np.inf, -np.inf, np.nan],
                }
            )

            # MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§å‡¦ç†ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’èª˜ç™º
            from app.services.ml.single_model.single_model_trainer import (
                SingleModelTrainer,
            )

            trainer = SingleModelTrainer(model_type="lightgbm")

            try:
                result = trainer.train_model(
                    training_data=error_inducing_data,
                    save_model=False,
                    threshold_up=0.02,
                    threshold_down=-0.02,
                )

                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã‹ã£ãŸå ´åˆ
                execution_time = time.time() - start_time

                self.results.append(
                    SecurityTestResult(
                        test_name="ã‚¨ãƒ©ãƒ¼æƒ…å ±æ¼æ´©",
                        security_category="information_disclosure",
                        success=True,
                        execution_time=execution_time,
                        vulnerability_detected=False,
                        security_level="HIGH",
                        protection_verified=True,
                        security_details={
                            "error_occurred": False,
                            "data_sanitization_effective": True,
                            "robust_error_handling": True,
                        },
                    )
                )

                logger.info(
                    "âœ… ã‚¨ãƒ©ãƒ¼æƒ…å ±æ¼æ´©ãƒ†ã‚¹ãƒˆå®Œäº†: ãƒ‡ãƒ¼ã‚¿ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãŒæœ‰åŠ¹"
                )

            except Exception as e:
                execution_time = time.time() - start_time

                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åˆ†æ
                error_msg = str(e)

                # æ©Ÿå¯†æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                sensitive_info = [
                    "file path",
                    "directory",
                    "username",
                    "password",
                    "key",
                    "token",
                    "internal",
                    "stack trace",
                    "line number",
                    "function name",
                ]

                info_disclosure = any(
                    info.lower() in error_msg.lower() for info in sensitive_info
                )

                # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                stack_trace_present = (
                    "traceback" in error_msg.lower() or 'file "' in error_msg
                )

                vulnerability_detected = info_disclosure or stack_trace_present
                protection_verified = not vulnerability_detected
                security_level = (
                    "CRITICAL"
                    if stack_trace_present
                    else ("MEDIUM" if info_disclosure else "HIGH")
                )

                self.results.append(
                    SecurityTestResult(
                        test_name="ã‚¨ãƒ©ãƒ¼æƒ…å ±æ¼æ´©",
                        security_category="information_disclosure",
                        success=protection_verified,
                        execution_time=execution_time,
                        vulnerability_detected=vulnerability_detected,
                        security_level=security_level,
                        protection_verified=protection_verified,
                        error_message=(
                            error_msg[:200] + "..."
                            if len(error_msg) > 200
                            else error_msg
                        ),
                        security_details={
                            "error_occurred": True,
                            "sensitive_info_disclosed": info_disclosure,
                            "stack_trace_present": stack_trace_present,
                            "error_message_length": len(error_msg),
                            "error_sanitization": not vulnerability_detected,
                        },
                    )
                )

                logger.info(
                    f"âœ… ã‚¨ãƒ©ãƒ¼æƒ…å ±æ¼æ´©ãƒ†ã‚¹ãƒˆå®Œäº†: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«={security_level}"
                )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                SecurityTestResult(
                    test_name="ã‚¨ãƒ©ãƒ¼æƒ…å ±æ¼æ´©",
                    security_category="information_disclosure",
                    success=False,
                    execution_time=execution_time,
                    vulnerability_detected=True,
                    security_level="UNKNOWN",
                    protection_verified=False,
                    error_message=str(e),
                    security_details={
                        "test_setup_error": True,
                        "error_type": type(e).__name__,
                    },
                )
            )

            logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼æƒ…å ±æ¼æ´©ãƒ†ã‚¹ãƒˆã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

    def test_advanced_input_validation_security(self):
        """é«˜åº¦ãªå…¥åŠ›æ¤œè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆï¼ˆè„†å¼±æ€§ä¿®æ­£å¾Œï¼‰"""
        logger.info("ğŸ”’ é«˜åº¦ãªå…¥åŠ›æ¤œè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            # æ‚ªæ„ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
            malicious_patterns = {
                "timestamp": pd.date_range("2024-01-01", periods=10, freq="1H"),
                # SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³
                "Open": [
                    100.0,
                    "'; DROP TABLE users; --",
                    "1' OR '1'='1",
                    "UNION SELECT * FROM sensitive_data",
                    105.0,
                    "1; DELETE FROM accounts",
                    110.0,
                    "' OR 1=1 --",
                    115.0,
                    "admin'--",
                ],
                # XSSæ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³
                "High": [
                    105.0,
                    "<script>alert('XSS')</script>",
                    "javascript:alert('XSS')",
                    "<img src=x onerror=alert('XSS')>",
                    110.0,
                    "<iframe src='javascript:alert(1)'></iframe>",
                    115.0,
                    "';alert(String.fromCharCode(88,83,83))//",
                    120.0,
                    "<svg onload=alert('XSS')>",
                ],
                # ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒ
                "Low": [
                    95.0,
                    "../../../etc/passwd",
                    "..\\..\\..\\windows\\system32\\config\\sam",
                    "....//....//....//etc/passwd",
                    100.0,
                    "%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd",
                    105.0,
                    "..%252f..%252f..%252fetc%252fpasswd",
                    110.0,
                    "..%c0%af..%c0%af..%c0%afetc%c0%afpasswd",
                ],
                # DoSæ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³
                "Close": [
                    100.0,
                    1e308,  # æœ€å¤§floatå€¤
                    -1e308,
                    float("inf"),
                    float("-inf"),
                    105.0,
                    "A" * 10000,  # é•·ã„æ–‡å­—åˆ—
                    110.0,
                    "\x00" * 1000,  # ãƒŒãƒ«æ–‡å­—
                    115.0,
                ],
                "Volume": [
                    1000,
                    -999999999,
                    1e20,
                    0,
                    "null",
                    "undefined",
                    "NaN",
                    1500,
                    2000,
                    2500,
                ],
            }

            malicious_data = pd.DataFrame(malicious_patterns)

            # ä¿®æ­£ã•ã‚ŒãŸDataValidatorã®ãƒ†ã‚¹ãƒˆ
            try:
                from app.utils.data_validation import DataValidator

                validator = DataValidator()
                validation_result = validator.validate_ohlcv_data(
                    malicious_data, strict_mode=True
                )

                # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼
                security_passed = True
                vulnerabilities = []

                # æ‚ªæ„ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒé©åˆ‡ã«æ¤œå‡ºã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
                if validation_result.get("is_valid", True):
                    security_passed = False
                    vulnerabilities.append("æ‚ªæ„ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

                # OHLCé•åã®æ¤œå‡º
                ohlc_violations = validation_result.get("ohlc_violations", 0)
                if ohlc_violations == 0:
                    vulnerabilities.append("OHLCè«–ç†é•åãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

                # è² ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®æ¤œå‡º
                negative_volumes = validation_result.get("negative_volumes", 0)
                if negative_volumes == 0:
                    vulnerabilities.append("è² ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

                security_level = "HIGH" if security_passed else "CRITICAL"

                logger.info(
                    f"âœ… ä¿®æ­£ã•ã‚ŒãŸDataValidatoræ¤œè¨¼å®Œäº†: {len(vulnerabilities)}å€‹ã®è„†å¼±æ€§"
                )

            except Exception as e:
                security_passed = False
                vulnerabilities = [f"DataValidatorã‚¨ãƒ©ãƒ¼: {e}"]
                security_level = "CRITICAL"

            execution_time = time.time() - start_time

            result = SecurityTestResult(
                test_name="é«˜åº¦ãªå…¥åŠ›æ¤œè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                security_category="INPUT_VALIDATION",
                success=security_passed,
                execution_time=execution_time,
                vulnerability_detected=len(vulnerabilities) > 0,
                security_level=security_level,
                protection_verified=security_passed,
                security_details={
                    "vulnerabilities_found": vulnerabilities,
                    "validation_result": (
                        validation_result if "validation_result" in locals() else {}
                    ),
                    "malicious_patterns_tested": len(malicious_patterns),
                },
            )

            self.results.append(result)
            logger.info(f"âœ… é«˜åº¦ãªå…¥åŠ›æ¤œè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Œäº†: {security_level}")

        except Exception as e:
            execution_time = time.time() - start_time
            result = SecurityTestResult(
                test_name="é«˜åº¦ãªå…¥åŠ›æ¤œè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                security_category="INPUT_VALIDATION",
                success=False,
                execution_time=execution_time,
                vulnerability_detected=True,
                security_level="CRITICAL",
                protection_verified=False,
                error_message=str(e),
            )
            self.results.append(result)
            logger.error(f"âŒ é«˜åº¦ãªå…¥åŠ›æ¤œè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    def test_data_consistency_security(self):
        """ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆï¼ˆè„†å¼±æ€§ä¿®æ­£å¾Œï¼‰"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            # ä¸æ•´åˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
            inconsistent_data = pd.DataFrame(
                {
                    "timestamp": pd.date_range("2024-01-01", periods=8, freq="1H"),
                    "Open": [100, 105, 110, 115, 120, 125, 130, 135],
                    "High": [105, 110, 115, 120, 125, 130, 135, 140],
                    "Low": [95, 100, 105, 110, 115, 120, 125, 130],
                    "Close": [105, 110, 115, 120, 125, 130, 135, 140],
                    "Volume": [1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500],
                }
            )

            # é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ 
            duplicate_row = inconsistent_data.iloc[3:4].copy()
            inconsistent_data = pd.concat(
                [inconsistent_data, duplicate_row], ignore_index=True
            )

            # ä¿®æ­£ã•ã‚ŒãŸDataFrequencyManagerã®ãƒ†ã‚¹ãƒˆ
            try:
                from app.services.ml.feature_engineering.data_frequency_manager import (
                    DataFrequencyManager,
                )

                freq_manager = DataFrequencyManager()
                consistency_result = freq_manager.validate_data_consistency(
                    inconsistent_data, None, None, "1h"
                )

                # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼
                security_passed = True
                vulnerabilities = []

                # é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æ¤œå‡º
                duplicate_timestamps = consistency_result.get("duplicate_timestamps", 0)
                if duplicate_timestamps == 0:
                    vulnerabilities.append("é‡è¤‡ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    security_passed = False

                # ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã®æ¤œè¨¼
                if consistency_result.get("is_valid", True):
                    vulnerabilities.append("ä¸æ•´åˆãƒ‡ãƒ¼ã‚¿ãŒæœ‰åŠ¹ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸ")
                    security_passed = False

                security_level = "HIGH" if security_passed else "MEDIUM"

                logger.info(
                    f"âœ… ä¿®æ­£ã•ã‚ŒãŸDataFrequencyManageræ¤œè¨¼å®Œäº†: {len(vulnerabilities)}å€‹ã®å•é¡Œ"
                )

            except Exception as e:
                security_passed = False
                vulnerabilities = [f"DataFrequencyManagerã‚¨ãƒ©ãƒ¼: {e}"]
                security_level = "CRITICAL"

            execution_time = time.time() - start_time

            result = SecurityTestResult(
                test_name="ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                security_category="DATA_INTEGRITY",
                success=security_passed,
                execution_time=execution_time,
                vulnerability_detected=len(vulnerabilities) > 0,
                security_level=security_level,
                protection_verified=security_passed,
                security_details={
                    "vulnerabilities_found": vulnerabilities,
                    "consistency_result": (
                        consistency_result if "consistency_result" in locals() else {}
                    ),
                    "duplicate_timestamps_added": 1,
                },
            )

            self.results.append(result)
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Œäº†: {security_level}")

        except Exception as e:
            execution_time = time.time() - start_time
            result = SecurityTestResult(
                test_name="ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                security_category="DATA_INTEGRITY",
                success=False,
                execution_time=execution_time,
                vulnerability_detected=True,
                security_level="CRITICAL",
                protection_verified=False,
                error_message=str(e),
            )
            self.results.append(result)
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")


if __name__ == "__main__":
    logger.info("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")

    test_suite = SecurityTestSuite()

    # å„ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    test_suite.test_input_validation_security()
    test_suite.test_data_protection()
    test_suite.test_file_access_security()
    test_suite.test_error_information_disclosure()

    # çµæœã‚µãƒãƒªãƒ¼
    total_tests = len(test_suite.results)
    successful_tests = sum(1 for r in test_suite.results if r.success)
    vulnerabilities_detected = sum(
        1 for r in test_suite.results if r.vulnerability_detected
    )
    high_security_tests = sum(
        1 for r in test_suite.results if r.security_level == "HIGH"
    )

    print("\n" + "=" * 80)
    print("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆçµæœ")
    print("=" * 80)
    print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
    print(f"âœ… æˆåŠŸ: {successful_tests}")
    print(f"âŒ å¤±æ•—: {total_tests - successful_tests}")
    print(f"ğŸš¨ è„†å¼±æ€§æ¤œå‡º: {vulnerabilities_detected}")
    print(f"ğŸ›¡ï¸ é«˜ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: {high_security_tests}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {(successful_tests/total_tests*100):.1f}%")
    print(f"ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç‡: {(high_security_tests/total_tests*100):.1f}%")

    print("\nğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆè©³ç´°:")
    for result in test_suite.results:
        status = "âœ…" if result.success else "âŒ"
        vulnerability = "ğŸš¨" if result.vulnerability_detected else "ğŸ›¡ï¸"
        print(f"{status} {result.test_name}")
        print(f"   ã‚«ãƒ†ã‚´ãƒª: {result.security_category}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {result.execution_time:.2f}ç§’")
        print(f"   ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«: {result.security_level}")
        print(f"   è„†å¼±æ€§: {vulnerability}")
        print(f"   ä¿è­·ç¢ºèª: {'âœ…' if result.protection_verified else 'âŒ'}")
        if result.error_message:
            print(f"   ã‚¨ãƒ©ãƒ¼: {result.error_message[:100]}...")

    print("=" * 80)

    logger.info("ğŸ¯ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†")
