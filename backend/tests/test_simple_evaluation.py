#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«ãªè©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯çµ±ä¸€ãƒ†ã‚¹ãƒˆ

ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã‹ã‚’ç°¡å˜ã«ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
"""

import sys
import logging
import numpy as np
import pandas as pd
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent))

from app.services.ml.evaluation.enhanced_metrics import (
    EnhancedMetricsCalculator,
    MetricsConfig,
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def test_enhanced_metrics_comprehensive():
    """EnhancedMetricsCalculatorã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== EnhancedMetricsCalculatoråŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ ===")

    try:
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ‡ãƒ¼ã‚¿ï¼‰
        np.random.seed(42)
        n_samples = 100

        # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        y_true = np.random.choice([0, 1, 2], size=n_samples, p=[0.6, 0.3, 0.1])

        # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ã‚‹ç¨‹åº¦ã®ç²¾åº¦ã‚’æŒã¤ã‚ˆã†ã«ï¼‰
        y_pred = y_true.copy()
        # 20%ã®äºˆæ¸¬ã‚’é–“é•ãˆã‚‹
        wrong_indices = np.random.choice(
            n_samples, size=int(n_samples * 0.2), replace=False
        )
        for idx in wrong_indices:
            y_pred[idx] = np.random.choice([0, 1, 2])

        # äºˆæ¸¬ç¢ºç‡ï¼ˆsoftmaxé¢¨ã«ï¼‰
        logits = np.random.randn(n_samples, 3)
        y_pred_proba = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)

        # è¨­å®šä½œæˆ
        config = MetricsConfig(
            include_balanced_accuracy=True,
            include_pr_auc=True,
            include_roc_auc=True,
            include_confusion_matrix=True,
            include_classification_report=True,
            average_method="weighted",
            zero_division=0,
        )

        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        calculator = EnhancedMetricsCalculator(config)
        metrics = calculator.calculate_comprehensive_metrics(
            y_true, y_pred, y_pred_proba, class_names=["Down", "Hold", "Up"]
        )

        # çµæœç¢ºèªï¼ˆå¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã§ã¯ roc_auc_ovr ã‚’ä½¿ç”¨ï¼‰
        required_metrics = [
            "accuracy",
            "balanced_accuracy",
            "f1_score",
            "precision",
            "recall",
            "roc_auc_ovr",
            "pr_auc_macro",
            "confusion_matrix",
            "classification_report",
        ]

        missing_metrics = []
        for metric in required_metrics:
            if metric not in metrics:
                missing_metrics.append(metric)

        if missing_metrics:
            logger.error(f"ä¸è¶³ã—ã¦ã„ã‚‹è©•ä¾¡æŒ‡æ¨™: {missing_metrics}")
            return False

        # æŒ‡æ¨™ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        assert (
            0 <= metrics["accuracy"] <= 1
        ), f"accuracyç¯„å›²ã‚¨ãƒ©ãƒ¼: {metrics['accuracy']}"
        assert (
            0 <= metrics["balanced_accuracy"] <= 1
        ), f"balanced_accuracyç¯„å›²ã‚¨ãƒ©ãƒ¼: {metrics['balanced_accuracy']}"
        assert (
            0 <= metrics["f1_score"] <= 1
        ), f"f1_scoreç¯„å›²ã‚¨ãƒ©ãƒ¼: {metrics['f1_score']}"

        logger.info("âœ… åŒ…æ‹¬çš„è©•ä¾¡æŒ‡æ¨™è¨ˆç®—æˆåŠŸ")
        logger.info(f"   accuracy: {metrics['accuracy']:.4f}")
        logger.info(f"   balanced_accuracy: {metrics['balanced_accuracy']:.4f}")
        logger.info(f"   f1_score: {metrics['f1_score']:.4f}")
        logger.info(f"   precision: {metrics['precision']:.4f}")
        logger.info(f"   recall: {metrics['recall']:.4f}")
        logger.info(f"   roc_auc_ovr: {metrics['roc_auc_ovr']:.4f}")
        logger.info(f"   pr_auc_macro: {metrics['pr_auc_macro']:.4f}")
        logger.info(f"   ç·è©•ä¾¡æŒ‡æ¨™æ•°: {len(metrics)}")

        return True

    except Exception as e:
        logger.error(f"åŒ…æ‹¬çš„è©•ä¾¡ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_binary_classification():
    """äºŒå€¤åˆ†é¡ã§ã®è©•ä¾¡ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== äºŒå€¤åˆ†é¡è©•ä¾¡ãƒ†ã‚¹ãƒˆ ===")

    try:
        # äºŒå€¤åˆ†é¡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        np.random.seed(42)
        n_samples = 80

        y_true = np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3])
        y_pred = y_true.copy()

        # 15%ã®äºˆæ¸¬ã‚’é–“é•ãˆã‚‹
        wrong_indices = np.random.choice(
            n_samples, size=int(n_samples * 0.15), replace=False
        )
        for idx in wrong_indices:
            y_pred[idx] = 1 - y_pred[idx]  # 0->1, 1->0

        # äºŒå€¤åˆ†é¡ã®äºˆæ¸¬ç¢ºç‡
        y_pred_proba = np.random.rand(n_samples, 2)
        y_pred_proba = y_pred_proba / y_pred_proba.sum(axis=1, keepdims=True)

        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        config = MetricsConfig()
        calculator = EnhancedMetricsCalculator(config)
        metrics = calculator.calculate_comprehensive_metrics(
            y_true, y_pred, y_pred_proba, class_names=["Negative", "Positive"]
        )

        # äºŒå€¤åˆ†é¡ç‰¹æœ‰ã®æŒ‡æ¨™ç¢ºèª
        binary_metrics = ["accuracy", "balanced_accuracy", "roc_auc", "pr_auc"]
        for metric in binary_metrics:
            assert metric in metrics, f"äºŒå€¤åˆ†é¡æŒ‡æ¨™ {metric} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            assert 0 <= metrics[metric] <= 1, f"{metric} ç¯„å›²ã‚¨ãƒ©ãƒ¼: {metrics[metric]}"

        logger.info("âœ… äºŒå€¤åˆ†é¡è©•ä¾¡æˆåŠŸ")
        logger.info(f"   accuracy: {metrics['accuracy']:.4f}")
        logger.info(f"   balanced_accuracy: {metrics['balanced_accuracy']:.4f}")
        logger.info(f"   roc_auc: {metrics['roc_auc']:.4f}")
        logger.info(f"   pr_auc: {metrics['pr_auc']:.4f}")

        return True

    except Exception as e:
        logger.error(f"äºŒå€¤åˆ†é¡è©•ä¾¡ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_multiclass_classification():
    """å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã§ã®è©•ä¾¡ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== å¤šã‚¯ãƒ©ã‚¹åˆ†é¡è©•ä¾¡ãƒ†ã‚¹ãƒˆ ===")

    try:
        # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆ5ã‚¯ãƒ©ã‚¹ï¼‰
        np.random.seed(42)
        n_samples = 100
        n_classes = 5

        y_true = np.random.choice(range(n_classes), size=n_samples)
        y_pred = y_true.copy()

        # 25%ã®äºˆæ¸¬ã‚’é–“é•ãˆã‚‹
        wrong_indices = np.random.choice(
            n_samples, size=int(n_samples * 0.25), replace=False
        )
        for idx in wrong_indices:
            y_pred[idx] = np.random.choice(range(n_classes))

        # å¤šã‚¯ãƒ©ã‚¹äºˆæ¸¬ç¢ºç‡
        y_pred_proba = np.random.dirichlet([1] * n_classes, size=n_samples)

        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        config = MetricsConfig()
        calculator = EnhancedMetricsCalculator(config)
        metrics = calculator.calculate_comprehensive_metrics(
            y_true,
            y_pred,
            y_pred_proba,
            class_names=[f"Class_{i}" for i in range(n_classes)],
        )

        # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ç‰¹æœ‰ã®æŒ‡æ¨™ç¢ºèª
        multiclass_metrics = [
            "accuracy",
            "balanced_accuracy",
            "roc_auc_ovr",
            "pr_auc_macro",
        ]
        for metric in multiclass_metrics:
            assert metric in metrics, f"å¤šã‚¯ãƒ©ã‚¹åˆ†é¡æŒ‡æ¨™ {metric} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            assert 0 <= metrics[metric] <= 1, f"{metric} ç¯„å›²ã‚¨ãƒ©ãƒ¼: {metrics[metric]}"

        logger.info("âœ… å¤šã‚¯ãƒ©ã‚¹åˆ†é¡è©•ä¾¡æˆåŠŸ")
        logger.info(f"   accuracy: {metrics['accuracy']:.4f}")
        logger.info(f"   balanced_accuracy: {metrics['balanced_accuracy']:.4f}")
        logger.info(f"   roc_auc_ovr: {metrics['roc_auc_ovr']:.4f}")
        logger.info(f"   pr_auc_macro: {metrics['pr_auc_macro']:.4f}")

        return True

    except Exception as e:
        logger.error(f"å¤šã‚¯ãƒ©ã‚¹åˆ†é¡è©•ä¾¡ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("=" * 60)
    logger.info("ğŸš€ ã‚·ãƒ³ãƒ—ãƒ«è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯çµ±ä¸€ãƒ†ã‚¹ãƒˆé–‹å§‹")
    logger.info("=" * 60)

    tests = [
        ("åŒ…æ‹¬çš„è©•ä¾¡", test_enhanced_metrics_comprehensive),
        ("äºŒå€¤åˆ†é¡", test_binary_classification),
        ("å¤šã‚¯ãƒ©ã‚¹åˆ†é¡", test_multiclass_classification),
    ]

    results = []

    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            logger.error(f"{test_name}ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            results.append((test_name, False))

    # çµæœã‚µãƒãƒªãƒ¼
    logger.info("=" * 60)
    logger.info("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    logger.info("=" * 60)

    success_count = sum(1 for _, success in results if success)
    total_count = len(results)

    for test_name, success in results:
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
        logger.info(f"{test_name}: {status}")

    logger.info(
        f"æˆåŠŸç‡: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)"
    )

    if success_count == total_count:
        logger.info("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯çµ±ä¸€ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        logger.info("âœ… ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°2.1ã¯æˆåŠŸã—ã¾ã—ãŸã€‚")
        return True
    else:
        logger.warning(
            f"âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚({total_count-success_count}å€‹ã®å¤±æ•—)"
        )
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
