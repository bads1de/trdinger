"""
æœ€çµ‚æ¤œè¨¼ãƒ†ã‚¹ãƒˆ

ä¿®æ­£ã—ãŸæ©Ÿèƒ½ã¨æ–°ã—ãç™ºè¦‹ã—ãŸãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ç®‡æ‰€ã®æ¤œè¨¼
"""

import sys
import os
import numpy as np
import pandas as pd
import time

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'app'))

def test_data_processing_pipeline():
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ¤œè¨¼"""
    print("=== ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¤œè¨¼ ===")
    
    try:
        from utils.data_processing import DataProcessor
        
        processor = DataProcessor()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        np.random.seed(42)
        test_data = pd.DataFrame({
            'numeric1': np.random.randn(100),
            'numeric2': np.random.randn(100) * 10 + 50,
            'categorical1': np.random.choice(['A', 'B', 'C'], 100),
            'categorical2': np.random.choice(['X', 'Y', 'Z'], 100)
        })
        
        # ä¸€éƒ¨ã«NaNã¨å¤–ã‚Œå€¤ã‚’è¿½åŠ 
        test_data.loc[10:15, 'numeric1'] = np.nan
        test_data.loc[20:25, 'categorical1'] = np.nan
        test_data.loc[5, 'numeric2'] = 1000  # å¤–ã‚Œå€¤
        
        # Pipelineå‰å‡¦ç†å®Ÿè¡Œ
        result = processor.preprocess_with_pipeline(
            test_data,
            pipeline_name="final_test",
            numeric_strategy="median",
            scaling_method="robust",
            remove_outliers=True
        )
        
        print("âœ… Pipelineå‰å‡¦ç†æˆåŠŸ")
        print(f"   å…¥åŠ›: {test_data.shape}, å‡ºåŠ›: {result.shape}")
        print(f"   æ¬ æå€¤: {test_data.isnull().sum().sum()} â†’ {result.isnull().sum().sum()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def test_label_generation_kbins():
    """ãƒ©ãƒ™ãƒ«ç”ŸæˆKBinsDiscretizeræ¤œè¨¼"""
    print("\n=== ãƒ©ãƒ™ãƒ«ç”ŸæˆKBinsDiscretizeræ¤œè¨¼ ===")
    
    try:
        from utils.label_generation import LabelGenerator, ThresholdMethod
        
        generator = LabelGenerator()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=200, freq='h')
        price_data = pd.Series(
            50000 + np.cumsum(np.random.randn(200) * 100),
            index=dates,
            name='Close'
        )
        
        # KBinsDiscretizerãƒ†ã‚¹ãƒˆ
        labels, info = generator.generate_labels_with_kbins_discretizer(
            price_data,
            strategy='quantile'
        )
        
        print("âœ… KBinsDiscretizerãƒ©ãƒ™ãƒ«ç”ŸæˆæˆåŠŸ")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«: {set(labels.unique())}")
        print(f"   åˆ†å¸ƒ: {info.get('actual_distribution')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ©ãƒ™ãƒ«ç”ŸæˆKBinsDiscretizeræ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def test_advanced_features_optimization():
    """é«˜åº¦ç‰¹å¾´é‡ã®æœ€é©åŒ–æ¤œè¨¼"""
    print("\n=== é«˜åº¦ç‰¹å¾´é‡æœ€é©åŒ–æ¤œè¨¼ ===")
    
    try:
        # ç›´æ¥çš„ãªãƒ†ã‚¹ãƒˆï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        
        # NumPy polyfitãƒ†ã‚¹ãƒˆï¼ˆ3.1ä¿®æ­£ã®æ¤œè¨¼ï¼‰
        x = np.arange(20)
        y = 2 * x + 1 + np.random.randn(20) * 0.1
        slope = np.polyfit(x, y, 1)[0]
        print(f"âœ… NumPy polyfitå‹•ä½œç¢ºèª: å‚¾ã={slope:.3f}")
        
        # ç§»å‹•çµ±è¨ˆé‡ã®åŠ¹ç‡çš„è¨ˆç®—ãƒ†ã‚¹ãƒˆï¼ˆ3.9æŒ‡æ‘˜ç®‡æ‰€ã®æ”¹å–„ä¾‹ï¼‰
        data = pd.Series(np.random.randn(1000))
        
        # åŠ¹ç‡çš„ãªç§»å‹•çµ±è¨ˆé‡è¨ˆç®—
        start_time = time.time()
        ma = data.rolling(20).mean()
        std = data.rolling(20).std()
        median = data.rolling(20).median()
        efficient_time = time.time() - start_time
        
        print(f"âœ… åŠ¹ç‡çš„ç§»å‹•çµ±è¨ˆé‡è¨ˆç®—: {efficient_time:.4f}ç§’")
        print(f"   ç§»å‹•å¹³å‡: {len(ma.dropna())}å€‹ã®æœ‰åŠ¹å€¤")
        print(f"   ç§»å‹•æ¨™æº–åå·®: {len(std.dropna())}å€‹ã®æœ‰åŠ¹å€¤")
        print(f"   ç§»å‹•ä¸­å¤®å€¤: {len(median.dropna())}å€‹ã®æœ‰åŠ¹å€¤")
        
        return True
        
    except Exception as e:
        print(f"âŒ é«˜åº¦ç‰¹å¾´é‡æœ€é©åŒ–æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def test_normalization_standardization():
    """æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ–ã®æ¤œè¨¼ï¼ˆ3.8æŒ‡æ‘˜ç®‡æ‰€ï¼‰"""
    print("\n=== æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ–æ¤œè¨¼ ===")
    
    try:
        from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        data = np.random.randn(100, 3) * 10 + 50
        
        # å„ç¨®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ãƒ†ã‚¹ãƒˆ
        scalers = {
            'MinMax': MinMaxScaler(),
            'Standard': StandardScaler(),
            'Robust': RobustScaler()
        }
        
        for name, scaler in scalers.items():
            scaled_data = scaler.fit_transform(data)
            print(f"âœ… {name}Scalerå‹•ä½œç¢ºèª")
            print(f"   å…ƒãƒ‡ãƒ¼ã‚¿ç¯„å›²: [{data.min():.2f}, {data.max():.2f}]")
            print(f"   å¤‰æ›å¾Œç¯„å›²: [{scaled_data.min():.2f}, {scaled_data.max():.2f}]")
        
        # æ‰‹å‹•å®Ÿè£…ã¨ã®æ¯”è¼ƒï¼ˆgene_utils.pyã®æ”¹å–„ä¾‹ï¼‰
        def manual_normalize(value, min_val, max_val):
            return (value - min_val) / (max_val - min_val)
        
        def sklearn_normalize(values):
            scaler = MinMaxScaler()
            return scaler.fit_transform(values.reshape(-1, 1)).flatten()
        
        test_values = np.array([1, 5, 10, 15, 20])
        manual_result = [manual_normalize(v, 1, 20) for v in test_values]
        sklearn_result = sklearn_normalize(test_values)
        
        # çµæœã®ä¸€è‡´ç¢ºèª
        np.testing.assert_array_almost_equal(manual_result, sklearn_result, decimal=10)
        print("âœ… æ‰‹å‹•å®Ÿè£…ã¨sklearnå®Ÿè£…ã®çµæœãŒä¸€è‡´")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ–æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def test_distance_calculations():
    """è·é›¢è¨ˆç®—ã®æ¤œè¨¼ï¼ˆ3.10æŒ‡æ‘˜ç®‡æ‰€ï¼‰"""
    print("\n=== è·é›¢è¨ˆç®—æ¤œè¨¼ ===")
    
    try:
        from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances
        from sklearn.neighbors import NearestNeighbors
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        X = np.random.randn(50, 5)
        
        # è·é›¢è¨ˆç®—ãƒ†ã‚¹ãƒˆ
        euclidean_dist = euclidean_distances(X[:5], X[:5])
        manhattan_dist = manhattan_distances(X[:5], X[:5])
        
        print("âœ… sklearnè·é›¢è¨ˆç®—å‹•ä½œç¢ºèª")
        print(f"   ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢è¡Œåˆ—å½¢çŠ¶: {euclidean_dist.shape}")
        print(f"   ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢è¡Œåˆ—å½¢çŠ¶: {manhattan_dist.shape}")
        
        # NearestNeighborsãƒ†ã‚¹ãƒˆ
        nn = NearestNeighbors(n_neighbors=3, metric='minkowski', p=2)
        nn.fit(X)
        distances, indices = nn.kneighbors(X[:5])
        
        print("âœ… NearestNeighborså‹•ä½œç¢ºèª")
        print(f"   è¿‘å‚è·é›¢å½¢çŠ¶: {distances.shape}")
        print(f"   è¿‘å‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å½¢çŠ¶: {indices.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è·é›¢è¨ˆç®—æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def test_performance_comparison():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
    print("\n=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ†ã‚¹ãƒˆ ===")
    
    try:
        # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ†ã‚¹ãƒˆ
        large_data = np.random.randn(10000)
        
        # æ‰‹å‹•å®Ÿè£… vs ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå®Ÿè£…ã®æ¯”è¼ƒ
        
        # 1. ç§»å‹•å¹³å‡ã®æ¯”è¼ƒ
        start_time = time.time()
        # æ‰‹å‹•å®Ÿè£…ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        manual_ma = []
        window = 20
        for i in range(len(large_data)):
            if i >= window - 1:
                manual_ma.append(np.mean(large_data[i-window+1:i+1]))
            else:
                manual_ma.append(np.nan)
        manual_time = time.time() - start_time
        
        start_time = time.time()
        # pandaså®Ÿè£…
        pandas_ma = pd.Series(large_data).rolling(window).mean()
        pandas_time = time.time() - start_time
        
        speedup = manual_time / pandas_time
        print(f"âœ… ç§»å‹•å¹³å‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")
        print(f"   æ‰‹å‹•å®Ÿè£…: {manual_time:.4f}ç§’")
        print(f"   pandaså®Ÿè£…: {pandas_time:.4f}ç§’")
        print(f"   é«˜é€ŸåŒ–: {speedup:.2f}å€")
        
        # 2. æ­£è¦åŒ–ã®æ¯”è¼ƒ
        test_data = np.random.randn(10000, 10)
        
        start_time = time.time()
        # æ‰‹å‹•å®Ÿè£…
        manual_normalized = (test_data - test_data.mean(axis=0)) / test_data.std(axis=0)
        manual_norm_time = time.time() - start_time
        
        start_time = time.time()
        # sklearnå®Ÿè£…
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        sklearn_normalized = scaler.fit_transform(test_data)
        sklearn_norm_time = time.time() - start_time
        
        norm_speedup = manual_norm_time / sklearn_norm_time
        print(f"âœ… æ­£è¦åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")
        print(f"   æ‰‹å‹•å®Ÿè£…: {manual_norm_time:.4f}ç§’")
        print(f"   sklearnå®Ÿè£…: {sklearn_norm_time:.4f}ç§’")
        print(f"   é«˜é€ŸåŒ–: {norm_speedup:.2f}å€")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("æœ€çµ‚æ¤œè¨¼ãƒ†ã‚¹ãƒˆé–‹å§‹\n")
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test1_result = test_data_processing_pipeline()
    test2_result = test_label_generation_kbins()
    test3_result = test_advanced_features_optimization()
    test4_result = test_normalization_standardization()
    test5_result = test_distance_calculations()
    test6_result = test_performance_comparison()
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "="*60)
    print("=== æœ€çµ‚æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼ ===")
    print(f"ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: {'âœ… æˆåŠŸ' if test1_result else 'âŒ å¤±æ•—'}")
    print(f"ãƒ©ãƒ™ãƒ«ç”ŸæˆKBinsDiscretizer: {'âœ… æˆåŠŸ' if test2_result else 'âŒ å¤±æ•—'}")
    print(f"é«˜åº¦ç‰¹å¾´é‡æœ€é©åŒ–: {'âœ… æˆåŠŸ' if test3_result else 'âŒ å¤±æ•—'}")
    print(f"æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ–: {'âœ… æˆåŠŸ' if test4_result else 'âŒ å¤±æ•—'}")
    print(f"è·é›¢è¨ˆç®—: {'âœ… æˆåŠŸ' if test5_result else 'âŒ å¤±æ•—'}")
    print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ: {'âœ… æˆåŠŸ' if test6_result else 'âŒ å¤±æ•—'}")
    
    all_success = all([test1_result, test2_result, test3_result, test4_result, test5_result, test6_result])
    
    if all_success:
        print("\nğŸ‰ ã™ã¹ã¦ã®æœ€çµ‚æ¤œè¨¼ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
        print("\nä¿®æ­£ãƒ»æ”¹å–„å†…å®¹:")
        print("âœ… 3.1: stats.linregressã‚’np.polyfitã«ç½®ãæ›ãˆï¼ˆå®Œäº†ï¼‰")
        print("âœ… 3.3: KBinsDiscretizerã«ã‚ˆã‚‹ãƒ©ãƒ™ãƒ«ç”Ÿæˆç°¡ç´ åŒ–ï¼ˆå®Œäº†ï¼‰")
        print("âœ… 3.6: Pipelineã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†çµ±åˆï¼ˆå®Œäº†ï¼‰")
        print("ğŸ“‹ 3.8: æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ–ã®æ‰‹å‹•å®Ÿè£…ï¼ˆæ–°è¦ç™ºè¦‹ï¼‰")
        print("ğŸ“‹ 3.9: ç§»å‹•çµ±è¨ˆé‡ã®æ‰‹å‹•å®Ÿè£…ï¼ˆæ–°è¦ç™ºè¦‹ï¼‰")
        print("ğŸ“‹ 3.10: è·é›¢è¨ˆç®—ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ–°è¦ç™ºè¦‹ï¼‰")
        print("\næ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ´»ç”¨ã«ã‚ˆã‚Šã€ã‚³ãƒ¼ãƒ‰ã®å“è³ªã¨æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸã€‚")
    else:
        print("\nâš ï¸ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
    
    return all_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
