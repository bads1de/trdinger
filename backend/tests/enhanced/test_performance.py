"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¤œè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã€‚
å‡¦ç†æ™‚é–“ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’æ¸¬å®šãƒ»è©•ä¾¡ã—ã¾ã™ã€‚
"""

import pytest
import numpy as np
import pandas as pd
import logging
import time
import psutil
import os
from typing import Dict, List, Tuple, Any
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils.data_processing import DataProcessor
from app.utils.label_generation import LabelGenerator, ThresholdMethod
from app.services.ml.feature_engineering.feature_engineering_service import FeatureEngineeringService

logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.start_time = None
        self.start_memory = None
        self.process = psutil.Process(os.getpid())
    
    def start(self):
        """ç›£è¦–é–‹å§‹"""
        self.start_time = time.time()
        self.start_memory = self.process.memory_info().rss
    
    def stop(self) -> Dict[str, float]:
        """ç›£è¦–çµ‚äº†ã¨çµæœå–å¾—"""
        end_time = time.time()
        end_memory = self.process.memory_info().rss
        
        return {
            'execution_time': end_time - self.start_time,
            'memory_used': end_memory - self.start_memory,
            'peak_memory': self.process.memory_info().rss
        }


class TestPerformance:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def test_data_processing_performance(self):
        """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        monitor = PerformanceMonitor()
        
        # ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
        sizes = [1000, 5000, 10000]
        results = {}
        
        for size in sizes:
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            data = pd.DataFrame({
                'feature1': np.random.normal(0, 1, size),
                'feature2': np.random.exponential(1, size),
                'feature3': np.random.uniform(-1, 1, size),
                'feature4': np.random.lognormal(0, 1, size)
            })
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
            monitor.start()
            
            result = processor.preprocess_features(
                data,
                scale_features=True,
                remove_outliers=True,
                outlier_method='iqr'
            )
            
            perf_stats = monitor.stop()
            results[size] = perf_stats
            
            logger.info(f"ã‚µã‚¤ã‚º {size}: å®Ÿè¡Œæ™‚é–“ {perf_stats['execution_time']:.3f}ç§’, "
                       f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ {perf_stats['memory_used']/1024/1024:.1f}MB")
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®ç¢ºèªï¼ˆã‚¼ãƒ­é™¤ç®—ã‚’å›é¿ï¼‰
        if results[1000]['execution_time'] > 0:
            time_ratio = results[10000]['execution_time'] / results[1000]['execution_time']
            logger.info(f"10å€ãƒ‡ãƒ¼ã‚¿ã§ã®æ™‚é–“æ¯”ç‡: {time_ratio:.2f}x")
            assert time_ratio < 50, f"å‡¦ç†æ™‚é–“ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãŒæ‚ªã™ãã¾ã™: {time_ratio}x"
        else:
            logger.info("åŸºæº–ã¨ãªã‚‹å®Ÿè¡Œæ™‚é–“ãŒ0ã®ãŸã‚ã€æ™‚é–“æ¯”ç‡ã®è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—")

        if results[1000]['memory_used'] > 0:
            memory_ratio = results[10000]['memory_used'] / results[1000]['memory_used']
            logger.info(f"10å€ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ¡ãƒ¢ãƒªæ¯”ç‡: {memory_ratio:.2f}x")
            assert memory_ratio < 20, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãŒæ‚ªã™ãã¾ã™: {memory_ratio}x"
        else:
            logger.info("åŸºæº–ã¨ãªã‚‹ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ0ã®ãŸã‚ã€ãƒ¡ãƒ¢ãƒªæ¯”ç‡ã®è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_feature_engineering_performance(self):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ ===")
        
        fe_service = FeatureEngineeringService()
        monitor = PerformanceMonitor()
        
        # ç•°ãªã‚‹æœŸé–“ã®OHLCVãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
        periods = [100, 500, 1000]
        results = {}
        
        for period in periods:
            # OHLCVãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            np.random.seed(42)
            dates = pd.date_range('2023-01-01', periods=period, freq='1h')
            
            base_price = 50000
            returns = np.random.normal(0, 0.02, period)
            prices = [base_price]
            
            for ret in returns[1:]:
                prices.append(prices[-1] * (1 + ret))
            
            data = pd.DataFrame({
                'timestamp': dates,
                'Open': prices,
                'High': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
                'Low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
                'Close': [p * (1 + np.random.normal(0, 0.005)) for p in prices],
                'Volume': np.random.lognormal(10, 1, period)
            }).set_index('timestamp')
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
            monitor.start()
            
            try:
                features = fe_service.calculate_advanced_features(data)
                perf_stats = monitor.stop()
                results[period] = perf_stats
                
                logger.info(f"æœŸé–“ {period}: å®Ÿè¡Œæ™‚é–“ {perf_stats['execution_time']:.3f}ç§’, "
                           f"ç‰¹å¾´é‡æ•° {features.shape[1]}, "
                           f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ {perf_stats['memory_used']/1024/1024:.1f}MB")
                
            except Exception as e:
                logger.warning(f"æœŸé–“ {period} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                results[period] = {'execution_time': float('inf'), 'memory_used': 0}
        
        # æœ‰åŠ¹ãªçµæœãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        valid_results = {k: v for k, v in results.items() if v['execution_time'] != float('inf')}
        assert len(valid_results) > 0, "æœ‰åŠ¹ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµæœãŒã‚ã‚Šã¾ã›ã‚“"
        
        logger.info("âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_label_generation_performance(self):
        """ãƒ©ãƒ™ãƒ«ç”Ÿæˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ©ãƒ™ãƒ«ç”Ÿæˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ ===")
        
        label_generator = LabelGenerator()
        monitor = PerformanceMonitor()
        
        # ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
        sizes = [1000, 5000, 10000, 50000]
        results = {}
        
        for size in sizes:
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            np.random.seed(42)
            base_price = 50000
            returns = np.random.normal(0, 0.02, size)
            prices = [base_price]
            
            for ret in returns[1:]:
                prices.append(prices[-1] * (1 + ret))
            
            price_series = pd.Series(prices, name='Close')
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
            monitor.start()
            
            labels, threshold_info = label_generator.generate_labels(
                price_series,
                method=ThresholdMethod.FIXED,
                threshold_up=0.02,
                threshold_down=-0.02
            )
            
            perf_stats = monitor.stop()
            results[size] = perf_stats
            
            logger.info(f"ã‚µã‚¤ã‚º {size}: å®Ÿè¡Œæ™‚é–“ {perf_stats['execution_time']:.3f}ç§’, "
                       f"ãƒ©ãƒ™ãƒ«æ•° {len(labels)}, "
                       f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ {perf_stats['memory_used']/1024/1024:.1f}MB")
        
        # ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®ç¢ºèª
        if len(results) >= 2:
            sizes_list = sorted(results.keys())
            time_per_sample = {}
            
            for size in sizes_list:
                time_per_sample[size] = results[size]['execution_time'] / size
            
            # ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Šã®å‡¦ç†æ™‚é–“ãŒåˆç†çš„ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            max_time_per_sample = max(time_per_sample.values())
            assert max_time_per_sample < 0.001, f"ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Šã®å‡¦ç†æ™‚é–“ãŒé…ã™ãã¾ã™: {max_time_per_sample:.6f}ç§’"
        
        logger.info("âœ… ãƒ©ãƒ™ãƒ«ç”Ÿæˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_memory_efficiency(self):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        monitor = PerformanceMonitor()
        
        # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ãƒ†ã‚¹ãƒˆ
        data_size = 10000
        feature_count = 50
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        data = pd.DataFrame({
            f'feature_{i}': np.random.normal(0, 1, data_size) 
            for i in range(feature_count)
        })
        
        initial_memory = monitor.process.memory_info().rss
        
        # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã®ãƒ†ã‚¹ãƒˆ
        monitor.start()
        optimized_data = processor.optimize_dtypes(data)
        perf_stats = monitor.stop()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¯”è¼ƒ
        original_memory = data.memory_usage(deep=True).sum()
        optimized_memory = optimized_data.memory_usage(deep=True).sum()
        memory_reduction = (original_memory - optimized_memory) / original_memory
        
        logger.info(f"å…ƒã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {original_memory/1024/1024:.1f}MB")
        logger.info(f"æœ€é©åŒ–å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {optimized_memory/1024/1024:.1f}MB")
        logger.info(f"ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç‡: {memory_reduction*100:.1f}%")
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒæ”¹å–„ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert memory_reduction >= 0, "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¦ã„ã¾ã™"
        
        logger.info("âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_concurrent_performance(self):
        """ä¸¦è¡Œå‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ä¸¦è¡Œå‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        
        # è¤‡æ•°ã®å°ã•ãªã‚¿ã‚¹ã‚¯ã‚’ä¸¦è¡Œå®Ÿè¡Œ
        task_count = 5
        data_size = 1000
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        datasets = []
        for i in range(task_count):
            data = pd.DataFrame({
                'feature1': np.random.normal(0, 1, data_size),
                'feature2': np.random.exponential(1, data_size)
            })
            datasets.append(data)
        
        # é †æ¬¡å®Ÿè¡Œã®æ™‚é–“æ¸¬å®š
        start_time = time.time()
        sequential_results = []
        for data in datasets:
            result = processor.preprocess_features(data, scale_features=True)
            sequential_results.append(result)
        sequential_time = time.time() - start_time
        
        logger.info(f"é †æ¬¡å®Ÿè¡Œæ™‚é–“: {sequential_time:.3f}ç§’")
        
        # ä¸¦è¡Œå®Ÿè¡Œã¯å®Ÿè£…ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€é †æ¬¡å®Ÿè¡Œã®åŠ¹ç‡æ€§ã®ã¿ç¢ºèª
        time_per_task = sequential_time / task_count
        assert time_per_task < 1.0, f"ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®å‡¦ç†æ™‚é–“ãŒé…ã™ãã¾ã™: {time_per_task:.3f}ç§’"
        
        logger.info("âœ… ä¸¦è¡Œå‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_scalability_limits(self):
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£é™ç•Œãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£é™ç•Œãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        
        # æ®µéšçš„ã«ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã—ã¦ãƒ†ã‚¹ãƒˆ
        max_successful_size = 0
        test_sizes = [1000, 5000, 10000, 20000, 50000]
        
        for size in test_sizes:
            try:
                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                data = pd.DataFrame({
                    'feature1': np.random.normal(0, 1, size),
                    'feature2': np.random.exponential(1, size)
                })
                
                start_time = time.time()
                result = processor.preprocess_features(data, scale_features=True)
                execution_time = time.time() - start_time
                
                max_successful_size = size
                logger.info(f"ã‚µã‚¤ã‚º {size}: æˆåŠŸ ({execution_time:.3f}ç§’)")
                
                # å‡¦ç†æ™‚é–“ãŒåˆç†çš„ãªç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if execution_time > 30:  # 30ç§’ä»¥ä¸Šã‹ã‹ã‚‹å ´åˆã¯åœæ­¢
                    logger.warning(f"ã‚µã‚¤ã‚º {size} ã§å‡¦ç†æ™‚é–“ãŒé•·ã™ãã¾ã™: {execution_time:.3f}ç§’")
                    break
                    
            except Exception as e:
                logger.warning(f"ã‚µã‚¤ã‚º {size} ã§å¤±æ•—: {e}")
                break
        
        logger.info(f"æœ€å¤§æˆåŠŸã‚µã‚¤ã‚º: {max_successful_size}")
        assert max_successful_size >= 1000, "æœ€å°é™ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãŒç¢ºä¿ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        logger.info("âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£é™ç•Œãƒ†ã‚¹ãƒˆå®Œäº†")


def run_all_performance_tests():
    """ã™ã¹ã¦ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    logger.info("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’é–‹å§‹")
    
    test_instance = TestPerformance()
    
    try:
        test_instance.test_data_processing_performance()
        test_instance.test_feature_engineering_performance()
        test_instance.test_label_generation_performance()
        test_instance.test_memory_efficiency()
        test_instance.test_concurrent_performance()
        test_instance.test_scalability_limits()
        
        logger.info("ğŸ‰ ã™ã¹ã¦ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    success = run_all_performance_tests()
    sys.exit(0 if success else 1)
