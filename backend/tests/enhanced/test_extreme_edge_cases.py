"""
æ¥µç«¯ãªã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ

æ¥µç«¯ãªå¢ƒç•Œæ¡ä»¶ã€ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ä¸‹ã§ã®
MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œã‚’æ¤œè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã€‚
"""

import numpy as np
import pandas as pd
import logging
import time
import psutil
import os
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils.data_processing import DataProcessor
from app.utils.label_generation import LabelGenerator, ThresholdMethod
from app.services.ml.feature_engineering.feature_engineering_service import FeatureEngineeringService

logger = logging.getLogger(__name__)


class TestExtremeEdgeCases:
    """æ¥µç«¯ãªã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def test_micro_dataset_handling(self):
        """ãƒã‚¤ã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ¥µå°ãƒ‡ãƒ¼ã‚¿ï¼‰ã®å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒã‚¤ã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ãƒ†ã‚¹ãƒˆ ===")
        
        # æ¥µå°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ2-10è¡Œï¼‰ã§ã®ãƒ†ã‚¹ãƒˆ
        sizes = [2, 3, 5, 10]
        
        for size in sizes:
            logger.info(f"ã‚µã‚¤ã‚º {size} ã®ãƒã‚¤ã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ†ã‚¹ãƒˆä¸­...")
            
            # ãƒã‚¤ã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            micro_data = pd.DataFrame({
                'Open': [100 + i for i in range(size)],
                'High': [102 + i for i in range(size)],
                'Low': [99 + i for i in range(size)],
                'Close': [101 + i for i in range(size)],
                'Volume': [1000 + i*100 for i in range(size)]
            })
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ
            processor = DataProcessor()
            try:
                processed = processor.preprocess_features(
                    micro_data[['Close', 'Volume']].copy(),
                    scale_features=True,
                    remove_outliers=False  # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã§ã¯å¤–ã‚Œå€¤é™¤å»ã‚’ç„¡åŠ¹åŒ–
                )
                logger.info(f"  ã‚µã‚¤ã‚º {size}: ãƒ‡ãƒ¼ã‚¿å‡¦ç†æˆåŠŸ ({len(processed)}è¡Œ)")
                
                # åŸºæœ¬çš„ãªæ•´åˆæ€§ç¢ºèª
                assert len(processed) > 0, f"ã‚µã‚¤ã‚º {size} ã§å‡¦ç†çµæœãŒç©ºã§ã™"
                assert not processed.isnull().all().all(), f"ã‚µã‚¤ã‚º {size} ã§å…¨ã¦NaNã§ã™"
                
            except Exception as e:
                logger.info(f"  ã‚µã‚¤ã‚º {size}: æœŸå¾…é€šã‚Šã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ - {e}")
            
            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
            fe_service = FeatureEngineeringService()
            try:
                features = fe_service.calculate_advanced_features(micro_data)
                logger.info(f"  ã‚µã‚¤ã‚º {size}: ç‰¹å¾´é‡è¨ˆç®—æˆåŠŸ ({features.shape[1]}ç‰¹å¾´é‡)")
            except Exception as e:
                logger.info(f"  ã‚µã‚¤ã‚º {size}: ç‰¹å¾´é‡è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼ - {e}")
            
            # ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒ†ã‚¹ãƒˆ
            label_generator = LabelGenerator()
            try:
                labels, _ = label_generator.generate_labels(
                    pd.Series(micro_data['Close'].values, name='Close'),
                    method=ThresholdMethod.FIXED,
                    threshold_up=0.02,
                    threshold_down=-0.02
                )
                logger.info(f"  ã‚µã‚¤ã‚º {size}: ãƒ©ãƒ™ãƒ«ç”ŸæˆæˆåŠŸ ({len(labels)}ãƒ©ãƒ™ãƒ«)")
            except Exception as e:
                logger.info(f"  ã‚µã‚¤ã‚º {size}: ãƒ©ãƒ™ãƒ«ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ - {e}")
        
        logger.info("âœ… ãƒã‚¤ã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_identical_values_dataset(self):
        """å…¨ã¦åŒã˜å€¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        logger.info("=== å…¨åŒå€¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ ===")
        
        # å…¨ã¦åŒã˜å€¤ã®ãƒ‡ãƒ¼ã‚¿
        identical_data = pd.DataFrame({
            'Open': [100.0] * 100,
            'High': [100.0] * 100,
            'Low': [100.0] * 100,
            'Close': [100.0] * 100,
            'Volume': [1000.0] * 100
        })
        
        processor = DataProcessor()
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ
        try:
            processed = processor.preprocess_features(
                identical_data[['Close', 'Volume']].copy(),
                scale_features=True,
                remove_outliers=True
            )
            
            # åŒå€¤ãƒ‡ãƒ¼ã‚¿ã§ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°çµæœç¢ºèª
            logger.info(f"åŒå€¤ãƒ‡ãƒ¼ã‚¿å‡¦ç†çµæœ: {processed.shape}")
            logger.info(f"Closeçµ±è¨ˆ: mean={processed['Close'].mean():.6f}, std={processed['Close'].std():.6f}")
            
            # æ¨™æº–åå·®ãŒ0ã®å ´åˆã®å‡¦ç†ç¢ºèª
            if processed['Close'].std() == 0:
                logger.info("âœ… åŒå€¤ãƒ‡ãƒ¼ã‚¿ã§æ¨™æº–åå·®0ãŒæ­£ã—ãå‡¦ç†ã•ã‚Œã¾ã—ãŸ")
            
        except Exception as e:
            logger.info(f"âœ… åŒå€¤ãƒ‡ãƒ¼ã‚¿ã§æœŸå¾…é€šã‚Šã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        fe_service = FeatureEngineeringService()
        try:
            features = fe_service.calculate_advanced_features(identical_data)
            
            # åŒå€¤ãƒ‡ãƒ¼ã‚¿ã§ã®ç‰¹å¾´é‡ç¢ºèª
            logger.info(f"åŒå€¤ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡: {features.shape}")
            
            # å¤‰åŒ–ç‡ç³»ã®ç‰¹å¾´é‡ãŒ0ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèª
            change_features = [col for col in features.columns if 'change' in col.lower() or 'return' in col.lower()]
            for col in change_features[:3]:  # æœ€åˆã®3ã¤ã‚’ãƒã‚§ãƒƒã‚¯
                if col in features.columns:
                    unique_values = features[col].nunique()
                    logger.info(f"å¤‰åŒ–ç‡ç‰¹å¾´é‡ {col}: ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°={unique_values}")
            
        except Exception as e:
            logger.info(f"âœ… åŒå€¤ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        logger.info("âœ… å…¨åŒå€¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_extreme_volatility_dataset(self):
        """æ¥µç«¯ãªå¤‰å‹•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        logger.info("=== æ¥µç«¯å¤‰å‹•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ ===")
        
        # æ¥µç«¯ãªå¤‰å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ
        extreme_patterns = {
            'sudden_spike': [100, 100, 100, 1000, 100, 100, 100],  # çªç„¶ã®ã‚¹ãƒ‘ã‚¤ã‚¯
            'gradual_explosion': [100, 200, 400, 800, 1600, 3200, 6400],  # æŒ‡æ•°çš„å¢—åŠ 
            'oscillation': [100, 10, 100, 10, 100, 10, 100],  # æ¿€ã—ã„æŒ¯å‹•
            'step_function': [100, 100, 200, 200, 300, 300, 400]  # ã‚¹ãƒ†ãƒƒãƒ—é–¢æ•°
        }
        
        for pattern_name, prices in extreme_patterns.items():
            logger.info(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern_name}' ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
            
            # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            size = len(prices)
            extreme_data = pd.DataFrame({
                'Open': prices,
                'High': [p * 1.1 for p in prices],
                'Low': [p * 0.9 for p in prices],
                'Close': prices,
                'Volume': [1000] * size
            })
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            processor = DataProcessor()
            try:
                processed = processor.preprocess_features(
                    extreme_data[['Close', 'Volume']].copy(),
                    scale_features=True,
                    remove_outliers=True
                )
                
                logger.info(f"  {pattern_name}: å‡¦ç†æˆåŠŸ ({len(processed)}è¡Œ)")
                
                # å¤–ã‚Œå€¤é™¤å»ã®åŠ¹æœç¢ºèª
                original_range = max(prices) - min(prices)
                processed_range = processed['Close'].max() - processed['Close'].min()
                logger.info(f"  {pattern_name}: å…ƒã®ç¯„å›²={original_range:.1f}, å‡¦ç†å¾Œç¯„å›²={processed_range:.3f}")
                
            except Exception as e:
                logger.info(f"  {pattern_name}: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ - {e}")
            
            # ãƒ©ãƒ™ãƒ«ç”Ÿæˆã§ã®æ¥µç«¯å¤‰å‹•ã®å‡¦ç†
            label_generator = LabelGenerator()
            try:
                labels, threshold_info = label_generator.generate_labels(
                    pd.Series(prices, name='Close'),
                    method=ThresholdMethod.FIXED,
                    threshold_up=0.1,  # 10%ã®é–¾å€¤
                    threshold_down=-0.1
                )
                
                label_counts = pd.Series(labels).value_counts()
                logger.info(f"  {pattern_name}: ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ {label_counts.to_dict()}")
                
            except Exception as e:
                logger.info(f"  {pattern_name}: ãƒ©ãƒ™ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼ - {e}")
        
        logger.info("âœ… æ¥µç«¯å¤‰å‹•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_memory_pressure_conditions(self):
        """ãƒ¡ãƒ¢ãƒªåœ§è¿«æ¡ä»¶ä¸‹ã§ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ¡ãƒ¢ãƒªåœ§è¿«æ¡ä»¶ãƒ†ã‚¹ãƒˆ ===")
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # æ®µéšçš„ã«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¢—ã‚„ã—ã¦ãƒ†ã‚¹ãƒˆ
        memory_stress_data = []
        
        try:
            # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ®µéšçš„ã«ä½œæˆ
            for i in range(5):
                size = 20000 * (i + 1)  # 20K, 40K, 60K, 80K, 100K
                
                logger.info(f"ãƒ¡ãƒ¢ãƒªã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ: ã‚µã‚¤ã‚º {size}")
                
                # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                data = pd.DataFrame({
                    'Close': np.random.normal(100, 10, size),
                    'Volume': np.random.lognormal(10, 1, size)
                })
                
                memory_stress_data.append(data)
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                current_memory = process.memory_info().rss
                memory_increase = current_memory - initial_memory
                
                logger.info(f"  ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_increase/1e6:.1f}MB")
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ1GBä»¥ä¸Šã«ãªã£ãŸã‚‰åœæ­¢
                if memory_increase > 1e9:
                    logger.warning("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ1GBã‚’è¶…ãˆãŸãŸã‚ã€ãƒ†ã‚¹ãƒˆã‚’åœæ­¢")
                    break
                
                # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ
                processor = DataProcessor()
                start_time = time.time()
                
                processed = processor.preprocess_features(
                    data.copy(),
                    scale_features=True,
                    remove_outliers=True
                )
                
                processing_time = time.time() - start_time
                logger.info(f"  å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã®æ¤œå‡º
                if processing_time > 30:  # 30ç§’ä»¥ä¸Š
                    logger.warning(f"å‡¦ç†æ™‚é–“ãŒé•·ã™ãã¾ã™: {processing_time:.2f}ç§’")
                    break
        
        except MemoryError:
            logger.info("âœ… ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«ç™ºç”Ÿã—ã¾ã—ãŸ")
        except Exception as e:
            logger.warning(f"ãƒ¡ãƒ¢ãƒªã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        finally:
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            memory_stress_data.clear()
            
        logger.info("âœ… ãƒ¡ãƒ¢ãƒªåœ§è¿«æ¡ä»¶ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_cpu_intensive_conditions(self):
        """CPUé›†ç´„çš„æ¡ä»¶ä¸‹ã§ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== CPUé›†ç´„çš„æ¡ä»¶ãƒ†ã‚¹ãƒˆ ===")
        
        # è¤‡é›‘ãªç‰¹å¾´é‡è¨ˆç®—ã‚’ä¸¦è¡Œå®Ÿè¡Œ
        fe_service = FeatureEngineeringService()
        
        # è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§åŒæ™‚ã«ç‰¹å¾´é‡è¨ˆç®—
        datasets = []
        for i in range(3):
            data = pd.DataFrame({
                'Open': np.random.normal(100, 10, 1000),
                'High': np.random.normal(105, 10, 1000),
                'Low': np.random.normal(95, 10, 1000),
                'Close': np.random.normal(100, 10, 1000),
                'Volume': np.random.lognormal(10, 1, 1000)
            })
            datasets.append(data)
        
        start_time = time.time()
        results = []
        
        try:
            for i, data in enumerate(datasets):
                logger.info(f"CPUé›†ç´„ãƒ†ã‚¹ãƒˆ {i+1}/3 ã‚’å®Ÿè¡Œä¸­...")
                
                features = fe_service.calculate_advanced_features(data)
                results.append(features)
                
                elapsed = time.time() - start_time
                logger.info(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {i+1}: {features.shape[1]}ç‰¹å¾´é‡, çµŒéæ™‚é–“: {elapsed:.2f}ç§’")
        
        except Exception as e:
            logger.warning(f"CPUé›†ç´„ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        total_time = time.time() - start_time
        logger.info(f"CPUé›†ç´„ãƒ†ã‚¹ãƒˆå®Œäº†: ç·æ™‚é–“ {total_time:.2f}ç§’")
        
        # CPUåŠ¹ç‡ã®ç¢ºèª
        if len(results) > 0:
            avg_features = sum(r.shape[1] for r in results) / len(results)
            features_per_second = avg_features * len(results) / total_time
            logger.info(f"ç‰¹å¾´é‡è¨ˆç®—åŠ¹ç‡: {features_per_second:.1f}ç‰¹å¾´é‡/ç§’")
        
        logger.info("âœ… CPUé›†ç´„çš„æ¡ä»¶ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_data_corruption_scenarios(self):
        """ãƒ‡ãƒ¼ã‚¿ç ´æã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿ç ´æã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ ===")
        
        # æ§˜ã€…ãªç ´æãƒ‘ã‚¿ãƒ¼ãƒ³
        corruption_scenarios = {
            'mixed_types': pd.DataFrame({
                'Close': [100, 'invalid', 102, None, 104],
                'Volume': [1000, 1001, 'error', 1003, 1004]
            }),
            'infinite_values': pd.DataFrame({
                'Close': [100, np.inf, 102, -np.inf, 104],
                'Volume': [1000, 1001, np.inf, 1003, 1004]
            }),
            'extreme_outliers': pd.DataFrame({
                'Close': [100, 101, 1e10, 103, 104],
                'Volume': [1000, 1001, 1002, -1e10, 1004]
            })
        }
        
        processor = DataProcessor()
        
        for scenario_name, corrupted_data in corruption_scenarios.items():
            logger.info(f"ç ´æã‚·ãƒŠãƒªã‚ª '{scenario_name}' ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
            
            try:
                processed = processor.preprocess_features(
                    corrupted_data.copy(),
                    scale_features=True,
                    remove_outliers=True
                )
                
                # ç ´æãƒ‡ãƒ¼ã‚¿ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚ŒãŸã‹ç¢ºèª
                has_invalid = processed.isnull().any().any() or np.isinf(processed.select_dtypes(include=[np.number])).any().any()
                
                if not has_invalid:
                    logger.info(f"  {scenario_name}: ç ´æãƒ‡ãƒ¼ã‚¿ãŒé©åˆ‡ã«ä¿®å¾©ã•ã‚Œã¾ã—ãŸ")
                else:
                    logger.warning(f"  {scenario_name}: ç„¡åŠ¹ãªå€¤ãŒæ®‹ã£ã¦ã„ã¾ã™")
                
            except Exception as e:
                logger.info(f"  {scenario_name}: æœŸå¾…é€šã‚Šã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ - {e}")
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ç ´æã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆå®Œäº†")


def run_all_extreme_edge_case_tests():
    """ã™ã¹ã¦ã®æ¥µç«¯ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    logger.info("ğŸ”¥ æ¥µç«¯ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’é–‹å§‹")
    
    test_instance = TestExtremeEdgeCases()
    
    try:
        test_instance.test_micro_dataset_handling()
        test_instance.test_identical_values_dataset()
        test_instance.test_extreme_volatility_dataset()
        test_instance.test_memory_pressure_conditions()
        test_instance.test_cpu_intensive_conditions()
        test_instance.test_data_corruption_scenarios()
        
        logger.info("ğŸ‰ ã™ã¹ã¦ã®æ¥µç«¯ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¥µç«¯ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    success = run_all_extreme_edge_case_tests()
    sys.exit(0 if success else 1)
