#!/usr/bin/env python3
"""
ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®çµ±åˆãƒ†ã‚¹ãƒˆ

å®Ÿéš›ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚¯ãƒ©ã‚¹ã§è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®çµ±ä¸€ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã‹ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
"""

import sys
import logging
import numpy as np
import pandas as pd
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent))

from app.services.ml.ensemble.stacking import StackingEnsemble
from app.services.ml.ensemble.bagging import BaggingEnsemble

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def create_simple_test_data(n_samples=50, n_features=3, n_classes=3):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    np.random.seed(42)
    
    # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
    y = np.random.choice(range(n_classes), size=n_samples, p=[0.5, 0.3, 0.2])
    y = pd.Series(y, name='target')
    
    return X, y


def test_stacking_ensemble_evaluation():
    """StackingEnsembleã®è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== StackingEnsembleè©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        X, y = create_simple_test_data(n_samples=30, n_features=3, n_classes=3)
        
        # StackingEnsembleã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        ensemble = StackingEnsemble()
        
        # _evaluate_predictionsãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãƒ†ã‚¹ãƒˆ
        y_pred = np.random.choice([0, 1, 2], size=len(y))
        y_pred_proba = np.random.dirichlet([1, 1, 1], size=len(y))
        
        metrics = ensemble._evaluate_predictions(y, y_pred, y_pred_proba)
        
        # çµæœç¢ºèª
        assert "accuracy" in metrics, "accuracyæŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        assert "balanced_accuracy" in metrics, "balanced_accuracyæŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        assert "f1_score" in metrics, "f1_scoreæŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        
        logger.info(f"âœ… StackingEnsembleè©•ä¾¡æˆåŠŸ")
        logger.info(f"   accuracy: {metrics.get('accuracy', 0):.4f}")
        logger.info(f"   balanced_accuracy: {metrics.get('balanced_accuracy', 0):.4f}")
        logger.info(f"   è©•ä¾¡æŒ‡æ¨™æ•°: {len(metrics)}")
        
        return True
        
    except Exception as e:
        logger.error(f"StackingEnsembleè©•ä¾¡ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_bagging_ensemble_evaluation():
    """BaggingEnsembleã®è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== BaggingEnsembleè©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        X, y = create_simple_test_data(n_samples=30, n_features=3, n_classes=3)
        
        # BaggingEnsembleã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        ensemble = BaggingEnsemble()
        
        # _evaluate_predictionsãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãƒ†ã‚¹ãƒˆ
        y_pred = np.random.choice([0, 1, 2], size=len(y))
        y_pred_proba = np.random.dirichlet([1, 1, 1], size=len(y))
        
        metrics = ensemble._evaluate_predictions(y, y_pred, y_pred_proba)
        
        # çµæœç¢ºèª
        assert "accuracy" in metrics, "accuracyæŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        assert "balanced_accuracy" in metrics, "balanced_accuracyæŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        assert "f1_score" in metrics, "f1_scoreæŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        
        logger.info(f"âœ… BaggingEnsembleè©•ä¾¡æˆåŠŸ")
        logger.info(f"   accuracy: {metrics.get('accuracy', 0):.4f}")
        logger.info(f"   balanced_accuracy: {metrics.get('balanced_accuracy', 0):.4f}")
        logger.info(f"   è©•ä¾¡æŒ‡æ¨™æ•°: {len(metrics)}")
        
        return True
        
    except Exception as e:
        logger.error(f"BaggingEnsembleè©•ä¾¡ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_evaluation_consistency():
    """è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        # åŒã˜ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è¤‡æ•°ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
        X, y = create_simple_test_data(n_samples=30, n_features=3, n_classes=3)
        
        # åŒã˜äºˆæ¸¬çµæœã‚’ä½¿ç”¨
        y_pred = np.random.choice([0, 1, 2], size=len(y))
        y_pred_proba = np.random.dirichlet([1, 1, 1], size=len(y))
        
        # è¤‡æ•°ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§è©•ä¾¡
        stacking = StackingEnsemble()
        bagging = BaggingEnsemble()
        
        stacking_metrics = stacking._evaluate_predictions(y, y_pred, y_pred_proba)
        bagging_metrics = bagging._evaluate_predictions(y, y_pred, y_pred_proba)
        
        # åŒã˜äºˆæ¸¬çµæœãªã®ã§ã€è©•ä¾¡æŒ‡æ¨™ã‚‚åŒã˜ã«ãªã‚‹ã¯ãš
        assert abs(stacking_metrics['accuracy'] - bagging_metrics['accuracy']) < 1e-10, "accuracyæŒ‡æ¨™ãŒä¸€è‡´ã—ã¾ã›ã‚“"
        assert abs(stacking_metrics['balanced_accuracy'] - bagging_metrics['balanced_accuracy']) < 1e-10, "balanced_accuracyæŒ‡æ¨™ãŒä¸€è‡´ã—ã¾ã›ã‚“"
        
        logger.info("âœ… è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ä¸€è²«æ€§ç¢ºèªæˆåŠŸ")
        logger.info(f"   Stacking accuracy: {stacking_metrics['accuracy']:.6f}")
        logger.info(f"   Bagging accuracy: {bagging_metrics['accuracy']:.6f}")
        logger.info(f"   å·®åˆ†: {abs(stacking_metrics['accuracy'] - bagging_metrics['accuracy']):.10f}")
        
        return True
        
    except Exception as e:
        logger.error(f"è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("=" * 60)
    logger.info("ğŸš€ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    logger.info("=" * 60)
    
    results = {}
    
    try:
        # 1. StackingEnsembleè©•ä¾¡ãƒ†ã‚¹ãƒˆ
        results['stacking'] = test_stacking_ensemble_evaluation()
        
        # 2. BaggingEnsembleè©•ä¾¡ãƒ†ã‚¹ãƒˆ
        results['bagging'] = test_bagging_ensemble_evaluation()
        
        # 3. è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ
        results['consistency'] = test_evaluation_consistency()
        
        # çµæœã‚µãƒãƒªãƒ¼
        logger.info("=" * 60)
        logger.info("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        logger.info("=" * 60)
        
        success_count = sum(results.values())
        total_count = len(results)
        
        for test_name, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
            logger.info(f"{test_name}: {status}")
        
        logger.info(f"æˆåŠŸç‡: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
        
        if success_count == total_count:
            logger.info("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯çµ±ä¸€ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
            return True
        else:
            logger.warning(f"âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚({total_count-success_count}å€‹ã®å¤±æ•—)")
            return False
            
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
