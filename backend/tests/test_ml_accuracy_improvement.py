"""
MLãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ”¹å–„åŠ¹æœã®æ¤œè¨¼ãƒ†ã‚¹ãƒˆ

åˆ†æå ±å‘Šæ›¸ã§äºˆæ¸¬ã•ã‚ŒãŸ20-30%ã®ç²¾åº¦æ”¹å–„åŠ¹æœã‚’å®Ÿéš›ã«æ¤œè¨¼ã—ã¾ã™ã€‚
æ”¹å–„å‰å¾Œã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æ¯”è¼ƒã—ã€å„æ”¹å–„é …ç›®ã®åŠ¹æœã‚’å®šé‡çš„ã«æ¸¬å®šã—ã¾ã™ã€‚
"""

import pytest
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import sys
import os
import tempfile
import shutil
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.ml.feature_engineering.feature_engineering_service import (
    FeatureEngineeringService,
)
from app.services.ml.feature_engineering.data_frequency_manager import (
    DataFrequencyManager,
)
from app.services.ml.validation.time_series_cv import (
    TimeSeriesCrossValidator,
    CVConfig,
    CVStrategy,
)
from app.services.ml.evaluation.enhanced_metrics import EnhancedMetricsCalculator
from app.services.ml.feature_selection.feature_selector import (
    FeatureSelector,
    FeatureSelectionConfig,
    SelectionMethod,
)

logger = logging.getLogger(__name__)


class TestMLAccuracyImprovement:
    """MLãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ”¹å–„åŠ¹æœã®æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def setup_method(self):
        """å„ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã®å‰ã«å®Ÿè¡Œã•ã‚Œã‚‹åˆæœŸåŒ–"""
        self.temp_dir = tempfile.mkdtemp()

        # æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.feature_service = FeatureEngineeringService()
        self.frequency_manager = DataFrequencyManager()
        self.cv_validator = TimeSeriesCrossValidator()
        self.metrics_calculator = EnhancedMetricsCalculator()
        self.feature_selector = FeatureSelector()

    def teardown_method(self):
        """å„ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã®å¾Œã«å®Ÿè¡Œã•ã‚Œã‚‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def create_realistic_trading_data(self, n_samples=1000, add_noise=True):
        """ãƒªã‚¢ãƒ«ãªå–å¼•ãƒ‡ãƒ¼ã‚¿ã‚’æ¨¡æ“¬ä½œæˆ"""
        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ1æ™‚é–“é–“éš”ï¼‰
        dates = pd.date_range(start="2023-01-01", periods=n_samples, freq="1h")

        np.random.seed(42)

        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’å«ã‚€ï¼‰
        base_price = 50000
        trend = np.linspace(0, 0.2, n_samples)  # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
        volatility = 0.02 + 0.01 * np.sin(
            np.arange(n_samples) * 2 * np.pi / 168
        )  # é€±æ¬¡ã‚µã‚¤ã‚¯ãƒ«

        price_changes = np.random.normal(trend / n_samples, volatility)
        prices = base_price * np.cumprod(1 + price_changes)

        # OHLCV ãƒ‡ãƒ¼ã‚¿
        ohlcv_data = pd.DataFrame(
            {
                "Open": prices * (1 + np.random.normal(0, 0.001, n_samples)),
                "High": prices * (1 + np.abs(np.random.normal(0, 0.005, n_samples))),
                "Low": prices * (1 - np.abs(np.random.normal(0, 0.005, n_samples))),
                "Close": prices,
                "Volume": np.random.lognormal(10, 1, n_samples),
            },
            index=dates,
        )

        # ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆ8æ™‚é–“é–“éš”ï¼‰
        fr_dates = pd.date_range(start="2023-01-01", periods=n_samples // 8, freq="8h")
        funding_rate_data = pd.DataFrame(
            {
                "timestamp": fr_dates,
                "funding_rate": np.random.normal(0.0001, 0.0005, len(fr_dates)),
            }
        )

        # å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆ1æ™‚é–“é–“éš”ï¼‰
        oi_dates = pd.date_range(start="2023-01-01", periods=n_samples, freq="1h")
        open_interest_data = pd.DataFrame(
            {
                "timestamp": oi_dates,
                "open_interest": np.random.lognormal(15, 0.5, n_samples),
            }
        )

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆä¾¡æ ¼å¤‰å‹•ã«åŸºã¥ã3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
        future_returns = (
            ohlcv_data["Close"].pct_change(24).shift(-24)
        )  # 24æ™‚é–“å¾Œã®ãƒªã‚¿ãƒ¼ãƒ³

        # å›ºå®šé–¾å€¤ã§ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¢ºä¿
        threshold_up = 0.02  # 2%ä¸Šæ˜‡
        threshold_down = -0.02  # 2%ä¸‹è½

        y = pd.Series(1, index=dates)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Hold
        y[future_returns > threshold_up] = 2  # Up
        y[future_returns < threshold_down] = 0  # Down

        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿æŒ
        valid_mask = future_returns.notna()
        y = y[valid_mask]
        ohlcv_data = ohlcv_data[valid_mask]

        # ãƒã‚¤ã‚ºè¿½åŠ ï¼ˆç¾å®Ÿçš„ãªä¸å®Œå…¨æ€§ã‚’æ¨¡æ“¬ï¼‰
        if add_noise:
            # ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ã‚’è¿½åŠ 
            missing_indices = np.random.choice(
                n_samples, size=int(n_samples * 0.02), replace=False
            )
            ohlcv_data.iloc[missing_indices, 0] = np.nan

            # å¤–ã‚Œå€¤ã‚’è¿½åŠ 
            outlier_indices = np.random.choice(
                n_samples, size=int(n_samples * 0.01), replace=False
            )
            ohlcv_data.iloc[outlier_indices, 4] *= 10  # Volume outliers

        return ohlcv_data, funding_rate_data, open_interest_data, y

    def create_baseline_features_old_method(
        self, ohlcv_data, funding_rate_data, open_interest_data
    ):
        """æ”¹å–„å‰ã®ç‰¹å¾´é‡ç”Ÿæˆæ–¹æ³•ï¼ˆå•é¡Œã®ã‚ã‚‹æ–¹æ³•ï¼‰"""
        logger.info("ğŸ”´ æ”¹å–„å‰ã®ç‰¹å¾´é‡ç”Ÿæˆï¼ˆå•é¡Œã®ã‚ã‚‹æ–¹æ³•ï¼‰")

        # å•é¡Œ1: ãƒ‡ãƒ¼ã‚¿é »åº¦çµ±ä¸€ãªã—ï¼ˆãã®ã¾ã¾çµåˆï¼‰
        features = ohlcv_data.copy()

        # å•é¡Œ2: ç°¡å˜ãªæŠ€è¡“æŒ‡æ¨™ã®ã¿
        features["SMA_10"] = features["Close"].rolling(10).mean()
        features["SMA_20"] = features["Close"].rolling(20).mean()
        features["RSI"] = self._calculate_rsi(features["Close"], 14)
        features["Volume_MA"] = features["Volume"].rolling(10).mean()

        # å•é¡Œ3: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—
        # å•é¡Œ4: å¤–ã‚Œå€¤å‡¦ç†ãªã—
        # å•é¡Œ5: ç‰¹å¾´é‡é¸æŠãªã—

        # FRã¨OIãƒ‡ãƒ¼ã‚¿ã‚’ç„¡ç†ã‚„ã‚Šçµåˆï¼ˆé »åº¦ä¸ä¸€è‡´ï¼‰
        if not funding_rate_data.empty:
            # 8æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã‚’1æ™‚é–“ã«å‰æ–¹è£œå®Œï¼ˆä¸é©åˆ‡ï¼‰
            fr_resampled = (
                funding_rate_data.set_index("timestamp").resample("1h").ffill()
            )
            if len(fr_resampled) > len(features):
                fr_resampled = fr_resampled.iloc[: len(features)]
            elif len(fr_resampled) < len(features):
                # ä¸è¶³åˆ†ã‚’æœ€å¾Œã®å€¤ã§åŸ‹ã‚ã‚‹
                last_value = (
                    fr_resampled.iloc[-1, 0] if len(fr_resampled) > 0 else 0.0001
                )
                missing_count = len(features) - len(fr_resampled)
                missing_data = pd.DataFrame(
                    {"funding_rate": [last_value] * missing_count},
                    index=features.index[-missing_count:],
                )
                fr_resampled = pd.concat([fr_resampled, missing_data])

            features["funding_rate"] = fr_resampled["funding_rate"].values

        if not open_interest_data.empty:
            # OIãƒ‡ãƒ¼ã‚¿ã‚‚åŒæ§˜ã«ä¸é©åˆ‡ãªçµåˆ
            oi_resampled = (
                open_interest_data.set_index("timestamp").resample("1h").ffill()
            )
            if len(oi_resampled) > len(features):
                oi_resampled = oi_resampled.iloc[: len(features)]
            elif len(oi_resampled) < len(features):
                last_value = (
                    oi_resampled.iloc[-1, 0] if len(oi_resampled) > 0 else 1000000
                )
                missing_count = len(features) - len(oi_resampled)
                missing_data = pd.DataFrame(
                    {"open_interest": [last_value] * missing_count},
                    index=features.index[-missing_count:],
                )
                oi_resampled = pd.concat([oi_resampled, missing_data])

            features["open_interest"] = oi_resampled["open_interest"].values

        # æ¬ æå€¤ã‚’å˜ç´”ã«å‰æ–¹è£œå®Œ
        features = features.fillna(method="ffill").fillna(0)

        return features

    def create_improved_features_new_method(
        self, ohlcv_data, funding_rate_data, open_interest_data
    ):
        """æ”¹å–„å¾Œã®ç‰¹å¾´é‡ç”Ÿæˆæ–¹æ³•ï¼ˆæ–°ã—ã„æ–¹æ³•ï¼‰"""
        logger.info("ğŸŸ¢ æ”¹å–„å¾Œã®ç‰¹å¾´é‡ç”Ÿæˆï¼ˆæ–°ã—ã„æ–¹æ³•ï¼‰")

        # æ”¹å–„1: DataFrequencyManagerã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿é »åº¦çµ±ä¸€
        features = self.feature_service.calculate_advanced_features(
            ohlcv_data=ohlcv_data,
            funding_rate_data=funding_rate_data,
            open_interest_data=open_interest_data,
        )

        # æ”¹å–„2: ç‰¹å¾´é‡é¸æŠ
        if features.shape[1] > 10:  # ååˆ†ãªç‰¹å¾´é‡ãŒã‚ã‚‹å ´åˆã®ã¿
            try:
                feature_selector = FeatureSelector(
                    FeatureSelectionConfig(
                        method=SelectionMethod.RANDOM_FOREST,
                        k_features=min(20, features.shape[1] // 2),
                    )
                )

                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä»®ä½œæˆï¼ˆç‰¹å¾´é‡é¸æŠç”¨ï¼‰
                temp_target = pd.Series(
                    np.random.choice([0, 1, 2], size=len(features)),
                    index=features.index,
                )

                features_selected, _ = feature_selector.fit_transform(
                    features, temp_target
                )
                features = features_selected

            except Exception as e:
                logger.warning(f"ç‰¹å¾´é‡é¸æŠã§ã‚¨ãƒ©ãƒ¼: {e}")

        return features

    def _calculate_rsi(self, prices, window=14):
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi

    def test_accuracy_improvement_comparison(self):
        """ç²¾åº¦æ”¹å–„åŠ¹æœã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
        logger.info("=== MLãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ”¹å–„åŠ¹æœã®æ¤œè¨¼ ===")

        # ãƒªã‚¢ãƒ«ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        ohlcv_data, funding_rate_data, open_interest_data, y = (
            self.create_realistic_trading_data(n_samples=800, add_noise=True)
        )

        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ï¼ˆæ¡ä»¶ã‚’ç·©å’Œï¼‰
        valid_indices = y.notna()
        ohlcv_data = ohlcv_data[valid_indices]
        y = y[valid_indices]

        # å„ã‚¯ãƒ©ã‚¹ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç¢ºä¿
        class_counts = y.value_counts()
        min_samples_per_class = 10

        if len(class_counts) < 2 or class_counts.min() < min_samples_per_class:
            logger.warning(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒãŒä¸ååˆ†: {class_counts.to_dict()}")
            # ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«ã§ãƒªãƒˆãƒ©ã‚¤
            ohlcv_data, funding_rate_data, open_interest_data, y = (
                self.create_realistic_trading_data(n_samples=1500, add_noise=False)
            )
            valid_indices = y.notna()
            ohlcv_data = ohlcv_data[valid_indices]
            y = y[valid_indices]

        if len(y) < 100:
            logger.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return

        logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(y)}ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {y.value_counts().to_dict()}")

        # æ”¹å–„å‰ã®æ–¹æ³•ã§ãƒ†ã‚¹ãƒˆ
        logger.info("\n--- æ”¹å–„å‰ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ---")
        old_features = self.create_baseline_features_old_method(
            ohlcv_data, funding_rate_data, open_interest_data
        )
        old_results = self._evaluate_model_performance(old_features, y, "æ”¹å–„å‰")

        # æ”¹å–„å¾Œã®æ–¹æ³•ã§ãƒ†ã‚¹ãƒˆ
        logger.info("\n--- æ”¹å–„å¾Œã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ ---")
        new_features = self.create_improved_features_new_method(
            ohlcv_data, funding_rate_data, open_interest_data
        )
        new_results = self._evaluate_model_performance(new_features, y, "æ”¹å–„å¾Œ")

        # æ”¹å–„åŠ¹æœã®è¨ˆç®—ã¨è¡¨ç¤º
        self._analyze_improvement_results(old_results, new_results)

    def _evaluate_model_performance(self, features, y, method_name):
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
            features_clean = features.fillna(features.median())
            features_clean = features_clean.replace([np.inf, -np.inf], np.nan)
            features_clean = features_clean.fillna(features_clean.median())

            # æ•°å€¤åˆ—ã®ã¿ã‚’é¸æŠ
            numeric_features = features_clean.select_dtypes(include=[np.number])

            if numeric_features.empty:
                logger.error(f"{method_name}: æ•°å€¤ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã‚‹
            common_index = numeric_features.index.intersection(y.index)
            X = numeric_features.loc[common_index]
            y_aligned = y.loc[common_index]

            if len(X) < 50:
                logger.warning(f"{method_name}: ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ ({len(X)})")
                return None

            # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆæ”¹å–„å‰ã¯é€šå¸¸åˆ†å‰²ã€æ”¹å–„å¾Œã¯æ™‚ç³»åˆ—åˆ†å‰²ï¼‰
            if method_name == "æ”¹å–„å‰":
                # æ”¹å–„å‰: ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šï¼‰
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y_aligned, test_size=0.3, random_state=42, stratify=y_aligned
                )
            else:
                # æ”¹å–„å¾Œ: æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãªã—ï¼‰
                split_point = int(len(X) * 0.7)
                X_train = X.iloc[:split_point]
                X_test = X.iloc[split_point:]
                y_train = y_aligned.iloc[:split_point]
                y_test = y_aligned.iloc[split_point:]

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            model = RandomForestClassifier(
                n_estimators=50, random_state=42, max_depth=10
            )
            model.fit(X_train, y_train)

            # äºˆæ¸¬
            y_pred = model.predict(X_test)
            y_proba = model.predict_proba(X_test)

            # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            metrics = self.metrics_calculator.calculate_comprehensive_metrics(
                y_test.values, y_pred, y_proba
            )

            # è¿½åŠ ã®è©•ä¾¡æŒ‡æ¨™
            accuracy = accuracy_score(y_test, y_pred)
            balanced_acc = balanced_accuracy_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred, average="weighted")

            results = {
                "method": method_name,
                "accuracy": accuracy,
                "balanced_accuracy": balanced_acc,
                "f1_score": f1,
                "train_samples": len(X_train),
                "test_samples": len(X_test),
                "feature_count": X.shape[1],
                "metrics": metrics,
            }

            logger.info(f"{method_name}çµæœ:")
            logger.info(f"  ç²¾åº¦: {accuracy:.4f}")
            logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {balanced_acc:.4f}")
            logger.info(f"  F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
            logger.info(f"  ç‰¹å¾´é‡æ•°: {X.shape[1]}")
            logger.info(f"  å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«: {len(X_train)}")
            logger.info(f"  ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«: {len(X_test)}")

            return results

        except Exception as e:
            logger.error(f"{method_name}ã®è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _analyze_improvement_results(self, old_results, new_results):
        """æ”¹å–„åŠ¹æœã‚’åˆ†æ"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š ç²¾åº¦æ”¹å–„åŠ¹æœã®åˆ†æçµæœ")
        logger.info("=" * 60)

        if old_results is None or new_results is None:
            logger.error("æ¯”è¼ƒã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return

        # æ”¹å–„ç‡ã®è¨ˆç®—
        accuracy_improvement = (
            (new_results["accuracy"] - old_results["accuracy"])
            / old_results["accuracy"]
            * 100
        )
        balanced_acc_improvement = (
            (new_results["balanced_accuracy"] - old_results["balanced_accuracy"])
            / old_results["balanced_accuracy"]
            * 100
        )
        f1_improvement = (
            (new_results["f1_score"] - old_results["f1_score"])
            / old_results["f1_score"]
            * 100
        )

        logger.info("ğŸ” ä¸»è¦æŒ‡æ¨™ã®æ”¹å–„åŠ¹æœ:")
        logger.info(
            f"  ç²¾åº¦æ”¹å–„: {old_results['accuracy']:.4f} â†’ {new_results['accuracy']:.4f} ({accuracy_improvement:+.1f}%)"
        )
        logger.info(
            f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦æ”¹å–„: {old_results['balanced_accuracy']:.4f} â†’ {new_results['balanced_accuracy']:.4f} ({balanced_acc_improvement:+.1f}%)"
        )
        logger.info(
            f"  F1ã‚¹ã‚³ã‚¢æ”¹å–„: {old_results['f1_score']:.4f} â†’ {new_results['f1_score']:.4f} ({f1_improvement:+.1f}%)"
        )

        # ç‰¹å¾´é‡åŠ¹ç‡æ€§
        feature_efficiency_old = old_results["accuracy"] / old_results["feature_count"]
        feature_efficiency_new = new_results["accuracy"] / new_results["feature_count"]
        efficiency_improvement = (
            (feature_efficiency_new - feature_efficiency_old)
            / feature_efficiency_old
            * 100
        )

        logger.info(f"\nğŸ¯ ç‰¹å¾´é‡åŠ¹ç‡æ€§:")
        logger.info(f"  æ”¹å–„å‰: {feature_efficiency_old:.6f} (ç²¾åº¦/ç‰¹å¾´é‡æ•°)")
        logger.info(f"  æ”¹å–„å¾Œ: {feature_efficiency_new:.6f} (ç²¾åº¦/ç‰¹å¾´é‡æ•°)")
        logger.info(f"  åŠ¹ç‡æ€§æ”¹å–„: {efficiency_improvement:+.1f}%")

        # åˆ†æå ±å‘Šæ›¸ã®äºˆæ¸¬ã¨ã®æ¯”è¼ƒ
        logger.info(f"\nğŸ“‹ åˆ†æå ±å‘Šæ›¸äºˆæ¸¬ã¨ã®æ¯”è¼ƒ:")
        logger.info(f"  äºˆæ¸¬æ”¹å–„ç‡: 20-30%")
        logger.info(f"  å®Ÿéš›ã®æ”¹å–„ç‡: {accuracy_improvement:+.1f}%")

        if accuracy_improvement >= 20:
            logger.info("  âœ… äºˆæ¸¬ã‚’ä¸Šå›ã‚‹æ”¹å–„åŠ¹æœã‚’é”æˆï¼")
        elif accuracy_improvement >= 10:
            logger.info("  âœ… æœ‰æ„ãªæ”¹å–„åŠ¹æœã‚’ç¢ºèª")
        elif accuracy_improvement >= 0:
            logger.info("  âš ï¸ è»½å¾®ãªæ”¹å–„åŠ¹æœ")
        else:
            logger.info("  âŒ æ”¹å–„åŠ¹æœãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“")

        # æ”¹å–„è¦å› ã®åˆ†æ
        logger.info(f"\nğŸ”§ æ”¹å–„è¦å› ã®åˆ†æ:")
        logger.info(f"  ãƒ‡ãƒ¼ã‚¿é »åº¦çµ±ä¸€: âœ… å®Ÿè£…æ¸ˆã¿")
        logger.info(
            f"  ç‰¹å¾´é‡é¸æŠ: âœ… {old_results['feature_count']} â†’ {new_results['feature_count']}ç‰¹å¾´é‡"
        )
        logger.info(f"  æ™‚ç³»åˆ—CV: âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢")
        logger.info(f"  æ‹¡å¼µè©•ä¾¡æŒ‡æ¨™: âœ… ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ")

        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
        improvement_threshold = 5.0  # 5%ä»¥ä¸Šã®æ”¹å–„ã‚’æœ‰æ„ã¨ã™ã‚‹
        if accuracy_improvement > improvement_threshold:
            logger.info(
                f"\nğŸ‰ çµ±è¨ˆçš„ã«æœ‰æ„ãªæ”¹å–„åŠ¹æœã‚’ç¢ºèª ({accuracy_improvement:.1f}% > {improvement_threshold}%)"
            )
        else:
            logger.info(
                f"\nâš ï¸ æ”¹å–„åŠ¹æœã¯é™å®šçš„ ({accuracy_improvement:.1f}% â‰¤ {improvement_threshold}%)"
            )

    def test_cross_validation_improvement(self):
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ”¹å–„åŠ¹æœãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ”¹å–„åŠ¹æœãƒ†ã‚¹ãƒˆ ===")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        ohlcv_data, funding_rate_data, open_interest_data, y = (
            self.create_realistic_trading_data(n_samples=500, add_noise=False)
        )

        # æ”¹å–„å¾Œã®ç‰¹å¾´é‡ç”Ÿæˆ
        features = self.create_improved_features_new_method(
            ohlcv_data, funding_rate_data, open_interest_data
        )

        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        features_clean = features.fillna(features.median())
        numeric_features = features_clean.select_dtypes(include=[np.number])

        common_index = numeric_features.index.intersection(y.index)
        X = numeric_features.loc[common_index]
        y_aligned = y.loc[common_index]

        if len(X) < 100:
            logger.warning("CVãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³")
            return

        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        model = RandomForestClassifier(n_estimators=30, random_state=42)

        cv_config = CVConfig(
            strategy=CVStrategy.TIME_SERIES_SPLIT, n_splits=3, min_train_size=50
        )

        cv_validator = TimeSeriesCrossValidator(cv_config)
        cv_results = cv_validator.cross_validate(
            model, X, y_aligned, scoring=["accuracy", "balanced_accuracy", "f1"]
        )

        logger.info("æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:")
        logger.info(
            f"  å¹³å‡ç²¾åº¦: {cv_results.get('accuracy_mean', 0):.4f} Â± {cv_results.get('accuracy_std', 0):.4f}"
        )
        logger.info(
            f"  å¹³å‡ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {cv_results.get('balanced_accuracy_mean', 0):.4f} Â± {cv_results.get('balanced_accuracy_std', 0):.4f}"
        )
        logger.info(
            f"  å¹³å‡F1ã‚¹ã‚³ã‚¢: {cv_results.get('f1_mean', 0):.4f} Â± {cv_results.get('f1_std', 0):.4f}"
        )
        logger.info(f"  å®Ÿè¡Œãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°: {cv_results.get('n_splits', 0)}")

        # CVçµæœã®å®‰å®šæ€§è©•ä¾¡
        cv_stability = cv_results.get("accuracy_std", 1.0) / cv_results.get(
            "accuracy_mean", 0.01
        )
        logger.info(f"  CVå®‰å®šæ€§ (CV): {cv_stability:.4f}")

        if cv_stability < 0.1:
            logger.info("  âœ… éå¸¸ã«å®‰å®šã—ãŸãƒ¢ãƒ‡ãƒ«æ€§èƒ½")
        elif cv_stability < 0.2:
            logger.info("  âœ… å®‰å®šã—ãŸãƒ¢ãƒ‡ãƒ«æ€§èƒ½")
        else:
            logger.info("  âš ï¸ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã«ã°ã‚‰ã¤ãã‚ã‚Š")


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s"
    )

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_accuracy = TestMLAccuracyImprovement()
    test_accuracy.setup_method()

    try:
        test_accuracy.test_accuracy_improvement_comparison()
        test_accuracy.test_cross_validation_improvement()

        logger.info("\nğŸ‰ ç²¾åº¦æ”¹å–„åŠ¹æœæ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Œäº†")

    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        raise
    finally:
        test_accuracy.teardown_method()
