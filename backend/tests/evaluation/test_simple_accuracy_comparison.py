"""
ã‚·ãƒ³ãƒ—ãƒ«ãªMLãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¯”è¼ƒãƒ†ã‚¹ãƒˆ

æ”¹å–„å‰å¾Œã®MLãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ç›´æ¥æ¯”è¼ƒã—ã€
åˆ†æå ±å‘Šæ›¸ã§äºˆæ¸¬ã•ã‚ŒãŸæ”¹å–„åŠ¹æœã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import pandas as pd
import numpy as np
import logging
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score
from sklearn.preprocessing import RobustScaler
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logger = logging.getLogger(__name__)


class SimpleAccuracyComparison:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªMLãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¯”è¼ƒã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.results = {}

    def create_trading_dataset(self, n_samples=1000):
        """å–å¼•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        np.random.seed(42)
        
        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        dates = pd.date_range(start='2023-01-01', periods=n_samples, freq='1h')
        
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒªã‚¢ãƒ«ãªä¾¡æ ¼å‹•å‘ã‚’æ¨¡æ“¬ï¼‰
        base_price = 50000
        trend = np.cumsum(np.random.normal(0, 0.001, n_samples))
        noise = np.random.normal(0, 0.02, n_samples)
        price_changes = trend + noise
        prices = base_price * np.cumprod(1 + price_changes)
        
        # åŸºæœ¬çš„ãªOHLCVãƒ‡ãƒ¼ã‚¿
        data = pd.DataFrame({
            'Open': prices * (1 + np.random.normal(0, 0.001, n_samples)),
            'High': prices * (1 + np.abs(np.random.normal(0, 0.005, n_samples))),
            'Low': prices * (1 - np.abs(np.random.normal(0, 0.005, n_samples))),
            'Close': prices,
            'Volume': np.random.lognormal(10, 1, n_samples),
        }, index=dates)
        
        # æŠ€è¡“æŒ‡æ¨™ã‚’è¿½åŠ 
        data['SMA_10'] = data['Close'].rolling(10).mean()
        data['SMA_20'] = data['Close'].rolling(20).mean()
        data['RSI'] = self._calculate_rsi(data['Close'])
        data['MACD'] = self._calculate_macd(data['Close'])
        data['BB_upper'], data['BB_lower'] = self._calculate_bollinger_bands(data['Close'])
        data['Volume_SMA'] = data['Volume'].rolling(10).mean()
        
        # ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆ8æ™‚é–“é–“éš”ã‚’1æ™‚é–“ã«è£œé–“ï¼‰
        fr_8h = np.random.normal(0.0001, 0.0005, n_samples // 8)
        fr_1h = np.repeat(fr_8h, 8)[:n_samples]
        data['Funding_Rate'] = fr_1h
        
        # å»ºç‰æ®‹é«˜ï¼ˆ1æ™‚é–“é–“éš”ï¼‰
        data['Open_Interest'] = np.random.lognormal(15, 0.5, n_samples)
        data['OI_Change'] = data['Open_Interest'].pct_change()
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆ24æ™‚é–“å¾Œã®ä¾¡æ ¼å¤‰å‹•ï¼‰
        future_returns = data['Close'].pct_change(24).shift(-24)
        
        # 3ã‚¯ãƒ©ã‚¹åˆ†é¡
        y = pd.Series(1, index=dates)  # Hold
        y[future_returns > 0.02] = 2   # Up (2%ä»¥ä¸Šä¸Šæ˜‡)
        y[future_returns < -0.02] = 0  # Down (2%ä»¥ä¸Šä¸‹è½)
        
        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿
        valid_mask = data.notna().all(axis=1) & y.notna()
        data = data[valid_mask]
        y = y[valid_mask]
        
        return data, y

    def _calculate_rsi(self, prices, window=14):
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.fillna(50)

    def _calculate_macd(self, prices, fast=12, slow=26):
        """MACDè¨ˆç®—"""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        return macd.fillna(0)

    def _calculate_bollinger_bands(self, prices, window=20, num_std=2):
        """ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰è¨ˆç®—"""
        sma = prices.rolling(window).mean()
        std = prices.rolling(window).std()
        upper = sma + (std * num_std)
        lower = sma - (std * num_std)
        return upper.fillna(prices), lower.fillna(prices)

    def test_baseline_model(self, X, y):
        """æ”¹å–„å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«"""
        logger.info("ğŸ”´ æ”¹å–„å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«")
        
        # å•é¡Œ1: ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šï¼‰
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # å•é¡Œ2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—
        # å•é¡Œ3: ç‰¹å¾´é‡é¸æŠãªã—
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=8)
        model.fit(X_train, y_train)
        
        # äºˆæ¸¬ã¨è©•ä¾¡
        y_pred = model.predict(X_test)
        
        results = {
            'accuracy': accuracy_score(y_test, y_pred),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'features': X.shape[1]
        }
        
        logger.info(f"  ç²¾åº¦: {results['accuracy']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {results['balanced_accuracy']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.4f}")
        logger.info(f"  ç‰¹å¾´é‡æ•°: {results['features']}")
        
        return results

    def test_improved_model(self, X, y):
        """æ”¹å–„å¾Œã®ãƒ¢ãƒ‡ãƒ«"""
        logger.info("ğŸŸ¢ æ”¹å–„å¾Œã®ãƒ¢ãƒ‡ãƒ«")
        
        # æ”¹å–„1: æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
        split_point = int(len(X) * 0.7)
        X_train = X.iloc[:split_point]
        X_test = X.iloc[split_point:]
        y_train = y.iloc[:split_point]
        y_test = y.iloc[split_point:]
        
        # æ”¹å–„2: RobustScaleré©ç”¨
        scaler = RobustScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )
        
        # æ”¹å–„3: ç‰¹å¾´é‡é¸æŠï¼ˆé‡è¦åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        temp_model = RandomForestClassifier(n_estimators=30, random_state=42)
        temp_model.fit(X_train_scaled, y_train)
        
        # é‡è¦åº¦ä¸Šä½ã®ç‰¹å¾´é‡ã‚’é¸æŠ
        feature_importance = pd.Series(
            temp_model.feature_importances_,
            index=X_train_scaled.columns
        ).sort_values(ascending=False)
        
        top_features = feature_importance.head(min(15, len(feature_importance))).index
        X_train_selected = X_train_scaled[top_features]
        X_test_selected = X_test_scaled[top_features]
        
        # æ”¹å–„4: ã‚ˆã‚Šè‰¯ã„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            max_depth=12,
            min_samples_split=5,
            min_samples_leaf=2
        )
        model.fit(X_train_selected, y_train)
        
        # äºˆæ¸¬ã¨è©•ä¾¡
        y_pred = model.predict(X_test_selected)
        
        results = {
            'accuracy': accuracy_score(y_test, y_pred),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'features': len(top_features),
            'selected_features': top_features.tolist()
        }
        
        logger.info(f"  ç²¾åº¦: {results['accuracy']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {results['balanced_accuracy']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.4f}")
        logger.info(f"  ç‰¹å¾´é‡æ•°: {results['features']} (é¸æŠå¾Œ)")
        
        return results

    def run_comparison(self):
        """æ¯”è¼ƒãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š MLãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ”¹å–„åŠ¹æœã®æ¤œè¨¼")
        logger.info("=" * 60)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        X, y = self.create_trading_dataset(n_samples=1200)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(X)}ã‚µãƒ³ãƒ—ãƒ«, {X.shape[1]}ç‰¹å¾´é‡")
        logger.info(f"ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
        baseline_results = self.test_baseline_model(X, y)
        
        # æ”¹å–„ãƒ¢ãƒ‡ãƒ«
        improved_results = self.test_improved_model(X, y)
        
        # æ”¹å–„åŠ¹æœã®åˆ†æ
        self._analyze_improvement(baseline_results, improved_results)
        
        return baseline_results, improved_results

    def _analyze_improvement(self, baseline, improved):
        """æ”¹å–„åŠ¹æœã‚’åˆ†æ"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“ˆ æ”¹å–„åŠ¹æœã®åˆ†æ")
        logger.info("=" * 60)
        
        # æ”¹å–„ç‡è¨ˆç®—
        accuracy_improvement = (improved['accuracy'] - baseline['accuracy']) / baseline['accuracy'] * 100
        balanced_acc_improvement = (improved['balanced_accuracy'] - baseline['balanced_accuracy']) / baseline['balanced_accuracy'] * 100
        f1_improvement = (improved['f1_score'] - baseline['f1_score']) / baseline['f1_score'] * 100
        
        logger.info("ğŸ¯ ä¸»è¦æŒ‡æ¨™ã®æ”¹å–„åŠ¹æœ:")
        logger.info(f"  ç²¾åº¦: {baseline['accuracy']:.4f} â†’ {improved['accuracy']:.4f} ({accuracy_improvement:+.1f}%)")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {baseline['balanced_accuracy']:.4f} â†’ {improved['balanced_accuracy']:.4f} ({balanced_acc_improvement:+.1f}%)")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {baseline['f1_score']:.4f} â†’ {improved['f1_score']:.4f} ({f1_improvement:+.1f}%)")
        
        # ç‰¹å¾´é‡åŠ¹ç‡æ€§
        baseline_efficiency = baseline['accuracy'] / baseline['features']
        improved_efficiency = improved['accuracy'] / improved['features']
        efficiency_improvement = (improved_efficiency - baseline_efficiency) / baseline_efficiency * 100
        
        logger.info(f"\nğŸ”§ ç‰¹å¾´é‡åŠ¹ç‡æ€§:")
        logger.info(f"  æ”¹å–„å‰: {baseline_efficiency:.6f} (ç²¾åº¦/ç‰¹å¾´é‡æ•°)")
        logger.info(f"  æ”¹å–„å¾Œ: {improved_efficiency:.6f} (ç²¾åº¦/ç‰¹å¾´é‡æ•°)")
        logger.info(f"  åŠ¹ç‡æ€§æ”¹å–„: {efficiency_improvement:+.1f}%")
        
        # åˆ†æå ±å‘Šæ›¸ã¨ã®æ¯”è¼ƒ
        logger.info(f"\nğŸ“‹ åˆ†æå ±å‘Šæ›¸äºˆæ¸¬ã¨ã®æ¯”è¼ƒ:")
        logger.info(f"  äºˆæ¸¬æ”¹å–„ç‡: 20-30%")
        logger.info(f"  å®Ÿéš›ã®ç²¾åº¦æ”¹å–„: {accuracy_improvement:+.1f}%")
        logger.info(f"  å®Ÿéš›ã®ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦æ”¹å–„: {balanced_acc_improvement:+.1f}%")
        
        # ç·åˆè©•ä¾¡
        avg_improvement = (accuracy_improvement + balanced_acc_improvement + f1_improvement) / 3
        
        if avg_improvement >= 20:
            logger.info("  âœ… äºˆæ¸¬ã‚’ä¸Šå›ã‚‹å„ªç§€ãªæ”¹å–„åŠ¹æœï¼")
        elif avg_improvement >= 10:
            logger.info("  âœ… æœ‰æ„ãªæ”¹å–„åŠ¹æœã‚’ç¢ºèª")
        elif avg_improvement >= 5:
            logger.info("  âš ï¸ è»½å¾®ãªæ”¹å–„åŠ¹æœ")
        else:
            logger.info("  âŒ æ”¹å–„åŠ¹æœãŒé™å®šçš„")
        
        logger.info(f"\nğŸ† ç·åˆæ”¹å–„ç‡: {avg_improvement:+.1f}%")
        
        # æ”¹å–„è¦å› ã®è©³ç´°
        logger.info(f"\nğŸ” å®Ÿè£…ã•ã‚ŒãŸæ”¹å–„è¦å› :")
        logger.info(f"  âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢: æ™‚ç³»åˆ—åˆ†å‰²æ¡ç”¨")
        logger.info(f"  âœ… ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: RobustScaleré©ç”¨")
        logger.info(f"  âœ… ç‰¹å¾´é‡é¸æŠ: {baseline['features']} â†’ {improved['features']}ç‰¹å¾´é‡")
        logger.info(f"  âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–: å®Ÿæ–½")
        
        return {
            'accuracy_improvement': accuracy_improvement,
            'balanced_accuracy_improvement': balanced_acc_improvement,
            'f1_improvement': f1_improvement,
            'average_improvement': avg_improvement,
            'efficiency_improvement': efficiency_improvement
        }


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s: %(message)s'
    )
    
    # æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    comparison = SimpleAccuracyComparison()
    baseline_results, improved_results = comparison.run_comparison()
    
    logger.info("\nğŸ‰ ç²¾åº¦æ”¹å–„åŠ¹æœæ¤œè¨¼å®Œäº†")
