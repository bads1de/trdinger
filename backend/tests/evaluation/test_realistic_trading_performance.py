"""
ãƒªã‚¢ãƒ«ãªå–å¼•ç’°å¢ƒã§ã®MLãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ

å®Ÿéš›ã®å–å¼•ç’°å¢ƒã«è¿‘ã„æ¡ä»¶ã§MLãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„åŠ¹æœã‚’æ¤œè¨¼ã—ã€
å®Ÿç”¨çš„ãªç²¾åº¦æ”¹å–„ã‚’æ¸¬å®šã—ã¾ã™ã€‚
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    f1_score,
    precision_recall_fscore_support,
)
from sklearn.preprocessing import RobustScaler
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logger = logging.getLogger(__name__)


class RealisticTradingPerformanceTest:
    """ãƒªã‚¢ãƒ«ãªå–å¼•ç’°å¢ƒã§ã®MLãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.results = {}

    def create_realistic_market_data(self, n_samples=2000):
        """ãƒªã‚¢ãƒ«ãªå¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        np.random.seed(42)

        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ1æ™‚é–“é–“éš”ï¼‰
        dates = pd.date_range(start="2023-01-01", periods=n_samples, freq="1h")

        # ã‚ˆã‚Šç¾å®Ÿçš„ãªä¾¡æ ¼å‹•å‘ï¼ˆè¤‡æ•°ã®å¸‚å ´ã‚µã‚¤ã‚¯ãƒ«ã‚’å«ã‚€ï¼‰
        base_price = 50000

        # è¤‡æ•°ã®å‘¨æœŸçš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµ„ã¿åˆã‚ã›
        daily_cycle = (
            np.sin(np.arange(n_samples) * 2 * np.pi / 24) * 0.005
        )  # æ—¥æ¬¡ã‚µã‚¤ã‚¯ãƒ«
        weekly_cycle = (
            np.sin(np.arange(n_samples) * 2 * np.pi / (24 * 7)) * 0.01
        )  # é€±æ¬¡ã‚µã‚¤ã‚¯ãƒ«
        monthly_trend = (
            np.sin(np.arange(n_samples) * 2 * np.pi / (24 * 30)) * 0.02
        )  # æœˆæ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰

        # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ + å‘¨æœŸçš„ãƒ‘ã‚¿ãƒ¼ãƒ³
        random_walk = np.cumsum(np.random.normal(0, 0.008, n_samples))
        price_pattern = daily_cycle + weekly_cycle + monthly_trend + random_walk

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        volatility = np.abs(np.random.normal(0.015, 0.005, n_samples))
        for i in range(1, n_samples):
            volatility[i] = 0.7 * volatility[i - 1] + 0.3 * volatility[i]

        # ä¾¡æ ¼ç”Ÿæˆ
        price_changes = price_pattern + np.random.normal(0, volatility)
        prices = base_price * np.cumprod(1 + price_changes)

        # OHLCV ãƒ‡ãƒ¼ã‚¿
        data = pd.DataFrame(
            {
                "Open": prices * (1 + np.random.normal(0, 0.001, n_samples)),
                "High": prices * (1 + np.abs(np.random.normal(0, 0.003, n_samples))),
                "Low": prices * (1 - np.abs(np.random.normal(0, 0.003, n_samples))),
                "Close": prices,
                "Volume": np.random.lognormal(10, 1, n_samples),
            },
            index=dates,
        )

        # ç¾å®Ÿçš„ãªæŠ€è¡“æŒ‡æ¨™
        data["Returns"] = data["Close"].pct_change()
        data["SMA_5"] = data["Close"].rolling(5).mean()
        data["SMA_10"] = data["Close"].rolling(10).mean()
        data["SMA_20"] = data["Close"].rolling(20).mean()
        data["EMA_12"] = data["Close"].ewm(span=12).mean()
        data["EMA_26"] = data["Close"].ewm(span=26).mean()

        # æŠ€è¡“æŒ‡æ¨™
        data["RSI"] = self._calculate_rsi(data["Close"])
        data["MACD"] = data["EMA_12"] - data["EMA_26"]
        data["MACD_Signal"] = data["MACD"].ewm(span=9).mean()
        data["MACD_Histogram"] = data["MACD"] - data["MACD_Signal"]

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        data["BB_Middle"] = data["Close"].rolling(20).mean()
        data["BB_Std"] = data["Close"].rolling(20).std()
        data["BB_Upper"] = data["BB_Middle"] + (data["BB_Std"] * 2)
        data["BB_Lower"] = data["BB_Middle"] - (data["BB_Std"] * 2)
        data["BB_Position"] = (data["Close"] - data["BB_Lower"]) / (
            data["BB_Upper"] - data["BB_Lower"]
        )

        # å‡ºæ¥é«˜æŒ‡æ¨™
        data["Volume_SMA"] = data["Volume"].rolling(10).mean()
        data["Volume_Ratio"] = data["Volume"] / data["Volume_SMA"]
        data["VWAP"] = (data["Close"] * data["Volume"]).rolling(20).sum() / data[
            "Volume"
        ].rolling(20).sum()

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™
        data["Volatility"] = data["Returns"].rolling(20).std()
        data["ATR"] = self._calculate_atr(data)

        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
        data["ROC"] = data["Close"].pct_change(10)
        data["Williams_R"] = self._calculate_williams_r(data)

        # ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆ8æ™‚é–“é–“éš”ã‚’1æ™‚é–“ã«è£œé–“ï¼‰
        fr_base = np.random.normal(0.0001, 0.0003, n_samples // 8)
        fr_1h = np.repeat(fr_base, 8)[:n_samples]
        data["Funding_Rate"] = fr_1h

        # å»ºç‰æ®‹é«˜
        oi_trend = np.cumsum(np.random.normal(0, 0.01, n_samples))
        data["Open_Interest"] = np.exp(
            15 + oi_trend + np.random.normal(0, 0.1, n_samples)
        )
        data["OI_Change"] = data["Open_Interest"].pct_change()

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªäºˆæ¸¬æœŸé–“ï¼‰
        prediction_horizon = 6  # 6æ™‚é–“å¾Œã®ä¾¡æ ¼å¤‰å‹•ã‚’äºˆæ¸¬
        future_returns = (
            data["Close"].pct_change(prediction_horizon).shift(-prediction_horizon)
        )

        # å‹•çš„é–¾å€¤ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ï¼‰
        rolling_vol = data["Returns"].rolling(24).std()
        dynamic_threshold = rolling_vol * 1.0  # 1Ïƒã®å¤‰å‹•

        # 3ã‚¯ãƒ©ã‚¹åˆ†é¡
        y = pd.Series(1, index=dates)  # Hold
        y[future_returns > dynamic_threshold] = 2  # Up
        y[future_returns < -dynamic_threshold] = 0  # Down

        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        # ç„¡é™å€¤ã‚’é™¤å»
        data = data.replace([np.inf, -np.inf], np.nan)

        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿
        valid_mask = data.notna().all(axis=1) & y.notna() & rolling_vol.notna()
        data = data[valid_mask]
        y = y[valid_mask]

        # æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        data = data.fillna(data.median())

        return data, y

    def _calculate_rsi(self, prices, window=14):
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.fillna(50)

    def _calculate_atr(self, data, window=14):
        """ATRè¨ˆç®—"""
        high_low = data["High"] - data["Low"]
        high_close = np.abs(data["High"] - data["Close"].shift())
        low_close = np.abs(data["Low"] - data["Close"].shift())
        true_range = np.maximum(high_low, np.maximum(high_close, low_close))
        atr = true_range.rolling(window=window).mean()
        return atr.fillna(true_range)

    def _calculate_williams_r(self, data, window=14):
        """Williams %Rè¨ˆç®—"""
        highest_high = data["High"].rolling(window=window).max()
        lowest_low = data["Low"].rolling(window=window).min()
        williams_r = -100 * (highest_high - data["Close"]) / (highest_high - lowest_low)
        return williams_r.fillna(-50)

    def _clean_data(self, X):
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        X_clean = X.copy()

        # ç„¡é™å€¤ã‚’é™¤å»
        X_clean = X_clean.replace([np.inf, -np.inf], np.nan)

        # ç•°å¸¸ã«å¤§ããªå€¤ã‚’é™¤å»
        for col in X_clean.select_dtypes(include=[np.number]).columns:
            Q1 = X_clean[col].quantile(0.01)
            Q99 = X_clean[col].quantile(0.99)
            X_clean[col] = X_clean[col].clip(lower=Q1, upper=Q99)

        # æ¬ æå€¤ã‚’è£œå®Œ
        X_clean = X_clean.fillna(X_clean.median())

        return X_clean

    def test_baseline_trading_model(self, X, y):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å–å¼•ãƒ¢ãƒ‡ãƒ«ï¼ˆæ”¹å–„å‰ï¼‰"""
        logger.info("ğŸ”´ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å–å¼•ãƒ¢ãƒ‡ãƒ«ï¼ˆæ”¹å–„å‰ï¼‰")

        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        X_clean = self._clean_data(X)

        # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆæ”¹å–„å‰ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã¯é˜²ãï¼‰
        split_point = int(len(X_clean) * 0.8)  # å­¦ç¿’æœŸé–“ã‚’é•·ã
        X_train = X_clean.iloc[:split_point]
        X_test = X_clean.iloc[split_point:]
        y_train = y.iloc[:split_point]
        y_test = y.iloc[split_point:]

        # åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«
        model = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=6)
        model.fit(X_train, y_train)

        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)

        # è©³ç´°ãªè©•ä¾¡
        results = self._calculate_detailed_metrics(
            y_test, y_pred, y_proba, "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"
        )
        results.update(
            {
                "train_samples": len(X_train),
                "test_samples": len(X_test),
                "features": X.shape[1],
            }
        )

        return results

    def test_improved_trading_model(self, X, y):
        """æ”¹å–„ã•ã‚ŒãŸå–å¼•ãƒ¢ãƒ‡ãƒ«"""
        logger.info("ğŸŸ¢ æ”¹å–„ã•ã‚ŒãŸå–å¼•ãƒ¢ãƒ‡ãƒ«")

        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        X_clean = self._clean_data(X)

        # æ™‚ç³»åˆ—åˆ†å‰²
        split_point = int(len(X_clean) * 0.8)
        X_train = X_clean.iloc[:split_point]
        X_test = X_clean.iloc[split_point:]
        y_train = y.iloc[:split_point]
        y_test = y.iloc[split_point:]

        # RobustScaleré©ç”¨
        scaler = RobustScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test), columns=X_test.columns, index=X_test.index
        )

        # ç‰¹å¾´é‡é¸æŠ
        temp_model = RandomForestClassifier(n_estimators=50, random_state=42)
        temp_model.fit(X_train_scaled, y_train)

        feature_importance = pd.Series(
            temp_model.feature_importances_, index=X_train_scaled.columns
        ).sort_values(ascending=False)

        # ä¸Šä½ç‰¹å¾´é‡ã‚’é¸æŠ
        top_features = feature_importance.head(min(15, len(feature_importance))).index
        X_train_selected = X_train_scaled[top_features]
        X_test_selected = X_test_scaled[top_features]

        # æ”¹å–„ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        model = RandomForestClassifier(
            n_estimators=200,
            random_state=42,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=3,
            class_weight="balanced",
            max_features="sqrt",
        )
        model.fit(X_train_selected, y_train)

        y_pred = model.predict(X_test_selected)
        y_proba = model.predict_proba(X_test_selected)

        # è©³ç´°ãªè©•ä¾¡
        results = self._calculate_detailed_metrics(y_test, y_pred, y_proba, "æ”¹å–„ç‰ˆ")
        results.update(
            {
                "train_samples": len(X_train),
                "test_samples": len(X_test),
                "features": len(top_features),
                "selected_features": top_features.tolist(),
            }
        )

        return results

    def test_time_series_cv_performance(self, X, y):
        """æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”µ æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½ãƒ†ã‚¹ãƒˆ")

        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        X_clean = self._clean_data(X)

        # RobustScaleré©ç”¨
        scaler = RobustScaler()
        X_scaled = pd.DataFrame(
            scaler.fit_transform(X_clean), columns=X_clean.columns, index=X_clean.index
        )

        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        tscv = TimeSeriesSplit(n_splits=5)
        model = RandomForestClassifier(
            n_estimators=100, random_state=42, max_depth=8, class_weight="balanced"
        )

        cv_results = {
            "accuracy": [],
            "balanced_accuracy": [],
            "f1_score": [],
            "precision": [],
            "recall": [],
        }

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
            X_train_cv = X_scaled.iloc[train_idx]
            X_test_cv = X_scaled.iloc[test_idx]
            y_train_cv = y.iloc[train_idx]
            y_test_cv = y.iloc[test_idx]

            model.fit(X_train_cv, y_train_cv)
            y_pred_cv = model.predict(X_test_cv)

            # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®çµæœ
            cv_results["accuracy"].append(accuracy_score(y_test_cv, y_pred_cv))
            cv_results["balanced_accuracy"].append(
                balanced_accuracy_score(y_test_cv, y_pred_cv)
            )
            cv_results["f1_score"].append(
                f1_score(y_test_cv, y_pred_cv, average="weighted")
            )

            precision, recall, _, _ = precision_recall_fscore_support(
                y_test_cv, y_pred_cv, average="weighted", zero_division=0
            )
            cv_results["precision"].append(precision)
            cv_results["recall"].append(recall)

            logger.info(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold+1}: ç²¾åº¦={cv_results['accuracy'][-1]:.4f}")

        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        results = {}
        for metric, values in cv_results.items():
            results[f"{metric}_mean"] = np.mean(values)
            results[f"{metric}_std"] = np.std(values)
            results[f"{metric}_min"] = np.min(values)
            results[f"{metric}_max"] = np.max(values)

        results["method"] = "æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"
        results["n_folds"] = len(cv_results["accuracy"])

        return results

    def _calculate_detailed_metrics(self, y_true, y_pred, y_proba, method_name):
        """è©³ç´°ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
        results = {
            "method": method_name,
            "accuracy": accuracy_score(y_true, y_pred),
            "balanced_accuracy": balanced_accuracy_score(y_true, y_pred),
            "f1_score": f1_score(y_true, y_pred, average="weighted"),
        }

        # ã‚¯ãƒ©ã‚¹åˆ¥ã®è©³ç´°æŒ‡æ¨™
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, zero_division=0
        )

        class_names = ["Down", "Hold", "Up"]
        for i, class_name in enumerate(class_names):
            if i < len(precision):
                results[f"{class_name}_precision"] = precision[i]
                results[f"{class_name}_recall"] = recall[i]
                results[f"{class_name}_f1"] = f1[i]
                results[f"{class_name}_support"] = support[i]

        # å–å¼•ã‚·ã‚°ãƒŠãƒ«ã®ç²¾åº¦ï¼ˆUp/Downã‚¯ãƒ©ã‚¹ã®ã¿ï¼‰
        trading_mask = (y_true != 1) & (y_pred != 1)  # Holdã‚’é™¤å¤–
        if trading_mask.sum() > 0:
            trading_accuracy = accuracy_score(
                y_true[trading_mask], y_pred[trading_mask]
            )
            results["trading_signal_accuracy"] = trading_accuracy
        else:
            results["trading_signal_accuracy"] = 0.0

        logger.info(f"  {method_name}çµæœ:")
        logger.info(f"    ç²¾åº¦: {results['accuracy']:.4f}")
        logger.info(f"    ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {results['balanced_accuracy']:.4f}")
        logger.info(f"    F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.4f}")
        logger.info(f"    å–å¼•ã‚·ã‚°ãƒŠãƒ«ç²¾åº¦: {results['trading_signal_accuracy']:.4f}")

        return results

    def run_realistic_performance_test(self):
        """ãƒªã‚¢ãƒ«ãªæ€§èƒ½ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        logger.info("=" * 80)
        logger.info("ğŸš€ ãƒªã‚¢ãƒ«ãªå–å¼•ç’°å¢ƒã§ã®MLãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
        logger.info("=" * 80)

        # ãƒªã‚¢ãƒ«ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        X, y = self.create_realistic_market_data(n_samples=2000)

        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(X)}ã‚µãƒ³ãƒ—ãƒ«, {X.shape[1]}ç‰¹å¾´é‡")
        logger.info(f"ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {y.value_counts().to_dict()}")

        # å„æ‰‹æ³•ã§ãƒ†ã‚¹ãƒˆ
        results = {}

        # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
        results["baseline"] = self.test_baseline_trading_model(X, y)

        # 2. æ”¹å–„ãƒ¢ãƒ‡ãƒ«
        results["improved"] = self.test_improved_trading_model(X, y)

        # 3. æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        results["time_series_cv"] = self.test_time_series_cv_performance(X, y)

        # çµæœåˆ†æ
        self._analyze_realistic_performance(results)

        return results

    def _analyze_realistic_performance(self, results):
        """ãƒªã‚¢ãƒ«ãªæ€§èƒ½çµæœã‚’åˆ†æ"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š ãƒªã‚¢ãƒ«ãªå–å¼•ç’°å¢ƒã§ã®æ€§èƒ½åˆ†æçµæœ")
        logger.info("=" * 80)

        baseline = results["baseline"]
        improved = results["improved"]
        cv_results = results["time_series_cv"]

        # ä¸»è¦æŒ‡æ¨™ã®æ”¹å–„åŠ¹æœ
        accuracy_improvement = (
            (improved["accuracy"] - baseline["accuracy"]) / baseline["accuracy"] * 100
        )
        balanced_acc_improvement = (
            (improved["balanced_accuracy"] - baseline["balanced_accuracy"])
            / baseline["balanced_accuracy"]
            * 100
        )
        f1_improvement = (
            (improved["f1_score"] - baseline["f1_score"]) / baseline["f1_score"] * 100
        )
        trading_signal_improvement = (
            (improved["trading_signal_accuracy"] - baseline["trading_signal_accuracy"])
            / baseline["trading_signal_accuracy"]
            * 100
        )

        logger.info("ğŸ¯ ä¸»è¦æŒ‡æ¨™ã®æ”¹å–„åŠ¹æœ:")
        logger.info(
            f"  ç²¾åº¦: {baseline['accuracy']:.4f} â†’ {improved['accuracy']:.4f} ({accuracy_improvement:+.1f}%)"
        )
        logger.info(
            f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {baseline['balanced_accuracy']:.4f} â†’ {improved['balanced_accuracy']:.4f} ({balanced_acc_improvement:+.1f}%)"
        )
        logger.info(
            f"  F1ã‚¹ã‚³ã‚¢: {baseline['f1_score']:.4f} â†’ {improved['f1_score']:.4f} ({f1_improvement:+.1f}%)"
        )
        logger.info(
            f"  å–å¼•ã‚·ã‚°ãƒŠãƒ«ç²¾åº¦: {baseline['trading_signal_accuracy']:.4f} â†’ {improved['trading_signal_accuracy']:.4f} ({trading_signal_improvement:+.1f}%)"
        )

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
        logger.info(f"\nğŸ“ˆ æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:")
        logger.info(
            f"  å¹³å‡ç²¾åº¦: {cv_results['accuracy_mean']:.4f} Â± {cv_results['accuracy_std']:.4f}"
        )
        logger.info(
            f"  å¹³å‡ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {cv_results['balanced_accuracy_mean']:.4f} Â± {cv_results['balanced_accuracy_std']:.4f}"
        )
        logger.info(
            f"  å¹³å‡F1ã‚¹ã‚³ã‚¢: {cv_results['f1_mean']:.4f} Â± {cv_results['f1_std']:.4f}"
        )

        # å®‰å®šæ€§è©•ä¾¡
        cv_stability = cv_results["accuracy_std"] / cv_results["accuracy_mean"]
        logger.info(f"  ãƒ¢ãƒ‡ãƒ«å®‰å®šæ€§ (CV): {cv_stability:.4f}")

        # ç·åˆè©•ä¾¡
        avg_improvement = (
            accuracy_improvement + balanced_acc_improvement + f1_improvement
        ) / 3

        logger.info(f"\nğŸ† ç·åˆè©•ä¾¡:")
        logger.info(f"  å¹³å‡æ”¹å–„ç‡: {avg_improvement:+.1f}%")
        logger.info(f"  å–å¼•ã‚·ã‚°ãƒŠãƒ«æ”¹å–„: {trading_signal_improvement:+.1f}%")
        logger.info(
            f"  ãƒ¢ãƒ‡ãƒ«å®‰å®šæ€§: {'é«˜' if cv_stability < 0.1 else 'ä¸­' if cv_stability < 0.2 else 'ä½'}"
        )

        if avg_improvement > 10:
            logger.info("  ğŸ‰ å„ªç§€ãªæ”¹å–„åŠ¹æœã‚’ç¢ºèªï¼")
        elif avg_improvement > 5:
            logger.info("  âœ… æœ‰æ„ãªæ”¹å–„åŠ¹æœã‚’ç¢ºèª")
        elif avg_improvement > 0:
            logger.info("  âš ï¸ è»½å¾®ãªæ”¹å–„åŠ¹æœ")
        else:
            logger.info("  âŒ æ”¹å–„åŠ¹æœãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“")

        # å®Ÿç”¨æ€§è©•ä¾¡
        if improved["trading_signal_accuracy"] > 0.55:
            logger.info("  ğŸ’° å®Ÿç”¨çš„ãªå–å¼•ã‚·ã‚°ãƒŠãƒ«ç²¾åº¦")
        elif improved["trading_signal_accuracy"] > 0.50:
            logger.info("  ğŸ“ˆ å–å¼•ã«ä½¿ç”¨å¯èƒ½ãªç²¾åº¦")
        else:
            logger.info("  âš ï¸ å–å¼•ã‚·ã‚°ãƒŠãƒ«ç²¾åº¦ãŒä¸ååˆ†")


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s"
    )

    # ãƒªã‚¢ãƒ«ãªæ€§èƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test = RealisticTradingPerformanceTest()
    results = test.run_realistic_performance_test()

    logger.info("\nğŸ‰ ãƒªã‚¢ãƒ«ãªå–å¼•ç’°å¢ƒã§ã®æ€§èƒ½ãƒ†ã‚¹ãƒˆå®Œäº†")
