"""
ã‚ªãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ»ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ

ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é–“ã®æ•´åˆæ€§ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©å…ƒã€åˆ†æ•£å‡¦ç†ã€ACIDç‰¹æ€§ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, backend_dir)

import pytest
import pandas as pd
import numpy as np
import time
import sqlite3
import tempfile
import shutil
import threading
import concurrent.futures
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import json
import hashlib

logger = logging.getLogger(__name__)


class TestDataConsistency:
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ»ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆå‰ã®æº–å‚™"""
        self.start_time = time.time()
        self.temp_dir = tempfile.mkdtemp()
        
    def teardown_method(self):
        """ãƒ†ã‚¹ãƒˆå¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        execution_time = time.time() - self.start_time
        logger.info(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“: {execution_time:.3f}ç§’")
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            shutil.rmtree(self.temp_dir)
        except:
            pass
    
    def create_test_database(self, db_path: str) -> None:
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ"""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS market_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                timestamp INTEGER NOT NULL,
                open_price REAL NOT NULL,
                high_price REAL NOT NULL,
                low_price REAL NOT NULL,
                close_price REAL NOT NULL,
                volume REAL NOT NULL,
                checksum TEXT,
                created_at INTEGER DEFAULT (strftime('%s', 'now'))
            )
        """)
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS strategy_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                strategy_name TEXT NOT NULL,
                symbol TEXT NOT NULL,
                timestamp INTEGER NOT NULL,
                sl_price REAL,
                tp_price REAL,
                risk_reward_ratio REAL,
                checksum TEXT,
                created_at INTEGER DEFAULT (strftime('%s', 'now'))
            )
        """)
        
        conn.commit()
        conn.close()
    
    def calculate_data_checksum(self, data: Dict) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã‚’è¨ˆç®—"""
        # è¾æ›¸ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def test_multi_source_data_consistency(self):
        """ãƒ†ã‚¹ãƒˆ53: ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é–“ã§ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§"""
        logger.info("ğŸ” ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            # è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            data_sources = {
                "source_a": [],
                "source_b": [],
                "source_c": []
            }
            
            # åŸºæº–ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            base_time = datetime(2023, 1, 1, 0, 0, 0)
            num_points = 1000
            
            for i in range(num_points):
                timestamp = base_time + timedelta(minutes=i)
                base_price = 50000 + np.random.normal(0, 100)
                
                # å„ã‚½ãƒ¼ã‚¹ã§å¾®å°ãªå·®ç•°ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
                for source_name in data_sources.keys():
                    # ã‚½ãƒ¼ã‚¹é–“ã§æœ€å¤§0.1%ã®ä¾¡æ ¼å·®ã‚’è¨±å®¹
                    price_variance = np.random.normal(0, base_price * 0.001)
                    
                    data_point = {
                        "timestamp": timestamp.isoformat(),
                        "symbol": "BTC/USDT",
                        "open": base_price + price_variance,
                        "high": base_price + price_variance + abs(np.random.normal(0, 50)),
                        "low": base_price + price_variance - abs(np.random.normal(0, 50)),
                        "close": base_price + price_variance + np.random.normal(0, 20),
                        "volume": np.random.exponential(1000),
                        "source": source_name
                    }
                    
                    data_sources[source_name].append(data_point)
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®åˆ†æ
            consistency_stats = {
                "price_differences": [],
                "volume_differences": [],
                "timestamp_mismatches": 0,
                "outliers": 0
            }
            
            # åŒä¸€æ™‚åˆ»ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒ
            for i in range(num_points):
                source_a_data = data_sources["source_a"][i]
                source_b_data = data_sources["source_b"][i]
                source_c_data = data_sources["source_c"][i]
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ä¸€è‡´ç¢ºèª
                if not (source_a_data["timestamp"] == source_b_data["timestamp"] == source_c_data["timestamp"]):
                    consistency_stats["timestamp_mismatches"] += 1
                
                # ä¾¡æ ¼å·®ã®åˆ†æ
                prices_a = [source_a_data["open"], source_a_data["high"], source_a_data["low"], source_a_data["close"]]
                prices_b = [source_b_data["open"], source_b_data["high"], source_b_data["low"], source_b_data["close"]]
                prices_c = [source_c_data["open"], source_c_data["high"], source_c_data["low"], source_c_data["close"]]
                
                for j in range(4):  # OHLC
                    max_price = max(prices_a[j], prices_b[j], prices_c[j])
                    min_price = min(prices_a[j], prices_b[j], prices_c[j])
                    price_diff_pct = (max_price - min_price) / min_price * 100
                    
                    consistency_stats["price_differences"].append(price_diff_pct)
                    
                    # ç•°å¸¸ãªä¾¡æ ¼å·®ï¼ˆ1%ä»¥ä¸Šï¼‰ã‚’æ¤œå‡º
                    if price_diff_pct > 1.0:
                        consistency_stats["outliers"] += 1
                
                # ãƒœãƒªãƒ¥ãƒ¼ãƒ å·®ã®åˆ†æ
                volumes = [source_a_data["volume"], source_b_data["volume"], source_c_data["volume"]]
                max_volume = max(volumes)
                min_volume = min(volumes)
                if min_volume > 0:
                    volume_diff_pct = (max_volume - min_volume) / min_volume * 100
                    consistency_stats["volume_differences"].append(volume_diff_pct)
            
            # æ•´åˆæ€§çµ±è¨ˆã®åˆ†æ
            avg_price_diff = np.mean(consistency_stats["price_differences"])
            max_price_diff = max(consistency_stats["price_differences"])
            p95_price_diff = np.percentile(consistency_stats["price_differences"], 95)
            
            avg_volume_diff = np.mean(consistency_stats["volume_differences"])
            max_volume_diff = max(consistency_stats["volume_differences"])
            
            logger.info(f"ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§çµæœ:")
            logger.info(f"  ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°: {num_points}")
            logger.info(f"  å¹³å‡ä¾¡æ ¼å·®: {avg_price_diff:.4f}%")
            logger.info(f"  æœ€å¤§ä¾¡æ ¼å·®: {max_price_diff:.4f}%")
            logger.info(f"  95%ileä¾¡æ ¼å·®: {p95_price_diff:.4f}%")
            logger.info(f"  å¹³å‡ãƒœãƒªãƒ¥ãƒ¼ãƒ å·®: {avg_volume_diff:.2f}%")
            logger.info(f"  æœ€å¤§ãƒœãƒªãƒ¥ãƒ¼ãƒ å·®: {max_volume_diff:.2f}%")
            logger.info(f"  ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä¸ä¸€è‡´: {consistency_stats['timestamp_mismatches']}")
            logger.info(f"  ä¾¡æ ¼ç•°å¸¸å€¤: {consistency_stats['outliers']}")
            
            # æ•´åˆæ€§è¦ä»¶ã®ç¢ºèª
            assert avg_price_diff < 0.5, f"å¹³å‡ä¾¡æ ¼å·®ãŒå¤§ãã™ãã¾ã™: {avg_price_diff:.4f}%"
            assert max_price_diff < 2.0, f"æœ€å¤§ä¾¡æ ¼å·®ãŒå¤§ãã™ãã¾ã™: {max_price_diff:.4f}%"
            assert consistency_stats["timestamp_mismatches"] == 0, f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä¸ä¸€è‡´ãŒã‚ã‚Šã¾ã™: {consistency_stats['timestamp_mismatches']}"
            assert consistency_stats["outliers"] < num_points * 0.01, f"ç•°å¸¸å€¤ãŒå¤šã™ãã¾ã™: {consistency_stats['outliers']}"
            
            logger.info("âœ… ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            
        except Exception as e:
            pytest.fail(f"ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def test_backup_restore_consistency(self):
        """ãƒ†ã‚¹ãƒˆ54: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©å…ƒå¾Œã®ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§"""
        logger.info("ğŸ” ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©å…ƒä¸€è²«æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
            original_db = os.path.join(self.temp_dir, "original.db")
            backup_db = os.path.join(self.temp_dir, "backup.db")
            restored_db = os.path.join(self.temp_dir, "restored.db")
            
            self.create_test_database(original_db)
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
            conn = sqlite3.connect(original_db)
            cursor = conn.cursor()
            
            test_data = []
            for i in range(1000):
                timestamp = int(time.time()) - (1000 - i) * 60  # 1åˆ†é–“éš”
                data = {
                    "symbol": "BTC/USDT",
                    "timestamp": timestamp,
                    "open_price": 50000 + np.random.normal(0, 100),
                    "high_price": 50000 + np.random.normal(0, 100) + 50,
                    "low_price": 50000 + np.random.normal(0, 100) - 50,
                    "close_price": 50000 + np.random.normal(0, 100),
                    "volume": np.random.exponential(1000)
                }
                
                # ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã‚’è¨ˆç®—
                checksum = self.calculate_data_checksum(data)
                data["checksum"] = checksum
                
                cursor.execute("""
                    INSERT INTO market_data (symbol, timestamp, open_price, high_price, low_price, close_price, volume, checksum)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (data["symbol"], data["timestamp"], data["open_price"], data["high_price"], 
                     data["low_price"], data["close_price"], data["volume"], data["checksum"]))
                
                test_data.append(data)
            
            conn.commit()
            
            # å…ƒãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’å–å¾—
            cursor.execute("SELECT COUNT(*), SUM(volume), AVG(close_price) FROM market_data")
            original_stats = cursor.fetchone()
            
            cursor.execute("SELECT checksum FROM market_data ORDER BY id")
            original_checksums = [row[0] for row in cursor.fetchall()]
            
            conn.close()
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            logger.info("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆä¸­...")
            shutil.copy2(original_db, backup_db)
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ç¢ºèª
            assert os.path.exists(backup_db), "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            backup_size = os.path.getsize(backup_db)
            original_size = os.path.getsize(original_db)
            assert backup_size == original_size, f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µã‚¤ã‚ºãŒç•°ãªã‚Šã¾ã™: {backup_size} vs {original_size}"
            
            # å¾©å…ƒå‡¦ç†
            logger.info("å¾©å…ƒå‡¦ç†ä¸­...")
            shutil.copy2(backup_db, restored_db)
            
            # å¾©å…ƒãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            conn = sqlite3.connect(restored_db)
            cursor = conn.cursor()
            
            # çµ±è¨ˆã®æ¯”è¼ƒ
            cursor.execute("SELECT COUNT(*), SUM(volume), AVG(close_price) FROM market_data")
            restored_stats = cursor.fetchone()
            
            assert restored_stats == original_stats, f"çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒä¸€è‡´ã—ã¾ã›ã‚“: {restored_stats} vs {original_stats}"
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã®æ¯”è¼ƒ
            cursor.execute("SELECT checksum FROM market_data ORDER BY id")
            restored_checksums = [row[0] for row in cursor.fetchall()]
            
            assert restored_checksums == original_checksums, "ãƒã‚§ãƒƒã‚¯ã‚µãƒ ãŒä¸€è‡´ã—ã¾ã›ã‚“"
            
            # å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æ¤œè¨¼
            cursor.execute("SELECT * FROM market_data ORDER BY id")
            restored_records = cursor.fetchall()
            
            conn.close()
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®è©³ç´°æ¤œè¨¼
            integrity_check_passed = 0
            for i, record in enumerate(restored_records):
                record_data = {
                    "symbol": record[1],
                    "timestamp": record[2],
                    "open_price": record[3],
                    "high_price": record[4],
                    "low_price": record[5],
                    "close_price": record[6],
                    "volume": record[7]
                }
                
                expected_checksum = self.calculate_data_checksum(record_data)
                actual_checksum = record[8]
                
                if expected_checksum == actual_checksum:
                    integrity_check_passed += 1
                else:
                    logger.warning(f"ãƒ¬ã‚³ãƒ¼ãƒ‰ {i+1}: ãƒã‚§ãƒƒã‚¯ã‚µãƒ ä¸ä¸€è‡´")
            
            integrity_rate = integrity_check_passed / len(restored_records)
            
            logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©å…ƒçµæœ:")
            logger.info(f"  å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {original_stats[0]}")
            logger.info(f"  å¾©å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {restored_stats[0]}")
            logger.info(f"  æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯é€šéç‡: {integrity_rate:.1%}")
            logger.info(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºä¸€è‡´: {backup_size == original_size}")
            
            # æ•´åˆæ€§è¦ä»¶ã®ç¢ºèª
            assert integrity_rate == 1.0, f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãŒå®Œå…¨ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {integrity_rate:.1%}"
            assert restored_stats[0] == original_stats[0], "ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“"
            
            logger.info("âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©å…ƒä¸€è²«æ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            
        except Exception as e:
            pytest.fail(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©å…ƒä¸€è²«æ€§ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def test_distributed_processing_synchronization(self):
        """ãƒ†ã‚¹ãƒˆ55: åˆ†æ•£å‡¦ç†ç’°å¢ƒã§ã®ãƒ‡ãƒ¼ã‚¿åŒæœŸ"""
        logger.info("ğŸ” åˆ†æ•£å‡¦ç†ãƒ‡ãƒ¼ã‚¿åŒæœŸãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            from app.services.auto_strategy.calculators.tpsl_calculator import TPSLCalculator
            
            # åˆ†æ•£å‡¦ç†ãƒãƒ¼ãƒ‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            num_nodes = 4
            shared_data = {
                "processed_items": 0,
                "results": [],
                "lock": threading.Lock(),
                "errors": []
            }
            
            # å‡¦ç†å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
            input_data = []
            for i in range(1000):
                data_item = {
                    "id": i,
                    "price": 50000 + np.random.normal(0, 100),
                    "sl_pct": 0.02,
                    "tp_pct": 0.04,
                    "direction": 1.0 if i % 2 == 0 else -1.0
                }
                input_data.append(data_item)
            
            def process_node(node_id: int, data_chunk: List[Dict]) -> Dict:
                """åˆ†æ•£å‡¦ç†ãƒãƒ¼ãƒ‰"""
                calculator = TPSLCalculator()
                node_results = []
                node_errors = []
                
                for item in data_chunk:
                    try:
                        # TP/SLè¨ˆç®—
                        sl_price, tp_price = calculator.calculate_basic_tpsl_prices(
                            item["price"], item["sl_pct"], item["tp_pct"], item["direction"]
                        )
                        
                        result = {
                            "id": item["id"],
                            "node_id": node_id,
                            "sl_price": sl_price,
                            "tp_price": tp_price,
                            "processed_at": time.time()
                        }
                        
                        node_results.append(result)
                        
                        # å…±æœ‰ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ï¼ˆåŒæœŸï¼‰
                        with shared_data["lock"]:
                            shared_data["processed_items"] += 1
                            shared_data["results"].append(result)
                        
                    except Exception as e:
                        node_errors.append({"id": item["id"], "error": str(e)})
                        with shared_data["lock"]:
                            shared_data["errors"].append({"node_id": node_id, "item_id": item["id"], "error": str(e)})
                
                return {
                    "node_id": node_id,
                    "processed_count": len(node_results),
                    "error_count": len(node_errors),
                    "results": node_results
                }
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ•£å‡¦ç†ãƒãƒ¼ãƒ‰ã«åˆ†å‰²
            chunk_size = len(input_data) // num_nodes
            data_chunks = [
                input_data[i:i + chunk_size] 
                for i in range(0, len(input_data), chunk_size)
            ]
            
            # åˆ†æ•£å‡¦ç†å®Ÿè¡Œ
            start_time = time.time()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=num_nodes) as executor:
                futures = [
                    executor.submit(process_node, i, chunk) 
                    for i, chunk in enumerate(data_chunks)
                ]
                
                node_results = [future.result() for future in futures]
            
            processing_time = time.time() - start_time
            
            # åŒæœŸçµæœã®åˆ†æ
            total_processed = sum(result["processed_count"] for result in node_results)
            total_errors = sum(result["error_count"] for result in node_results)
            
            # ãƒ‡ãƒ¼ã‚¿é‡è¤‡ãƒã‚§ãƒƒã‚¯
            all_ids = [result["id"] for result in shared_data["results"]]
            unique_ids = set(all_ids)
            duplicate_count = len(all_ids) - len(unique_ids)
            
            # ãƒ‡ãƒ¼ã‚¿æ¬ æãƒã‚§ãƒƒã‚¯
            expected_ids = set(range(len(input_data)))
            processed_ids = set(all_ids)
            missing_ids = expected_ids - processed_ids
            
            logger.info(f"åˆ†æ•£å‡¦ç†åŒæœŸçµæœ:")
            logger.info(f"  å‡¦ç†ãƒãƒ¼ãƒ‰æ•°: {num_nodes}")
            logger.info(f"  ç·å‡¦ç†ä»¶æ•°: {total_processed}")
            logger.info(f"  ã‚¨ãƒ©ãƒ¼ä»¶æ•°: {total_errors}")
            logger.info(f"  å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
            logger.info(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {total_processed/processing_time:.1f}ä»¶/ç§’")
            logger.info(f"  é‡è¤‡ãƒ‡ãƒ¼ã‚¿: {duplicate_count}ä»¶")
            logger.info(f"  æ¬ æãƒ‡ãƒ¼ã‚¿: {len(missing_ids)}ä»¶")
            
            # ãƒãƒ¼ãƒ‰åˆ¥çµ±è¨ˆ
            for result in node_results:
                logger.info(f"  ãƒãƒ¼ãƒ‰{result['node_id']}: {result['processed_count']}ä»¶å‡¦ç†, {result['error_count']}ä»¶ã‚¨ãƒ©ãƒ¼")
            
            # åŒæœŸè¦ä»¶ã®ç¢ºèª
            assert duplicate_count == 0, f"ãƒ‡ãƒ¼ã‚¿é‡è¤‡ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {duplicate_count}ä»¶"
            assert len(missing_ids) == 0, f"ãƒ‡ãƒ¼ã‚¿æ¬ æãŒç™ºç”Ÿã—ã¾ã—ãŸ: {len(missing_ids)}ä»¶"
            assert total_processed == len(input_data), f"å‡¦ç†ä»¶æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“: {total_processed} vs {len(input_data)}"
            assert total_errors < len(input_data) * 0.01, f"ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã™ãã¾ã™: {total_errors}/{len(input_data)}"
            
            # çµæœã®ä¸€è²«æ€§ç¢ºèª
            for result in shared_data["results"][:10]:  # æœ€åˆã®10ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                assert result["sl_price"] is not None, "SLä¾¡æ ¼ãŒNullã§ã™"
                assert result["tp_price"] is not None, "TPä¾¡æ ¼ãŒNullã§ã™"
                assert result["sl_price"] > 0, "SLä¾¡æ ¼ãŒè² ã§ã™"
                assert result["tp_price"] > 0, "TPä¾¡æ ¼ãŒè² ã§ã™"
            
            logger.info("âœ… åˆ†æ•£å‡¦ç†ãƒ‡ãƒ¼ã‚¿åŒæœŸãƒ†ã‚¹ãƒˆæˆåŠŸ")

        except Exception as e:
            pytest.fail(f"åˆ†æ•£å‡¦ç†ãƒ‡ãƒ¼ã‚¿åŒæœŸãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    def test_transaction_atomicity(self):
        """ãƒ†ã‚¹ãƒˆ56: ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†ã®åŸå­æ€§ï¼ˆACIDç‰¹æ€§ï¼‰"""
        logger.info("ğŸ” ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³åŸå­æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
            test_db = os.path.join(self.temp_dir, "transaction_test.db")
            self.create_test_database(test_db)

            # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆ
            transaction_stats = {
                "successful_transactions": 0,
                "failed_transactions": 0,
                "rollback_count": 0,
                "data_consistency_errors": 0
            }

            def execute_transaction(transaction_id: int, should_fail: bool = False) -> bool:
                """ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
                conn = sqlite3.connect(test_db)
                cursor = conn.cursor()

                try:
                    # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
                    cursor.execute("BEGIN TRANSACTION")

                    # è¤‡æ•°ã®é–¢é€£æ“ä½œã‚’å®Ÿè¡Œ
                    timestamp = int(time.time()) + transaction_id

                    # å¸‚å ´ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
                    market_data = {
                        "symbol": "BTC/USDT",
                        "timestamp": timestamp,
                        "open_price": 50000 + np.random.normal(0, 100),
                        "high_price": 50000 + np.random.normal(0, 100) + 50,
                        "low_price": 50000 + np.random.normal(0, 100) - 50,
                        "close_price": 50000 + np.random.normal(0, 100),
                        "volume": np.random.exponential(1000)
                    }

                    checksum = self.calculate_data_checksum(market_data)

                    cursor.execute("""
                        INSERT INTO market_data (symbol, timestamp, open_price, high_price, low_price, close_price, volume, checksum)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (market_data["symbol"], market_data["timestamp"], market_data["open_price"],
                         market_data["high_price"], market_data["low_price"], market_data["close_price"],
                         market_data["volume"], checksum))

                    market_data_id = cursor.lastrowid

                    # æˆ¦ç•¥çµæœæŒ¿å…¥
                    strategy_result = {
                        "strategy_name": f"test_strategy_{transaction_id}",
                        "symbol": "BTC/USDT",
                        "timestamp": timestamp,
                        "sl_price": market_data["close_price"] * 0.98,
                        "tp_price": market_data["close_price"] * 1.04,
                        "risk_reward_ratio": 2.0
                    }

                    strategy_checksum = self.calculate_data_checksum(strategy_result)

                    cursor.execute("""
                        INSERT INTO strategy_results (strategy_name, symbol, timestamp, sl_price, tp_price, risk_reward_ratio, checksum)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (strategy_result["strategy_name"], strategy_result["symbol"], strategy_result["timestamp"],
                         strategy_result["sl_price"], strategy_result["tp_price"], strategy_result["risk_reward_ratio"],
                         strategy_checksum))

                    # æ„å›³çš„ãªå¤±æ•—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    if should_fail:
                        raise Exception(f"Intentional failure for transaction {transaction_id}")

                    # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒŸãƒƒãƒˆ
                    conn.commit()
                    transaction_stats["successful_transactions"] += 1
                    return True

                except Exception as e:
                    # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    conn.rollback()
                    transaction_stats["failed_transactions"] += 1
                    transaction_stats["rollback_count"] += 1
                    logger.debug(f"Transaction {transaction_id} rolled back: {e}")
                    return False

                finally:
                    conn.close()

            # è¤‡æ•°ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¸¦è¡Œå®Ÿè¡Œ
            num_transactions = 100
            failure_rate = 0.1  # 10%ã®å¤±æ•—ç‡

            start_time = time.time()

            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = []
                for i in range(num_transactions):
                    should_fail = np.random.random() < failure_rate
                    future = executor.submit(execute_transaction, i, should_fail)
                    futures.append(future)

                results = [future.result() for future in futures]

            processing_time = time.time() - start_time

            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æ¤œè¨¼
            conn = sqlite3.connect(test_db)
            cursor = conn.cursor()

            # å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã¨ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼çµæœã®ä»¶æ•°ç¢ºèª
            cursor.execute("SELECT COUNT(*) FROM market_data")
            market_data_count = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM strategy_results")
            strategy_results_count = cursor.fetchone()[0]

            # æˆåŠŸã—ãŸãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã¨å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            expected_count = transaction_stats["successful_transactions"]

            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã®æ•´åˆæ€§ç¢ºèª
            cursor.execute("SELECT * FROM market_data")
            market_records = cursor.fetchall()

            checksum_errors = 0
            for record in market_records:
                record_data = {
                    "symbol": record[1],
                    "timestamp": record[2],
                    "open_price": record[3],
                    "high_price": record[4],
                    "low_price": record[5],
                    "close_price": record[6],
                    "volume": record[7]
                }

                expected_checksum = self.calculate_data_checksum(record_data)
                actual_checksum = record[8]

                if expected_checksum != actual_checksum:
                    checksum_errors += 1

            conn.close()

            logger.info(f"ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³åŸå­æ€§ãƒ†ã‚¹ãƒˆçµæœ:")
            logger.info(f"  ç·ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³æ•°: {num_transactions}")
            logger.info(f"  æˆåŠŸãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³: {transaction_stats['successful_transactions']}")
            logger.info(f"  å¤±æ•—ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³: {transaction_stats['failed_transactions']}")
            logger.info(f"  ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å›æ•°: {transaction_stats['rollback_count']}")
            logger.info(f"  å¸‚å ´ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {market_data_count}")
            logger.info(f"  æˆ¦ç•¥çµæœä»¶æ•°: {strategy_results_count}")
            logger.info(f"  ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã‚¨ãƒ©ãƒ¼: {checksum_errors}")
            logger.info(f"  å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")

            # ACIDç‰¹æ€§ã®ç¢ºèª
            assert market_data_count == expected_count, f"å¸‚å ´ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãŒä¸ä¸€è‡´: {market_data_count} vs {expected_count}"
            assert strategy_results_count == expected_count, f"æˆ¦ç•¥çµæœä»¶æ•°ãŒä¸ä¸€è‡´: {strategy_results_count} vs {expected_count}"
            assert checksum_errors == 0, f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {checksum_errors}ä»¶"
            assert transaction_stats["rollback_count"] == transaction_stats["failed_transactions"], "ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å›æ•°ãŒä¸ä¸€è‡´"

            logger.info("âœ… ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³åŸå­æ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ")

        except Exception as e:
            pytest.fail(f"ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³åŸå­æ€§ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    def test_database_lock_contention(self):
        """ãƒ†ã‚¹ãƒˆ57: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒƒã‚¯ç«¶åˆæ™‚ã®å‡¦ç†"""
        logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒƒã‚¯ç«¶åˆãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
            test_db = os.path.join(self.temp_dir, "lock_test.db")
            self.create_test_database(test_db)

            # ãƒ­ãƒƒã‚¯ç«¶åˆçµ±è¨ˆ
            lock_stats = {
                "read_operations": 0,
                "write_operations": 0,
                "lock_timeouts": 0,
                "deadlocks": 0,
                "successful_operations": 0,
                "failed_operations": 0
            }

            def heavy_read_operation(thread_id: int) -> Dict:
                """é‡ã„èª­ã¿å–ã‚Šæ“ä½œ"""
                conn = sqlite3.connect(test_db, timeout=5.0)
                cursor = conn.cursor()

                try:
                    start_time = time.time()

                    # è¤‡é›‘ãªèª­ã¿å–ã‚Šã‚¯ã‚¨ãƒª
                    cursor.execute("""
                        SELECT symbol, AVG(close_price), COUNT(*), MIN(timestamp), MAX(timestamp)
                        FROM market_data
                        GROUP BY symbol
                    """)
                    results = cursor.fetchall()

                    # è¿½åŠ ã®èª­ã¿å–ã‚Šæ“ä½œ
                    cursor.execute("SELECT COUNT(*) FROM strategy_results")
                    count = cursor.fetchone()[0]

                    execution_time = time.time() - start_time
                    lock_stats["read_operations"] += 1
                    lock_stats["successful_operations"] += 1

                    return {
                        "thread_id": thread_id,
                        "operation": "read",
                        "success": True,
                        "execution_time": execution_time,
                        "results_count": len(results)
                    }

                except sqlite3.OperationalError as e:
                    if "database is locked" in str(e).lower():
                        lock_stats["lock_timeouts"] += 1
                    lock_stats["failed_operations"] += 1

                    return {
                        "thread_id": thread_id,
                        "operation": "read",
                        "success": False,
                        "error": str(e)
                    }

                finally:
                    conn.close()

            def heavy_write_operation(thread_id: int) -> Dict:
                """é‡ã„æ›¸ãè¾¼ã¿æ“ä½œ"""
                conn = sqlite3.connect(test_db, timeout=5.0)
                cursor = conn.cursor()

                try:
                    start_time = time.time()

                    # è¤‡æ•°ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŒ¿å…¥
                    for i in range(10):
                        # thread_idãŒæ–‡å­—åˆ—ã®å ´åˆã¯æ•°å€¤ã«å¤‰æ›
                        thread_id_num = int(thread_id.split('_')[-1]) if isinstance(thread_id, str) else thread_id
                        timestamp = int(time.time()) + thread_id_num * 1000 + i

                        market_data = {
                            "symbol": "BTC/USDT",
                            "timestamp": timestamp,
                            "open_price": 50000 + np.random.normal(0, 100),
                            "high_price": 50000 + np.random.normal(0, 100) + 50,
                            "low_price": 50000 + np.random.normal(0, 100) - 50,
                            "close_price": 50000 + np.random.normal(0, 100),
                            "volume": np.random.exponential(1000)
                        }

                        checksum = self.calculate_data_checksum(market_data)

                        cursor.execute("""
                            INSERT INTO market_data (symbol, timestamp, open_price, high_price, low_price, close_price, volume, checksum)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """, (market_data["symbol"], market_data["timestamp"], market_data["open_price"],
                             market_data["high_price"], market_data["low_price"], market_data["close_price"],
                             market_data["volume"], checksum))

                    conn.commit()
                    execution_time = time.time() - start_time
                    lock_stats["write_operations"] += 1
                    lock_stats["successful_operations"] += 1

                    return {
                        "thread_id": thread_id,
                        "operation": "write",
                        "success": True,
                        "execution_time": execution_time,
                        "records_inserted": 10
                    }

                except sqlite3.OperationalError as e:
                    if "database is locked" in str(e).lower():
                        lock_stats["lock_timeouts"] += 1
                    elif "deadlock" in str(e).lower():
                        lock_stats["deadlocks"] += 1
                    lock_stats["failed_operations"] += 1

                    return {
                        "thread_id": thread_id,
                        "operation": "write",
                        "success": False,
                        "error": str(e)
                    }

                finally:
                    conn.close()

            # åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
            conn = sqlite3.connect(test_db)
            cursor = conn.cursor()

            for i in range(100):
                timestamp = int(time.time()) - 1000 + i
                market_data = {
                    "symbol": "BTC/USDT",
                    "timestamp": timestamp,
                    "open_price": 50000,
                    "high_price": 50100,
                    "low_price": 49900,
                    "close_price": 50000,
                    "volume": 1000
                }

                checksum = self.calculate_data_checksum(market_data)

                cursor.execute("""
                    INSERT INTO market_data (symbol, timestamp, open_price, high_price, low_price, close_price, volume, checksum)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (market_data["symbol"], market_data["timestamp"], market_data["open_price"],
                     market_data["high_price"], market_data["low_price"], market_data["close_price"],
                     market_data["volume"], checksum))

            conn.commit()
            conn.close()

            # ä¸¦è¡Œæ“ä½œã§ãƒ­ãƒƒã‚¯ç«¶åˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            num_read_threads = 5
            num_write_threads = 3

            start_time = time.time()

            with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
                futures = []

                # èª­ã¿å–ã‚Šã‚¹ãƒ¬ãƒƒãƒ‰
                for i in range(num_read_threads):
                    future = executor.submit(heavy_read_operation, f"read_{i}")
                    futures.append(future)

                # æ›¸ãè¾¼ã¿ã‚¹ãƒ¬ãƒƒãƒ‰
                for i in range(num_write_threads):
                    future = executor.submit(heavy_write_operation, f"write_{i}")
                    futures.append(future)

                results = [future.result() for future in futures]

            total_time = time.time() - start_time

            # çµæœåˆ†æ
            successful_reads = sum(1 for r in results if r["operation"] == "read" and r["success"])
            successful_writes = sum(1 for r in results if r["operation"] == "write" and r["success"])
            failed_operations = sum(1 for r in results if not r["success"])

            logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒƒã‚¯ç«¶åˆãƒ†ã‚¹ãƒˆçµæœ:")
            logger.info(f"  ç·æ“ä½œæ•°: {len(results)}")
            logger.info(f"  æˆåŠŸã—ãŸèª­ã¿å–ã‚Š: {successful_reads}/{num_read_threads}")
            logger.info(f"  æˆåŠŸã—ãŸæ›¸ãè¾¼ã¿: {successful_writes}/{num_write_threads}")
            logger.info(f"  å¤±æ•—ã—ãŸæ“ä½œ: {failed_operations}")
            logger.info(f"  ãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {lock_stats['lock_timeouts']}")
            logger.info(f"  ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯: {lock_stats['deadlocks']}")
            logger.info(f"  ç·å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")

            # ãƒ­ãƒƒã‚¯ç«¶åˆå‡¦ç†ã®ç¢ºèª
            success_rate = (successful_reads + successful_writes) / len(results)
            assert success_rate >= 0.7, f"æˆåŠŸç‡ãŒä½ã™ãã¾ã™: {success_rate:.1%}"
            assert lock_stats["deadlocks"] == 0, f"ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {lock_stats['deadlocks']}"

            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æœ€çµ‚ç¢ºèª
            conn = sqlite3.connect(test_db)
            cursor = conn.cursor()

            cursor.execute("SELECT COUNT(*) FROM market_data")
            final_count = cursor.fetchone()[0]

            expected_min_count = 100 + (successful_writes * 10)  # åˆæœŸ100 + æ›¸ãè¾¼ã¿æˆåŠŸæ•° * 10
            assert final_count >= expected_min_count, f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãŒæœŸå¾…å€¤ã‚’ä¸‹å›ã‚Šã¾ã™: {final_count} < {expected_min_count}"

            conn.close()

            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒƒã‚¯ç«¶åˆãƒ†ã‚¹ãƒˆæˆåŠŸ")

        except Exception as e:
            pytest.fail(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒƒã‚¯ç«¶åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_instance = TestDataConsistency()
    
    tests = [
        test_instance.test_multi_source_data_consistency,
        test_instance.test_backup_restore_consistency,
        test_instance.test_distributed_processing_synchronization,
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            test_instance.setup_method()
            test()
            test_instance.teardown_method()
            passed += 1
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆå¤±æ•—: {test.__name__}: {e}")
            failed += 1
    
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ»ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆçµæœ: æˆåŠŸ {passed}, å¤±æ•— {failed}")
    print(f"æˆåŠŸç‡: {passed / (passed + failed) * 100:.1f}%")
