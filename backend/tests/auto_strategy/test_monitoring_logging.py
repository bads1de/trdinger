"""
ã‚ªãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ ç›£è¦–ãƒ»ãƒ­ã‚°ãƒ†ã‚¹ãƒˆ

ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ©ãƒ¼ãƒˆã€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ã‚¨ãƒ©ãƒ¼è¿½è·¡ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, backend_dir)

import pytest
import pandas as pd
import numpy as np
import time
import logging
import tempfile
import shutil
import json
import threading
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import psutil
import traceback
from collections import deque
import warnings

logger = logging.getLogger(__name__)


class CustomLogHandler(logging.Handler):
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self):
        super().__init__()
        self.logs = deque(maxlen=10000)
        self.error_count = 0
        self.warning_count = 0
        self.info_count = 0
        
    def emit(self, record):
        log_entry = {
            "timestamp": datetime.fromtimestamp(record.created),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        
        if record.exc_info:
            log_entry["exception"] = traceback.format_exception(*record.exc_info)
        
        self.logs.append(log_entry)
        
        if record.levelno >= logging.ERROR:
            self.error_count += 1
        elif record.levelno >= logging.WARNING:
            self.warning_count += 1
        elif record.levelno >= logging.INFO:
            self.info_count += 1


class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.metrics = {
            "cpu_usage": deque(maxlen=1000),
            "memory_usage": deque(maxlen=1000),
            "response_times": deque(maxlen=1000),
            "error_rates": deque(maxlen=1000),
            "throughput": deque(maxlen=1000)
        }
        self.start_time = time.time()
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
    
    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring:
            try:
                # CPUä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=0.1)
                self.metrics["cpu_usage"].append(cpu_percent)
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
                memory_info = psutil.virtual_memory()
                self.metrics["memory_usage"].append(memory_info.percent)
                
                time.sleep(0.5)
                
            except Exception as e:
                logger.warning(f"ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def record_response_time(self, response_time: float):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’è¨˜éŒ²"""
        self.metrics["response_times"].append(response_time)
    
    def record_error_rate(self, error_rate: float):
        """ã‚¨ãƒ©ãƒ¼ç‡ã‚’è¨˜éŒ²"""
        self.metrics["error_rates"].append(error_rate)
    
    def record_throughput(self, throughput: float):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’è¨˜éŒ²"""
        self.metrics["throughput"].append(throughput)
    
    def get_statistics(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        stats = {}
        
        for metric_name, values in self.metrics.items():
            if values:
                stats[metric_name] = {
                    "count": len(values),
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "min": min(values),
                    "max": max(values),
                    "p50": np.percentile(values, 50),
                    "p95": np.percentile(values, 95),
                    "p99": np.percentile(values, 99)
                }
            else:
                stats[metric_name] = {"count": 0}
        
        return stats


class TestMonitoringLogging:
    """ç›£è¦–ãƒ»ãƒ­ã‚°ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆå‰ã®æº–å‚™"""
        self.start_time = time.time()
        self.temp_dir = tempfile.mkdtemp()
        self.log_handler = CustomLogHandler()
        self.performance_monitor = PerformanceMonitor()
        
        # ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
        root_logger = logging.getLogger()
        root_logger.addHandler(self.log_handler)
        root_logger.setLevel(logging.DEBUG)
        
    def teardown_method(self):
        """ãƒ†ã‚¹ãƒˆå¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        execution_time = time.time() - self.start_time
        logger.info(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“: {execution_time:.3f}ç§’")
        
        # ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤
        root_logger = logging.getLogger()
        root_logger.removeHandler(self.log_handler)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–åœæ­¢
        self.performance_monitor.stop_monitoring()
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            shutil.rmtree(self.temp_dir)
        except:
            pass
    
    def test_anomaly_detection_alerts(self):
        """ãƒ†ã‚¹ãƒˆ63: ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ©ãƒ¼ãƒˆã®æ­£ç¢ºæ€§"""
        logger.info("ğŸ” ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ©ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            from app.services.auto_strategy.calculators.tpsl_calculator import TPSLCalculator
            
            calculator = TPSLCalculator()
            
            # ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
            class AnomalyDetector:
                def __init__(self):
                    self.baseline_metrics = {
                        "response_time": {"mean": 0.01, "std": 0.005, "threshold": 3.0},
                        "error_rate": {"mean": 0.001, "std": 0.0005, "threshold": 5.0},
                        "cpu_usage": {"mean": 20.0, "std": 5.0, "threshold": 3.0},
                        "memory_usage": {"mean": 50.0, "std": 10.0, "threshold": 2.5}
                    }
                    self.alerts = []
                
                def check_anomaly(self, metric_name: str, value: float) -> bool:
                    """ç•°å¸¸å€¤ã‚’ãƒã‚§ãƒƒã‚¯"""
                    if metric_name not in self.baseline_metrics:
                        return False
                    
                    baseline = self.baseline_metrics[metric_name]
                    z_score = abs(value - baseline["mean"]) / baseline["std"]
                    
                    if z_score > baseline["threshold"]:
                        alert = {
                            "timestamp": datetime.now(),
                            "metric": metric_name,
                            "value": value,
                            "z_score": z_score,
                            "threshold": baseline["threshold"],
                            "severity": "HIGH" if z_score > baseline["threshold"] * 1.5 else "MEDIUM"
                        }
                        self.alerts.append(alert)
                        logger.warning(f"ç•°å¸¸æ¤œçŸ¥: {metric_name}={value:.4f} (Z-score: {z_score:.2f})")
                        return True
                    
                    return False
            
            detector = AnomalyDetector()
            
            # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã¨ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’æ··åœ¨ã•ã›ã¦ãƒ†ã‚¹ãƒˆ
            test_scenarios = [
                # æ­£å¸¸ãªã‚·ãƒŠãƒªã‚ª
                {"response_time": 0.008, "error_rate": 0.0008, "cpu_usage": 18.0, "memory_usage": 45.0},
                {"response_time": 0.012, "error_rate": 0.0012, "cpu_usage": 22.0, "memory_usage": 55.0},
                {"response_time": 0.009, "error_rate": 0.0009, "cpu_usage": 19.0, "memory_usage": 48.0},
                
                # ç•°å¸¸ãªã‚·ãƒŠãƒªã‚ª
                {"response_time": 0.050, "error_rate": 0.001, "cpu_usage": 20.0, "memory_usage": 50.0},  # é«˜ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
                {"response_time": 0.010, "error_rate": 0.008, "cpu_usage": 20.0, "memory_usage": 50.0},  # é«˜ã‚¨ãƒ©ãƒ¼ç‡
                {"response_time": 0.010, "error_rate": 0.001, "cpu_usage": 45.0, "memory_usage": 50.0},  # é«˜CPUä½¿ç”¨ç‡
                {"response_time": 0.010, "error_rate": 0.001, "cpu_usage": 20.0, "memory_usage": 85.0},  # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
                
                # è¤‡åˆç•°å¸¸
                {"response_time": 0.080, "error_rate": 0.010, "cpu_usage": 50.0, "memory_usage": 90.0},  # è¤‡æ•°ç•°å¸¸
            ]
            
            anomaly_stats = {
                "total_checks": 0,
                "anomalies_detected": 0,
                "false_positives": 0,
                "false_negatives": 0,
                "true_positives": 0,
                "true_negatives": 0
            }
            
            for i, scenario in enumerate(test_scenarios):
                logger.info(f"ã‚·ãƒŠãƒªã‚ª {i+1}: {scenario}")
                
                # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                scenario_anomalies = 0
                expected_anomalies = 0
                
                # æœŸå¾…ã•ã‚Œã‚‹ç•°å¸¸æ•°ã‚’è¨ˆç®—ï¼ˆæ‰‹å‹•ã§è¨­å®šï¼‰
                if i >= 3:  # ç•°å¸¸ã‚·ãƒŠãƒªã‚ª
                    if i == 3:  # é«˜ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
                        expected_anomalies = 1
                    elif i == 4:  # é«˜ã‚¨ãƒ©ãƒ¼ç‡
                        expected_anomalies = 1
                    elif i == 5:  # é«˜CPUä½¿ç”¨ç‡
                        expected_anomalies = 1
                    elif i == 6:  # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
                        expected_anomalies = 1
                    elif i == 7:  # è¤‡åˆç•°å¸¸
                        expected_anomalies = 4
                
                for metric_name, value in scenario.items():
                    anomaly_stats["total_checks"] += 1
                    is_anomaly = detector.check_anomaly(metric_name, value)
                    
                    if is_anomaly:
                        scenario_anomalies += 1
                        anomaly_stats["anomalies_detected"] += 1
                
                # ç²¾åº¦ã®è©•ä¾¡
                if expected_anomalies > 0:  # ç•°å¸¸ãŒæœŸå¾…ã•ã‚Œã‚‹å ´åˆ
                    if scenario_anomalies > 0:
                        anomaly_stats["true_positives"] += 1
                    else:
                        anomaly_stats["false_negatives"] += 1
                else:  # æ­£å¸¸ãŒæœŸå¾…ã•ã‚Œã‚‹å ´åˆ
                    if scenario_anomalies == 0:
                        anomaly_stats["true_negatives"] += 1
                    else:
                        anomaly_stats["false_positives"] += 1
                
                # å®Ÿéš›ã®å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                try:
                    start_time = time.time()
                    sl_price, tp_price = calculator.calculate_basic_tpsl_prices(50000, 0.02, 0.04, 1.0)
                    response_time = time.time() - start_time
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®ç•°å¸¸ãƒã‚§ãƒƒã‚¯
                    detector.check_anomaly("response_time", response_time)
                    
                except Exception as e:
                    logger.error(f"ã‚·ãƒŠãƒªã‚ª {i+1} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    detector.check_anomaly("error_rate", 1.0)  # 100%ã‚¨ãƒ©ãƒ¼ç‡
            
            # ç•°å¸¸æ¤œçŸ¥ã®ç²¾åº¦åˆ†æ
            total_scenarios = len(test_scenarios)
            precision = anomaly_stats["true_positives"] / (anomaly_stats["true_positives"] + anomaly_stats["false_positives"]) if (anomaly_stats["true_positives"] + anomaly_stats["false_positives"]) > 0 else 0
            recall = anomaly_stats["true_positives"] / (anomaly_stats["true_positives"] + anomaly_stats["false_negatives"]) if (anomaly_stats["true_positives"] + anomaly_stats["false_negatives"]) > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            logger.info(f"ç•°å¸¸æ¤œçŸ¥çµæœ:")
            logger.info(f"  ç·ãƒã‚§ãƒƒã‚¯æ•°: {anomaly_stats['total_checks']}")
            logger.info(f"  æ¤œçŸ¥ã•ã‚ŒãŸç•°å¸¸: {anomaly_stats['anomalies_detected']}")
            logger.info(f"  çœŸé™½æ€§: {anomaly_stats['true_positives']}")
            logger.info(f"  çœŸé™°æ€§: {anomaly_stats['true_negatives']}")
            logger.info(f"  å½é™½æ€§: {anomaly_stats['false_positives']}")
            logger.info(f"  å½é™°æ€§: {anomaly_stats['false_negatives']}")
            logger.info(f"  ç²¾åº¦ (Precision): {precision:.3f}")
            logger.info(f"  å†ç¾ç‡ (Recall): {recall:.3f}")
            logger.info(f"  F1ã‚¹ã‚³ã‚¢: {f1_score:.3f}")
            logger.info(f"  ã‚¢ãƒ©ãƒ¼ãƒˆæ•°: {len(detector.alerts)}")
            
            # ç•°å¸¸æ¤œçŸ¥ã®è¦ä»¶ç¢ºèª
            assert len(detector.alerts) > 0, "ç•°å¸¸ãŒæ¤œçŸ¥ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
            assert precision >= 0.7, f"ç²¾åº¦ãŒä½ã™ãã¾ã™: {precision:.3f}"
            assert recall >= 0.7, f"å†ç¾ç‡ãŒä½ã™ãã¾ã™: {recall:.3f}"
            assert f1_score >= 0.7, f"F1ã‚¹ã‚³ã‚¢ãŒä½ã™ãã¾ã™: {f1_score:.3f}"
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆã®è©³ç´°ç¢ºèª
            high_severity_alerts = [alert for alert in detector.alerts if alert["severity"] == "HIGH"]
            medium_severity_alerts = [alert for alert in detector.alerts if alert["severity"] == "MEDIUM"]
            
            logger.info(f"é«˜é‡è¦åº¦ã‚¢ãƒ©ãƒ¼ãƒˆ: {len(high_severity_alerts)}ä»¶")
            logger.info(f"ä¸­é‡è¦åº¦ã‚¢ãƒ©ãƒ¼ãƒˆ: {len(medium_severity_alerts)}ä»¶")
            
            logger.info("âœ… ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ©ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
            
        except Exception as e:
            pytest.fail(f"ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ©ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def test_log_file_integrity_rotation(self):
        """ãƒ†ã‚¹ãƒˆ64: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å®Œæ•´æ€§ã¨ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        logger.info("ğŸ” ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å®Œæ•´æ€§ãƒ»ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
            class LogFileManager:
                def __init__(self, log_dir: str, max_file_size: int = 1024*1024, max_files: int = 5):
                    self.log_dir = log_dir
                    self.max_file_size = max_file_size
                    self.max_files = max_files
                    self.current_file = None
                    self.current_size = 0
                    self.file_count = 0
                    
                    os.makedirs(log_dir, exist_ok=True)
                    self._create_new_log_file()
                
                def _create_new_log_file(self):
                    """æ–°ã—ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"auto_strategy_{timestamp}_{self.file_count}.log"
                    filepath = os.path.join(self.log_dir, filename)
                    
                    if self.current_file:
                        self.current_file.close()
                    
                    self.current_file = open(filepath, 'w', encoding='utf-8')
                    self.current_size = 0
                    self.file_count += 1
                    
                    return filepath
                
                def write_log(self, message: str):
                    """ãƒ­ã‚°ã‚’æ›¸ãè¾¼ã¿"""
                    if not self.current_file:
                        self._create_new_log_file()
                    
                    log_entry = f"{datetime.now().isoformat()} - {message}\n"
                    self.current_file.write(log_entry)
                    self.current_file.flush()
                    
                    self.current_size += len(log_entry.encode('utf-8'))
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                    if self.current_size >= self.max_file_size:
                        self._rotate_log_files()
                
                def _rotate_log_files(self):
                    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
                    logger.info(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ (ã‚µã‚¤ã‚º: {self.current_size} bytes)")
                    
                    # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    log_files = sorted([
                        f for f in os.listdir(self.log_dir) 
                        if f.startswith("auto_strategy_") and f.endswith(".log")
                    ])
                    
                    while len(log_files) >= self.max_files:
                        oldest_file = log_files.pop(0)
                        oldest_file_path = os.path.join(self.log_dir, oldest_file)
                        try:
                            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ«ã‚’é–‰ã˜ã¦ã‹ã‚‰å‰Šé™¤ã‚’è©¦è¡Œ
                            if hasattr(self, 'current_file_handle') and self.current_file_handle:
                                self.current_file_handle.close()
                                self.current_file_handle = None

                            os.remove(oldest_file_path)
                            logger.info(f"å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤: {oldest_file}")
                        except (PermissionError, OSError) as e:
                            # Windowsã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½¿ç”¨ä¸­ã®å ´åˆã¯è­¦å‘Šã‚’å‡ºã—ã¦ç¶šè¡Œ
                            logger.warning(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«å¤±æ•—ï¼ˆç¶šè¡Œã—ã¾ã™ï¼‰: {oldest_file} - {e}")
                            break  # å‰Šé™¤ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                    
                    # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                    self._create_new_log_file()
                
                def get_log_files_info(self) -> List[Dict]:
                    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—"""
                    log_files = []
                    
                    for filename in os.listdir(self.log_dir):
                        if filename.startswith("auto_strategy_") and filename.endswith(".log"):
                            filepath = os.path.join(self.log_dir, filename)
                            stat = os.stat(filepath)
                            
                            log_files.append({
                                "filename": filename,
                                "size": stat.st_size,
                                "created": datetime.fromtimestamp(stat.st_ctime),
                                "modified": datetime.fromtimestamp(stat.st_mtime)
                            })
                    
                    return sorted(log_files, key=lambda x: x["created"])
                
                def verify_log_integrity(self) -> Dict[str, Any]:
                    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ã‚’æ¤œè¨¼"""
                    integrity_results = {
                        "total_files": 0,
                        "total_size": 0,
                        "corrupted_files": 0,
                        "empty_files": 0,
                        "valid_files": 0,
                        "encoding_errors": 0
                    }
                    
                    for filename in os.listdir(self.log_dir):
                        if filename.startswith("auto_strategy_") and filename.endswith(".log"):
                            filepath = os.path.join(self.log_dir, filename)
                            integrity_results["total_files"] += 1
                            
                            try:
                                file_size = os.path.getsize(filepath)
                                integrity_results["total_size"] += file_size
                                
                                if file_size == 0:
                                    integrity_results["empty_files"] += 1
                                    continue
                                
                                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®æ¤œè¨¼
                                with open(filepath, 'r', encoding='utf-8') as f:
                                    line_count = 0
                                    for line in f:
                                        line_count += 1
                                        # åŸºæœ¬çš„ãªãƒ­ã‚°å½¢å¼ãƒã‚§ãƒƒã‚¯
                                        if not line.strip():
                                            continue
                                        
                                        # ISOå½¢å¼ã®æ—¥æ™‚ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                        if " - " not in line:
                                            raise ValueError(f"Invalid log format in line {line_count}")
                                
                                integrity_results["valid_files"] += 1
                                
                            except UnicodeDecodeError:
                                integrity_results["encoding_errors"] += 1
                                logger.warning(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {filename}")
                            except Exception as e:
                                integrity_results["corrupted_files"] += 1
                                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ç ´æ: {filename} - {e}")
                    
                    return integrity_results
                
                def close(self):
                    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’é–‰ã˜ã‚‹"""
                    if self.current_file:
                        self.current_file.close()
            
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ãƒ†ã‚¹ãƒˆ
            log_manager = LogFileManager(
                log_dir=os.path.join(self.temp_dir, "logs"),
                max_file_size=1024,  # 1KBï¼ˆãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ãè¨­å®šï¼‰
                max_files=3
            )
            
            # å¤§é‡ã®ãƒ­ã‚°ã‚’ç”Ÿæˆã—ã¦ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ†ã‚¹ãƒˆ
            log_messages = []
            for i in range(100):
                message = f"Test log message {i:03d} - Processing auto strategy calculation with various parameters and detailed information"
                log_manager.write_log(message)
                log_messages.append(message)
                
                # æ™‚ã€…ç•°ãªã‚‹ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’æ··åœ¨
                if i % 10 == 0:
                    log_manager.write_log(f"INFO: Checkpoint reached at iteration {i}")
                elif i % 15 == 0:
                    log_manager.write_log(f"WARNING: High CPU usage detected at iteration {i}")
                elif i % 25 == 0:
                    log_manager.write_log(f"ERROR: Simulated error at iteration {i}")
            
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®å–å¾—
            log_files_info = log_manager.get_log_files_info()
            
            logger.info(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±:")
            for file_info in log_files_info:
                logger.info(f"  {file_info['filename']}: {file_info['size']} bytes, ä½œæˆ: {file_info['created']}")
            
            # æ•´åˆæ€§æ¤œè¨¼
            integrity_results = log_manager.verify_log_integrity()
            
            logger.info(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ•´åˆæ€§æ¤œè¨¼çµæœ:")
            logger.info(f"  ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {integrity_results['total_files']}")
            logger.info(f"  ç·ã‚µã‚¤ã‚º: {integrity_results['total_size']} bytes")
            logger.info(f"  æœ‰åŠ¹ãƒ•ã‚¡ã‚¤ãƒ«: {integrity_results['valid_files']}")
            logger.info(f"  ç ´æãƒ•ã‚¡ã‚¤ãƒ«: {integrity_results['corrupted_files']}")
            logger.info(f"  ç©ºãƒ•ã‚¡ã‚¤ãƒ«: {integrity_results['empty_files']}")
            logger.info(f"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {integrity_results['encoding_errors']}")
            
            # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã®ç¢ºèª
            assert integrity_results["total_files"] <= 5, f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ã¾ã™: {integrity_results['total_files']}"
            assert integrity_results["total_files"] > 1, "ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            # æ•´åˆæ€§ã®ç¢ºèª
            assert integrity_results["corrupted_files"] == 0, f"ç ´æãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã™: {integrity_results['corrupted_files']}"
            assert integrity_results["encoding_errors"] == 0, f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Šã¾ã™: {integrity_results['encoding_errors']}"
            # ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æœ‰åŠ¹ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æ‰±ã†
            valid_files_including_empty = integrity_results["valid_files"] + integrity_results["empty_files"]
            assert valid_files_including_empty == integrity_results["total_files"], "ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã™"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
            for file_info in log_files_info[:-1]:  # æœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ä»¥å¤–
                assert file_info["size"] >= 1024, f"ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™: {file_info['size']}"
            
            log_manager.close()
            
            logger.info("âœ… ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å®Œæ•´æ€§ãƒ»ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆæˆåŠŸ")

        except Exception as e:
            pytest.fail(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å®Œæ•´æ€§ãƒ»ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    def test_performance_metrics_collection(self):
        """ãƒ†ã‚¹ãƒˆ65: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åé›†ç²¾åº¦"""
        logger.info("ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            from app.services.auto_strategy.calculators.tpsl_calculator import TPSLCalculator

            calculator = TPSLCalculator()

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹
            self.performance_monitor.start_monitoring()

            # è² è·ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª
            test_scenarios = [
                {"name": "è»½è² è·", "iterations": 100, "complexity": "low"},
                {"name": "ä¸­è² è·", "iterations": 500, "complexity": "medium"},
                {"name": "é«˜è² è·", "iterations": 1000, "complexity": "high"}
            ]

            scenario_results = {}

            for scenario in test_scenarios:
                scenario_name = scenario["name"]
                iterations = scenario["iterations"]
                complexity = scenario["complexity"]

                logger.info(f"{scenario_name}ãƒ†ã‚¹ãƒˆé–‹å§‹ ({iterations}å›åå¾©)")

                scenario_start = time.time()
                response_times = []
                error_count = 0

                for i in range(iterations):
                    operation_start = time.time()

                    try:
                        # è¤‡é›‘åº¦ã«å¿œã˜ãŸå‡¦ç†
                        if complexity == "low":
                            # å˜ç´”ãªè¨ˆç®—
                            price = 50000 + np.random.normal(0, 100)
                            sl_price, tp_price = calculator.calculate_basic_tpsl_prices(price, 0.02, 0.04, 1.0)

                        elif complexity == "medium":
                            # ä¸­ç¨‹åº¦ã®è¨ˆç®—
                            for _ in range(5):
                                price = 50000 + np.random.normal(0, 100)
                                sl_price, tp_price = calculator.calculate_basic_tpsl_prices(price, 0.02, 0.04, 1.0)

                        elif complexity == "high":
                            # è¤‡é›‘ãªè¨ˆç®—
                            for _ in range(10):
                                price = 50000 + np.random.normal(0, 100)
                                sl_pct = 0.01 + np.random.uniform(0, 0.03)
                                tp_pct = 0.02 + np.random.uniform(0, 0.06)
                                direction = np.random.choice([1.0, -1.0])
                                sl_price, tp_price = calculator.calculate_basic_tpsl_prices(price, sl_pct, tp_pct, direction)

                        operation_time = time.time() - operation_start
                        response_times.append(operation_time)

                        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã«è¨˜éŒ²
                        self.performance_monitor.record_response_time(operation_time)

                    except Exception as e:
                        error_count += 1
                        logger.debug(f"{scenario_name} åå¾© {i+1} ã‚¨ãƒ©ãƒ¼: {e}")

                    # é€²æ—è¡¨ç¤º
                    if (i + 1) % (iterations // 10) == 0:
                        progress = (i + 1) / iterations * 100
                        logger.info(f"  é€²æ—: {progress:.0f}%")

                scenario_time = time.time() - scenario_start

                # ã‚·ãƒŠãƒªã‚ªçµæœã®åˆ†æ
                if response_times:
                    avg_response_time = np.mean(response_times)
                    p95_response_time = np.percentile(response_times, 95)
                    p99_response_time = np.percentile(response_times, 99)
                    throughput = len(response_times) / scenario_time
                    error_rate = error_count / iterations
                else:
                    avg_response_time = 0
                    p95_response_time = 0
                    p99_response_time = 0
                    throughput = 0
                    error_rate = 1.0

                scenario_results[scenario_name] = {
                    "iterations": iterations,
                    "total_time": scenario_time,
                    "avg_response_time": avg_response_time,
                    "p95_response_time": p95_response_time,
                    "p99_response_time": p99_response_time,
                    "throughput": throughput,
                    "error_count": error_count,
                    "error_rate": error_rate,
                    "success_rate": 1.0 - error_rate
                }

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã«è¨˜éŒ²
                self.performance_monitor.record_throughput(throughput)
                self.performance_monitor.record_error_rate(error_rate)

                logger.info(f"{scenario_name}ãƒ†ã‚¹ãƒˆå®Œäº†:")
                logger.info(f"  ç·æ™‚é–“: {scenario_time:.3f}ç§’")
                logger.info(f"  å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {avg_response_time*1000:.2f}ms")
                logger.info(f"  95%ileãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {p95_response_time*1000:.2f}ms")
                logger.info(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f}ops/sec")
                logger.info(f"  ã‚¨ãƒ©ãƒ¼ç‡: {error_rate:.1%}")

            # ç›£è¦–åœæ­¢
            time.sleep(1)  # æœ€å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†
            self.performance_monitor.stop_monitoring()

            # å…¨ä½“çµ±è¨ˆã®å–å¾—
            overall_stats = self.performance_monitor.get_statistics()

            logger.info(f"å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ:")
            for metric_name, stats in overall_stats.items():
                if stats["count"] > 0:
                    logger.info(f"  {metric_name}:")
                    logger.info(f"    ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats['count']}")
                    logger.info(f"    å¹³å‡: {stats['mean']:.4f}")
                    logger.info(f"    æ¨™æº–åå·®: {stats['std']:.4f}")
                    logger.info(f"    æœ€å°: {stats['min']:.4f}")
                    logger.info(f"    æœ€å¤§: {stats['max']:.4f}")
                    logger.info(f"    95%ile: {stats['p95']:.4f}")

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ã®ç¢ºèª
            for scenario_name, result in scenario_results.items():
                # æˆåŠŸç‡ã®ç¢ºèª
                assert result["success_rate"] >= 0.95, f"{scenario_name}: æˆåŠŸç‡ãŒä½ã™ãã¾ã™: {result['success_rate']:.1%}"

                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®ç¢ºèª
                if scenario_name == "è»½è² è·":
                    assert result["avg_response_time"] < 0.01, f"{scenario_name}: å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒé•·ã™ãã¾ã™: {result['avg_response_time']*1000:.2f}ms"
                elif scenario_name == "ä¸­è² è·":
                    assert result["avg_response_time"] < 0.05, f"{scenario_name}: å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒé•·ã™ãã¾ã™: {result['avg_response_time']*1000:.2f}ms"
                elif scenario_name == "é«˜è² è·":
                    assert result["avg_response_time"] < 0.1, f"{scenario_name}: å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒé•·ã™ãã¾ã™: {result['avg_response_time']*1000:.2f}ms"

                # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ç¢ºèª
                assert result["throughput"] > 10, f"{scenario_name}: ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒä½ã™ãã¾ã™: {result['throughput']:.1f}ops/sec"

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®ç²¾åº¦ç¢ºèª
            response_time_stats = overall_stats.get("response_times", {})
            if response_time_stats.get("count", 0) > 0:
                # åé›†ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå¦¥å½“ãªç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                assert response_time_stats["mean"] > 0, "å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒç„¡åŠ¹ã§ã™"
                assert response_time_stats["std"] >= 0, "æ¨™æº–åå·®ãŒç„¡åŠ¹ã§ã™"
                assert response_time_stats["min"] >= 0, "æœ€å°ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒç„¡åŠ¹ã§ã™"
                assert response_time_stats["max"] >= response_time_stats["min"], "æœ€å¤§å€¤ãŒæœ€å°å€¤ã‚ˆã‚Šå°ã•ã„ã§ã™"

            logger.info("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ†ã‚¹ãƒˆæˆåŠŸ")

        except Exception as e:
            pytest.fail(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    def test_error_tracking_stack_traces(self):
        """ãƒ†ã‚¹ãƒˆ66: ã‚¨ãƒ©ãƒ¼è¿½è·¡ã¨ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®è©³ç´°åº¦"""
        logger.info("ğŸ” ã‚¨ãƒ©ãƒ¼è¿½è·¡ãƒ»ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")

        try:
            # ã‚¨ãƒ©ãƒ¼è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ 
            class ErrorTracker:
                def __init__(self):
                    self.errors = []
                    self.error_counts = {}
                    self.error_patterns = {}

                def track_error(self, error: Exception, context: Dict[str, Any] = None):
                    """ã‚¨ãƒ©ãƒ¼ã‚’è¿½è·¡"""
                    error_info = {
                        "timestamp": datetime.now(),
                        "error_type": type(error).__name__,
                        "error_message": str(error),
                        "stack_trace": traceback.format_exc(),
                        "context": context or {}
                    }

                    self.errors.append(error_info)

                    # ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥ã®ã‚«ã‚¦ãƒ³ãƒˆ
                    error_type = error_info["error_type"]
                    self.error_counts[error_type] = self.error_counts.get(error_type, 0) + 1

                    # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
                    error_pattern = self._extract_error_pattern(error_info["stack_trace"])
                    self.error_patterns[error_pattern] = self.error_patterns.get(error_pattern, 0) + 1

                    logger.error(f"ã‚¨ãƒ©ãƒ¼è¿½è·¡: {error_type} - {error_info['error_message']}")

                def _extract_error_pattern(self, stack_trace: str) -> str:
                    """ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
                    lines = stack_trace.strip().split('\n')
                    if len(lines) >= 2:
                        # æœ€å¾Œã®è¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰ã¨æœ€å¾Œã‹ã‚‰2ç•ªç›®ã®è¡Œï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç®‡æ‰€ï¼‰ã‚’ä½¿ç”¨
                        error_line = lines[-1]
                        location_line = lines[-2] if len(lines) > 1 else ""

                        # ãƒ•ã‚¡ã‚¤ãƒ«åã¨è¡Œç•ªå·ã‚’æŠ½å‡º
                        import re
                        location_match = re.search(r'File "([^"]+)", line (\d+)', location_line)
                        if location_match:
                            filename = os.path.basename(location_match.group(1))
                            line_number = location_match.group(2)
                            return f"{filename}:{line_number}"

                    return "unknown"

                def get_error_summary(self) -> Dict[str, Any]:
                    """ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
                    total_errors = len(self.errors)

                    if total_errors == 0:
                        return {"total_errors": 0}

                    # æœ€ã‚‚å¤šã„ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—
                    most_common_type = max(self.error_counts.items(), key=lambda x: x[1])

                    # æœ€ã‚‚å¤šã„ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
                    most_common_pattern = max(self.error_patterns.items(), key=lambda x: x[1])

                    # æ™‚é–“åˆ¥ã‚¨ãƒ©ãƒ¼åˆ†å¸ƒ
                    error_times = [error["timestamp"] for error in self.errors]
                    time_span = (max(error_times) - min(error_times)).total_seconds() if len(error_times) > 1 else 0
                    error_rate = total_errors / max(time_span, 1)  # ã‚¨ãƒ©ãƒ¼/ç§’

                    return {
                        "total_errors": total_errors,
                        "unique_error_types": len(self.error_counts),
                        "unique_error_patterns": len(self.error_patterns),
                        "most_common_type": most_common_type,
                        "most_common_pattern": most_common_pattern,
                        "error_rate": error_rate,
                        "time_span": time_span
                    }

            error_tracker = ErrorTracker()

            # æ„å›³çš„ã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
            error_test_cases = [
                {
                    "name": "ã‚¼ãƒ­é™¤ç®—ã‚¨ãƒ©ãƒ¼",
                    "function": lambda: 1 / 0,
                    "context": {"operation": "division", "value": 0}
                },
                {
                    "name": "å‹ã‚¨ãƒ©ãƒ¼",
                    "function": lambda: "string" + 123,
                    "context": {"operation": "concatenation", "types": ["str", "int"]}
                },
                {
                    "name": "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼",
                    "function": lambda: [1, 2, 3][10],
                    "context": {"operation": "indexing", "index": 10, "list_length": 3}
                },
                {
                    "name": "ã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼",
                    "function": lambda: {"a": 1}["b"],
                    "context": {"operation": "dict_access", "key": "b", "available_keys": ["a"]}
                },
                {
                    "name": "å€¤ã‚¨ãƒ©ãƒ¼",
                    "function": lambda: int("not_a_number"),
                    "context": {"operation": "conversion", "value": "not_a_number", "target_type": "int"}
                },
                {
                    "name": "å±æ€§ã‚¨ãƒ©ãƒ¼",
                    "function": lambda: "string".nonexistent_method(),
                    "context": {"operation": "method_call", "object_type": "str", "method": "nonexistent_method"}
                }
            ]

            # å„ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã‚’è¤‡æ•°å›å®Ÿè¡Œ
            for test_case in error_test_cases:
                for attempt in range(3):  # å„ã‚¨ãƒ©ãƒ¼ã‚’3å›ç™ºç”Ÿã•ã›ã‚‹
                    try:
                        test_case["function"]()
                    except Exception as e:
                        context = test_case["context"].copy()
                        context["test_case"] = test_case["name"]
                        context["attempt"] = attempt + 1
                        error_tracker.track_error(e, context)

            # å®Ÿéš›ã®å‡¦ç†ã§ã®ã‚¨ãƒ©ãƒ¼ã‚‚ãƒ†ã‚¹ãƒˆ
            from app.services.auto_strategy.calculators.tpsl_calculator import TPSLCalculator
            calculator = TPSLCalculator()

            # ç„¡åŠ¹ãªå…¥åŠ›ã§ã®ã‚¨ãƒ©ãƒ¼
            invalid_inputs = [
                {"price": None, "sl_pct": 0.02, "tp_pct": 0.04, "direction": 1.0},
                {"price": "invalid", "sl_pct": 0.02, "tp_pct": 0.04, "direction": 1.0},
                {"price": 50000, "sl_pct": None, "tp_pct": 0.04, "direction": 1.0},
                {"price": 50000, "sl_pct": 0.02, "tp_pct": "invalid", "direction": 1.0},
                {"price": 50000, "sl_pct": 0.02, "tp_pct": 0.04, "direction": "invalid"},
                {"price": float('inf'), "sl_pct": 0.02, "tp_pct": 0.04, "direction": 1.0},
                {"price": float('nan'), "sl_pct": 0.02, "tp_pct": 0.04, "direction": 1.0},
            ]

            for i, inputs in enumerate(invalid_inputs):
                try:
                    calculator.calculate_basic_tpsl_prices(
                        inputs["price"], inputs["sl_pct"], inputs["tp_pct"], inputs["direction"]
                    )
                except Exception as e:
                    context = {
                        "operation": "tpsl_calculation",
                        "input_index": i,
                        "inputs": inputs
                    }
                    error_tracker.track_error(e, context)

            # ã‚¨ãƒ©ãƒ¼è¿½è·¡çµæœã®åˆ†æ
            error_summary = error_tracker.get_error_summary()

            logger.info(f"ã‚¨ãƒ©ãƒ¼è¿½è·¡çµæœ:")
            logger.info(f"  ç·ã‚¨ãƒ©ãƒ¼æ•°: {error_summary['total_errors']}")
            logger.info(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—æ•°: {error_summary['unique_error_types']}")
            logger.info(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {error_summary['unique_error_patterns']}")
            logger.info(f"  æœ€å¤šã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {error_summary['most_common_type'][0]} ({error_summary['most_common_type'][1]}å›)")
            logger.info(f"  æœ€å¤šã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³: {error_summary['most_common_pattern'][0]} ({error_summary['most_common_pattern'][1]}å›)")
            logger.info(f"  ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç‡: {error_summary['error_rate']:.3f}ã‚¨ãƒ©ãƒ¼/ç§’")

            # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
            logger.info("ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:")
            for error_type, count in sorted(error_tracker.error_counts.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"  {error_type}: {count}å›")

            # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®è©³ç´°åº¦ç¢ºèª
            detailed_traces = 0
            for error in error_tracker.errors:
                stack_trace = error["stack_trace"]

                # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®åŸºæœ¬è¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
                has_file_info = "File " in stack_trace
                has_line_number = "line " in stack_trace
                has_function_name = "in " in stack_trace
                has_error_message = error["error_message"] in stack_trace

                if has_file_info and has_line_number and has_function_name and has_error_message:
                    detailed_traces += 1

                # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®è¡Œæ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆè©³ç´°åº¦ã®æŒ‡æ¨™ï¼‰
                trace_lines = len(stack_trace.strip().split('\n'))
                assert trace_lines >= 2, f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãŒçŸ­ã™ãã¾ã™: {trace_lines}è¡Œ"

            detail_rate = detailed_traces / len(error_tracker.errors) if error_tracker.errors else 0

            logger.info(f"è©³ç´°ãªã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ç‡: {detail_rate:.1%}")

            # ã‚¨ãƒ©ãƒ¼è¿½è·¡ã®è¦ä»¶ç¢ºèª
            assert error_summary["total_errors"] > 0, "ã‚¨ãƒ©ãƒ¼ãŒè¿½è·¡ã•ã‚Œã¦ã„ã¾ã›ã‚“"
            assert error_summary["unique_error_types"] >= 5, f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã®å¤šæ§˜æ€§ãŒä¸è¶³: {error_summary['unique_error_types']}"
            assert detail_rate >= 0.9, f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®è©³ç´°åº¦ãŒä¸è¶³: {detail_rate:.1%}"

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã®ç¢ºèª
            context_errors = [error for error in error_tracker.errors if error["context"]]
            context_rate = len(context_errors) / len(error_tracker.errors) if error_tracker.errors else 0

            logger.info(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ä»˜ãã‚¨ãƒ©ãƒ¼ç‡: {context_rate:.1%}")
            assert context_rate >= 0.8, f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ãŒä¸è¶³: {context_rate:.1%}"

            logger.info("âœ… ã‚¨ãƒ©ãƒ¼è¿½è·¡ãƒ»ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆæˆåŠŸ")

        except Exception as e:
            pytest.fail(f"ã‚¨ãƒ©ãƒ¼è¿½è·¡ãƒ»ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_instance = TestMonitoringLogging()
    
    tests = [
        test_instance.test_anomaly_detection_alerts,
        test_instance.test_log_file_integrity_rotation,
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            test_instance.setup_method()
            test()
            test_instance.teardown_method()
            passed += 1
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆå¤±æ•—: {test.__name__}: {e}")
            failed += 1
    
    print(f"\nğŸ“Š ç›£è¦–ãƒ»ãƒ­ã‚°ãƒ†ã‚¹ãƒˆçµæœ: æˆåŠŸ {passed}, å¤±æ•— {failed}")
    print(f"æˆåŠŸç‡: {passed / (passed + failed) * 100:.1f}%")
