"""
ã‚ªãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€åŒæ™‚æ¥ç¶šã€CPU/ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, backend_dir)

import pytest
import pandas as pd
import numpy as np
import time
import psutil
import threading
import concurrent.futures
import gc
import logging
from typing import Dict, Any
from collections import deque

logger = logging.getLogger(__name__)


class TestPerformance:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆå‰ã®æº–å‚™"""
        self.start_time = time.time()
        self.start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        self.start_cpu_percent = psutil.cpu_percent(interval=1)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        self.performance_data = {
            "memory_samples": deque(maxlen=1000),
            "cpu_samples": deque(maxlen=1000),
            "response_times": deque(maxlen=1000)
        }
        self.monitoring_active = True
        
        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        self.monitor_thread = threading.Thread(target=self._monitor_performance, daemon=True)
        self.monitor_thread.start()
        
    def teardown_method(self):
        """ãƒ†ã‚¹ãƒˆå¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.monitoring_active = False
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        end_cpu_percent = psutil.cpu_percent(interval=1)
        
        execution_time = end_time - self.start_time
        memory_delta = end_memory - self.start_memory
        cpu_delta = end_cpu_percent - self.start_cpu_percent
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        if self.performance_data["memory_samples"]:
            max_memory = max(self.performance_data["memory_samples"])
            avg_memory = sum(self.performance_data["memory_samples"]) / len(self.performance_data["memory_samples"])
            logger.info(f"ãƒ¡ãƒ¢ãƒª: é–‹å§‹={self.start_memory:.1f}MB, æœ€å¤§={max_memory:.1f}MB, å¹³å‡={avg_memory:.1f}MB, å¤‰åŒ–={memory_delta:+.1f}MB")
        
        if self.performance_data["cpu_samples"]:
            max_cpu = max(self.performance_data["cpu_samples"])
            avg_cpu = sum(self.performance_data["cpu_samples"]) / len(self.performance_data["cpu_samples"])
            logger.info(f"CPU: é–‹å§‹={self.start_cpu_percent:.1f}%, æœ€å¤§={max_cpu:.1f}%, å¹³å‡={avg_cpu:.1f}%, å¤‰åŒ–={cpu_delta:+.1f}%")
        
        logger.info(f"å®Ÿè¡Œæ™‚é–“: {execution_time:.3f}ç§’")
        
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        gc.collect()
    
    def _monitor_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç›£è¦–"""
        while self.monitoring_active:
            try:
                memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
                cpu_percent = psutil.cpu_percent(interval=None)
                
                self.performance_data["memory_samples"].append(memory_mb)
                self.performance_data["cpu_samples"].append(cpu_percent)
                
                time.sleep(0.5)  # 0.5ç§’é–“éš”ã§ç›£è¦–
            except:
                break
    
    def create_large_dataset(self, size: int = 10000) -> pd.DataFrame:
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        np.random.seed(42)
        dates = pd.date_range(start='2020-01-01', periods=size, freq='h')
        
        base_price = 50000
        returns = np.random.normal(0, 0.02, size)
        prices = [base_price]
        
        for ret in returns[1:]:
            prices.append(prices[-1] * (1 + ret))
        
        data = pd.DataFrame({
            'timestamp': dates,
            'Open': [p * (1 + np.random.normal(0, 0.001)) for p in prices],
            'High': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
            'Low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
            'Close': prices,
            'Volume': np.random.exponential(1000, size),
        })
        
        data.set_index('timestamp', inplace=True)
        return data
    
    def test_large_dataset_processing_speed(self):
        """ãƒ†ã‚¹ãƒˆ36: 10,000è¡Œä»¥ä¸Šã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®å‡¦ç†é€Ÿåº¦"""
        logger.info("ğŸ” å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†é€Ÿåº¦ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            from app.services.auto_strategy.services.ml_orchestrator import MLOrchestrator
            
            # 10,000è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            large_data = self.create_large_dataset(10000)
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(large_data)}è¡Œ, {large_data.memory_usage(deep=True).sum() / 1024 / 1024:.1f}MB")
            
            ml_orchestrator = MLOrchestrator(enable_automl=False)  # è»½é‡åŒ–
            
            # å‡¦ç†æ™‚é–“æ¸¬å®š
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            try:
                ml_indicators = ml_orchestrator.calculate_ml_indicators(large_data)
                
                end_time = time.time()
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                processing_time = end_time - start_time
                memory_used = end_memory - start_memory
                
                # çµæœæ¤œè¨¼
                if ml_indicators and "ML_UP_PROB" in ml_indicators:
                    result_size = len(ml_indicators["ML_UP_PROB"])
                    throughput = len(large_data) / processing_time  # è¡Œ/ç§’
                    
                    logger.info(f"å‡¦ç†çµæœ: {result_size}å€‹ã®äºˆæ¸¬å€¤ç”Ÿæˆ")
                    logger.info(f"å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
                    logger.info(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f}è¡Œ/ç§’")
                    logger.info(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:+.1f}MB")
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ã®ç¢ºèª
                    assert processing_time < 300, f"å‡¦ç†æ™‚é–“ãŒé•·ã™ãã¾ã™: {processing_time:.1f}ç§’"  # 5åˆ†ä»¥å†…
                    assert throughput > 10, f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒä½ã™ãã¾ã™: {throughput:.1f}è¡Œ/ç§’"
                    assert memory_used < 1000, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã™ãã¾ã™: {memory_used:.1f}MB"
                    
                else:
                    logger.info("å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§MLæŒ‡æ¨™ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆæœŸå¾…ã•ã‚Œã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™ï¼‰")
                    
            except Exception as e:
                logger.info(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ï¼ˆæœŸå¾…ã•ã‚Œã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™ï¼‰: {e}")
            
            logger.info("âœ… å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†é€Ÿåº¦ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            
        except Exception as e:
            pytest.fail(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†é€Ÿåº¦ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def test_concurrent_connection_limit(self):
        """ãƒ†ã‚¹ãƒˆ37: åŒæ™‚æ¥ç¶šæ•°ã®ä¸Šé™ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ” åŒæ™‚æ¥ç¶šæ•°ä¸Šé™ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            from app.services.auto_strategy.calculators.tpsl_calculator import TPSLCalculator
            
            def simulate_connection(connection_id: int) -> Dict[str, Any]:
                """æ¥ç¶šã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
                try:
                    start_time = time.time()
                    
                    calculator = TPSLCalculator()
                    
                    # è¤‡æ•°ã®è¨ˆç®—ã‚’å®Ÿè¡Œ
                    results = []
                    for _ in range(5):
                        current_price = 50000 + np.random.randint(-1000, 1000)
                        sl_pct = np.random.uniform(0.01, 0.03)
                        tp_pct = np.random.uniform(0.02, 0.06)
                        direction = np.random.choice([1.0, -1.0])
                        
                        sl_price, tp_price = calculator.calculate_basic_tpsl_prices(
                            current_price, sl_pct, tp_pct, direction
                        )
                        
                        if sl_price is not None and tp_price is not None:
                            results.append({"sl": sl_price, "tp": tp_price})
                    
                    execution_time = time.time() - start_time
                    
                    return {
                        "connection_id": connection_id,
                        "success": True,
                        "execution_time": execution_time,
                        "calculations": len(results),
                        "error": None
                    }
                    
                except Exception as e:
                    return {
                        "connection_id": connection_id,
                        "success": False,
                        "execution_time": time.time() - start_time,
                        "calculations": 0,
                        "error": str(e)
                    }
            
            # æ®µéšçš„ã«æ¥ç¶šæ•°ã‚’å¢—åŠ 
            connection_counts = [10, 25, 50, 100]
            results_by_count = {}
            
            for num_connections in connection_counts:
                logger.info(f"åŒæ™‚æ¥ç¶šæ•° {num_connections} ã§ãƒ†ã‚¹ãƒˆä¸­...")
                
                start_time = time.time()
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=num_connections) as executor:
                    futures = [executor.submit(simulate_connection, i) for i in range(num_connections)]
                    
                    try:
                        results = [future.result(timeout=30) for future in futures]
                        total_time = time.time() - start_time
                        
                        successful_results = [r for r in results if r["success"]]
                        success_rate = len(successful_results) / len(results)
                        
                        if successful_results:
                            avg_time = sum(r["execution_time"] for r in successful_results) / len(successful_results)
                            max_time = max(r["execution_time"] for r in successful_results)
                            
                            results_by_count[num_connections] = {
                                "success_rate": success_rate,
                                "avg_response_time": avg_time,
                                "max_response_time": max_time,
                                "total_time": total_time,
                                "throughput": len(results) / total_time
                            }
                            
                            logger.info(f"  æˆåŠŸç‡: {success_rate:.1%}")
                            logger.info(f"  å¹³å‡å¿œç­”æ™‚é–“: {avg_time:.3f}ç§’")
                            logger.info(f"  æœ€å¤§å¿œç­”æ™‚é–“: {max_time:.3f}ç§’")
                            logger.info(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {len(results)/total_time:.1f}req/ç§’")
                            
                            # æ€§èƒ½åŠ£åŒ–ã®ç¢ºèª
                            if success_rate < 0.9:
                                logger.warning(f"åŒæ™‚æ¥ç¶šæ•° {num_connections} ã§æˆåŠŸç‡ãŒä½ä¸‹: {success_rate:.1%}")
                                break
                            
                            if avg_time > 5.0:
                                logger.warning(f"åŒæ™‚æ¥ç¶šæ•° {num_connections} ã§å¿œç­”æ™‚é–“ãŒåŠ£åŒ–: {avg_time:.3f}ç§’")
                                break
                        
                    except concurrent.futures.TimeoutError:
                        logger.warning(f"åŒæ™‚æ¥ç¶šæ•° {num_connections} ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿ")
                        break
            
            # çµæœåˆ†æ
            if results_by_count:
                logger.info("\nåŒæ™‚æ¥ç¶šæ•°ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
                for count, metrics in results_by_count.items():
                    logger.info(f"  {count}æ¥ç¶š: æˆåŠŸç‡={metrics['success_rate']:.1%}, å¿œç­”æ™‚é–“={metrics['avg_response_time']:.3f}ç§’")
                
                max_successful_connections = max(
                    count for count, metrics in results_by_count.items() 
                    if metrics['success_rate'] >= 0.9
                )
                logger.info(f"æ¨å¥¨æœ€å¤§åŒæ™‚æ¥ç¶šæ•°: {max_successful_connections}")
            
            logger.info("âœ… åŒæ™‚æ¥ç¶šæ•°ä¸Šé™ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            
        except Exception as e:
            pytest.fail(f"åŒæ™‚æ¥ç¶šæ•°ä¸Šé™ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def test_cpu_memory_optimization(self):
        """ãƒ†ã‚¹ãƒˆ38: CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®æœ€é©åŒ–ç¢ºèª"""
        logger.info("ğŸ” CPU/ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            from app.services.auto_strategy.services.ml_orchestrator import MLOrchestrator
            
            test_data = self.create_large_dataset(2000)
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
            baseline_memory = psutil.Process().memory_info().rss / 1024 / 1024
            baseline_cpu = psutil.cpu_percent(interval=1)
            
            # CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–ã—ãªãŒã‚‰å‡¦ç†å®Ÿè¡Œ
            cpu_samples = []
            memory_samples = []
            
            def monitor_resources():
                """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’ç›£è¦–"""
                for _ in range(20):  # 10ç§’é–“ç›£è¦–
                    cpu_samples.append(psutil.cpu_percent(interval=0.5))
                    memory_samples.append(psutil.Process().memory_info().rss / 1024 / 1024)
            
            # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
            monitor_thread = threading.Thread(target=monitor_resources, daemon=True)
            monitor_thread.start()
            
            # MLå‡¦ç†å®Ÿè¡Œ
            start_time = time.time()
            
            try:
                ml_orchestrator = MLOrchestrator(enable_automl=False)
                ml_indicators = ml_orchestrator.calculate_ml_indicators(test_data)
                
                processing_time = time.time() - start_time
                
                # ç›£è¦–å®Œäº†ã¾ã§å¾…æ©Ÿ
                monitor_thread.join(timeout=15)
                
                # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡åˆ†æ
                if cpu_samples and memory_samples:
                    max_cpu = max(cpu_samples)
                    avg_cpu = sum(cpu_samples) / len(cpu_samples)
                    max_memory = max(memory_samples)
                    avg_memory = sum(memory_samples) / len(memory_samples)
                    memory_peak = max_memory - baseline_memory
                    
                    logger.info(f"CPUä½¿ç”¨ç‡: æœ€å¤§={max_cpu:.1f}%, å¹³å‡={avg_cpu:.1f}%")
                    logger.info(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: æœ€å¤§={max_memory:.1f}MB, å¹³å‡={avg_memory:.1f}MB, ãƒ”ãƒ¼ã‚¯å¢—åŠ ={memory_peak:.1f}MB")
                    logger.info(f"å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
                    
                    # æœ€é©åŒ–ã®ç¢ºèª
                    assert max_cpu < 90, f"CPUä½¿ç”¨ç‡ãŒé«˜ã™ãã¾ã™: {max_cpu:.1f}%"
                    assert memory_peak < 500, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ ãŒå¤§ãã™ãã¾ã™: {memory_peak:.1f}MB"
                    
                    # åŠ¹ç‡æ€§ã®è¨ˆç®—
                    if ml_indicators and "ML_UP_PROB" in ml_indicators:
                        result_count = len(ml_indicators["ML_UP_PROB"])
                        cpu_efficiency = result_count / (avg_cpu * processing_time) if avg_cpu > 0 else 0
                        memory_efficiency = result_count / memory_peak if memory_peak > 0 else 0
                        
                        logger.info(f"CPUåŠ¹ç‡æ€§: {cpu_efficiency:.2f}çµæœ/(CPU%*ç§’)")
                        logger.info(f"ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§: {memory_efficiency:.2f}çµæœ/MB")
                
            except Exception as e:
                logger.info(f"MLå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ï¼ˆæœŸå¾…ã•ã‚Œã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™ï¼‰: {e}")
            
            logger.info("âœ… CPU/ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            
        except Exception as e:
            pytest.fail(f"CPU/ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def test_response_time_consistency(self):
        """ãƒ†ã‚¹ãƒˆ39: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ” ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            from app.services.auto_strategy.calculators.tpsl_calculator import TPSLCalculator
            
            calculator = TPSLCalculator()
            response_times = []
            
            # 100å›ã®è¨ˆç®—ã‚’å®Ÿè¡Œã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’æ¸¬å®š
            num_iterations = 100
            
            for i in range(num_iterations):
                start_time = time.time()
                
                try:
                    # æ¨™æº–çš„ãªè¨ˆç®—
                    current_price = 50000
                    sl_pct = 0.02
                    tp_pct = 0.04
                    direction = 1.0
                    
                    sl_price, tp_price = calculator.calculate_basic_tpsl_prices(
                        current_price, sl_pct, tp_pct, direction
                    )
                    
                    response_time = time.time() - start_time
                    response_times.append(response_time)
                    
                    if (i + 1) % 20 == 0:
                        logger.info(f"é€²æ—: {i+1}/{num_iterations} å®Œäº†")
                    
                except Exception as e:
                    logger.warning(f"åå¾© {i+1} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            
            if response_times:
                # çµ±è¨ˆåˆ†æ
                mean_time = np.mean(response_times)
                std_time = np.std(response_times)
                min_time = min(response_times)
                max_time = max(response_times)
                median_time = np.median(response_times)
                
                # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
                p95_time = np.percentile(response_times, 95)
                p99_time = np.percentile(response_times, 99)
                
                # å¤‰å‹•ä¿‚æ•°ï¼ˆå¹³å‡æ™‚é–“ãŒéå¸¸ã«å°ã•ã„å ´åˆã®å¯¾å¿œï¼‰
                if mean_time > 1e-6:  # 1ãƒã‚¤ã‚¯ãƒ­ç§’ä»¥ä¸Šã®å ´åˆ
                    cv = std_time / mean_time
                else:
                    cv = 0.0  # éå¸¸ã«é«˜é€Ÿãªå‡¦ç†ã®å ´åˆã¯å¤‰å‹•ä¿‚æ•°ã‚’0ã¨ã™ã‚‹

                logger.info(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“çµ±è¨ˆ ({len(response_times)}å›æ¸¬å®š):")
                logger.info(f"  å¹³å‡: {mean_time*1000:.3f}ms")
                logger.info(f"  æ¨™æº–åå·®: {std_time*1000:.3f}ms")
                logger.info(f"  æœ€å°: {min_time*1000:.3f}ms")
                logger.info(f"  æœ€å¤§: {max_time*1000:.3f}ms")
                logger.info(f"  ä¸­å¤®å€¤: {median_time*1000:.3f}ms")
                logger.info(f"  95%ile: {p95_time*1000:.3f}ms")
                logger.info(f"  99%ile: {p99_time*1000:.3f}ms")
                logger.info(f"  å¤‰å‹•ä¿‚æ•°: {cv:.3f}")

                # ä¸€è²«æ€§ã®ç¢ºèªï¼ˆéå¸¸ã«é«˜é€Ÿãªå‡¦ç†ã®å ´åˆã¯ç·©å’Œï¼‰
                if mean_time > 1e-6:
                    assert mean_time < 0.1, f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒé•·ã™ãã¾ã™: {mean_time*1000:.1f}ms"
                    assert p95_time < 0.2, f"95%ileãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒé•·ã™ãã¾ã™: {p95_time*1000:.1f}ms"
                    assert cv < 1.0, f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®ã°ã‚‰ã¤ããŒå¤§ãã™ãã¾ã™: {cv:.3f}"
                else:
                    logger.info("éå¸¸ã«é«˜é€Ÿãªå‡¦ç†ã®ãŸã‚ã€è©³ç´°ãªä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                
                # å¤–ã‚Œå€¤ã®æ¤œå‡º
                outliers = [t for t in response_times if abs(t - mean_time) > 3 * std_time]
                outlier_rate = len(outliers) / len(response_times)
                
                logger.info(f"å¤–ã‚Œå€¤: {len(outliers)}å€‹ ({outlier_rate:.1%})")
                
                if outlier_rate > 0.05:  # 5%ä»¥ä¸Šã®å¤–ã‚Œå€¤ã¯å•é¡Œ
                    logger.warning(f"å¤–ã‚Œå€¤ãŒå¤šã™ãã¾ã™: {outlier_rate:.1%}")
                else:
                    logger.info("ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®ä¸€è²«æ€§ã¯è‰¯å¥½ã§ã™")
            
            logger.info("âœ… ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            
        except Exception as e:
            pytest.fail(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_instance = TestPerformance()
    
    tests = [
        test_instance.test_large_dataset_processing_speed,
        test_instance.test_concurrent_connection_limit,
        test_instance.test_cpu_memory_optimization,
        test_instance.test_response_time_consistency,
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            test_instance.setup_method()
            test()
            test_instance.teardown_method()
            passed += 1
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆå¤±æ•—: {test.__name__}: {e}")
            failed += 1
    
    print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ: æˆåŠŸ {passed}, å¤±æ•— {failed}")
    print(f"æˆåŠŸç‡: {passed / (passed + failed) * 100:.1f}%")
