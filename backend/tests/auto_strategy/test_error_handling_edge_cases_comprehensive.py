"""
ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ

å„ç¨®ã‚¨ãƒ©ãƒ¼æ¡ä»¶ã€å¢ƒç•Œå€¤ã€ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³ã€
ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ãªã©ã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚’å®Ÿæ–½ã—ã¾ã™ã€‚
"""

import logging
import pytest
import numpy as np
import pandas as pd
import uuid
import threading
import time
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, Any, List

from app.services.auto_strategy.services.auto_strategy_service import AutoStrategyService
from app.services.auto_strategy.services.ml_orchestrator import MLOrchestrator
from app.services.auto_strategy.generators.smart_condition_generator import SmartConditionGenerator

logger = logging.getLogger(__name__)


class TestErrorHandlingEdgeCasesComprehensive:
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    @pytest.fixture
    def auto_strategy_service(self):
        """AutoStrategyServiceã®ãƒ†ã‚¹ãƒˆç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"""
        return AutoStrategyService(enable_smart_generation=True)

    @pytest.fixture
    def ml_orchestrator(self):
        """MLOrchestratorã®ãƒ†ã‚¹ãƒˆç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"""
        return MLOrchestrator(enable_automl=True)

    @pytest.fixture
    def smart_generator(self):
        """SmartConditionGeneratorã®ãƒ†ã‚¹ãƒˆç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"""
        return SmartConditionGenerator(enable_smart_generation=True)

    def test_null_and_none_value_handling(self, auto_strategy_service):
        """Nullãƒ»Noneå€¤ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        null_none_cases = [
            (None, None, None, None, None),
            ("", "", {}, {}, None),
            (str(uuid.uuid4()), None, {}, {}, None),
            (str(uuid.uuid4()), "Test", None, {}, None),
            (str(uuid.uuid4()), "Test", {}, None, None),
        ]
        
        for experiment_id, experiment_name, ga_config, backtest_config, background_tasks in null_none_cases:
            try:
                auto_strategy_service.start_strategy_generation(
                    experiment_id=experiment_id,
                    experiment_name=experiment_name,
                    ga_config_dict=ga_config,
                    backtest_config_dict=backtest_config,
                    background_tasks=background_tasks
                )
                pytest.fail(f"Null/Noneå€¤ {(experiment_id, experiment_name)} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã›ã‚“ã§ã—ãŸ")
                
            except Exception as e:
                # é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒè¡Œã‚ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
                assert any(keyword in str(e).lower() for keyword in ['none', 'null', 'invalid', 'missing', 'error'])

    def test_extreme_boundary_values(self, auto_strategy_service):
        """æ¥µç«¯ãªå¢ƒç•Œå€¤ãƒ†ã‚¹ãƒˆ"""
        boundary_cases = [
            # æ¥µç«¯ã«å°ã•ã„å€¤
            {
                "population_size": 1,
                "generations": 1,
                "crossover_rate": 0.0,
                "mutation_rate": 0.0,
                "elite_size": 0,
                "max_indicators": 1
            },
            # æ¥µç«¯ã«å¤§ãã„å€¤
            {
                "population_size": 10000,
                "generations": 1000,
                "crossover_rate": 1.0,
                "mutation_rate": 1.0,
                "elite_size": 5000,
                "max_indicators": 100
            },
            # ç¯„å›²å¤–ã®å€¤
            {
                "population_size": -1,
                "generations": -1,
                "crossover_rate": -0.5,
                "mutation_rate": 1.5,
                "elite_size": -1,
                "max_indicators": -1
            },
        ]
        
        for ga_config in boundary_cases:
            try:
                from app.services.auto_strategy.models.ga_config import GAConfig
                config = GAConfig.from_dict(ga_config)
                is_valid, errors = config.validate()
                
                if not is_valid:
                    logger.info(f"å¢ƒç•Œå€¤ {ga_config} ã§é©åˆ‡ã«ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {errors}")
                else:
                    logger.warning(f"å¢ƒç•Œå€¤ {ga_config} ã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãŒé€šéã—ã¾ã—ãŸ")
                    
            except Exception as e:
                # é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒè¡Œã‚ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
                assert any(keyword in str(e).lower() for keyword in ['invalid', 'range', 'boundary', 'value'])

    def test_malformed_data_structures(self, ml_orchestrator):
        """ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ãƒ†ã‚¹ãƒˆ"""
        malformed_data_cases = [
            # ä¸æ­£ãªDataFrame
            pd.DataFrame({"invalid": []}),  # ç©ºã®åˆ—
            pd.DataFrame({"col1": [1, 2], "col2": [3]}),  # é•·ã•ãŒç•°ãªã‚‹åˆ—
            pd.DataFrame({"timestamp": ["invalid_date"]}),  # ç„¡åŠ¹ãªæ—¥ä»˜
            # ä¸æ­£ãªè¾æ›¸æ§‹é€ 
            {"nested": {"deeply": {"invalid": {"structure": None}}}},
            # å¾ªç’°å‚ç…§ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            {"self_ref": "circular"},
        ]
        
        for malformed_data in malformed_data_cases:
            try:
                if isinstance(malformed_data, pd.DataFrame):
                    result = ml_orchestrator.calculate_ml_indicators(malformed_data)
                    if result is not None:
                        logger.warning(f"ä¸æ­£ãªDataFrame {malformed_data.shape} ã§çµæœãŒè¿”ã•ã‚Œã¾ã—ãŸ")
                else:
                    # è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯åˆ¥ã®å‡¦ç†
                    logger.info(f"ä¸æ­£ãªæ§‹é€  {type(malformed_data)} ã‚’ãƒ†ã‚¹ãƒˆ")
                    
            except Exception as e:
                # é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒè¡Œã‚ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
                assert any(keyword in str(e).lower() for keyword in ['invalid', 'malformed', 'structure', 'data'])

    def test_memory_exhaustion_simulation(self, auto_strategy_service):
        """ãƒ¡ãƒ¢ãƒªæ¯æ¸‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        try:
            # å¤§é‡ã®ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã™ã‚‹è¨­å®š
            large_config = {
                "population_size": 1000,
                "generations": 100,
                "crossover_rate": 0.8,
                "mutation_rate": 0.1,
                "elite_size": 100,
                "max_indicators": 50,
                "allowed_indicators": ["SMA"] * 50  # å¤§é‡ã®æŒ‡æ¨™
            }
            
            backtest_config = {
                "symbol": "BTC/USDT",
                "timeframe": "1m",  # é«˜é »åº¦ãƒ‡ãƒ¼ã‚¿
                "start_date": "2020-01-01",
                "end_date": "2024-12-31",  # é•·æœŸé–“
                "initial_capital": 100000,
                "commission_rate": 0.00055
            }
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss
            
            with patch.object(auto_strategy_service, 'persistence_service'):
                with patch.object(auto_strategy_service, 'experiment_manager'):
                    try:
                        auto_strategy_service.start_strategy_generation(
                            experiment_id=str(uuid.uuid4()),
                            experiment_name="Memory Test",
                            ga_config_dict=large_config,
                            backtest_config_dict=backtest_config,
                            background_tasks=Mock()
                        )
                        
                        memory_after = process.memory_info().rss
                        memory_increase = memory_after - memory_before
                        
                        # ãƒ¡ãƒ¢ãƒªå¢—åŠ ãŒç•°å¸¸ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                        assert memory_increase < 500 * 1024 * 1024, \
                            f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒç•°å¸¸: {memory_increase / 1024 / 1024:.2f}MB"
                            
                    except MemoryError:
                        logger.info("ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«ç™ºç”Ÿã—ã¾ã—ãŸ")
                    except Exception as e:
                        logger.info(f"ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
                        
        except ImportError:
            pytest.skip("psutilãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")

    def test_concurrent_access_race_conditions(self, auto_strategy_service):
        """ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹ãƒ»ç«¶åˆçŠ¶æ…‹ãƒ†ã‚¹ãƒˆ"""
        results = []
        errors = []
        
        def concurrent_operation(thread_id):
            try:
                experiment_id = str(uuid.uuid4())
                
                with patch.object(auto_strategy_service, 'persistence_service'):
                    with patch.object(auto_strategy_service, 'experiment_manager'):
                        auto_strategy_service.start_strategy_generation(
                            experiment_id=experiment_id,
                            experiment_name=f"Concurrent Test {thread_id}",
                            ga_config_dict={
                                "population_size": 10,
                                "generations": 5,
                                "crossover_rate": 0.8,
                                "mutation_rate": 0.1,
                                "elite_size": 2,
                                "max_indicators": 3,
                                "allowed_indicators": ["SMA", "EMA"]
                            },
                            backtest_config_dict={
                                "symbol": "BTC/USDT",
                                "timeframe": "1h",
                                "start_date": "2024-01-01",
                                "end_date": "2024-01-31",
                                "initial_capital": 100000,
                                "commission_rate": 0.00055
                            },
                            background_tasks=Mock()
                        )
                        results.append(thread_id)
                        
            except Exception as e:
                errors.append((thread_id, e))
        
        # å¤šæ•°ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§åŒæ™‚å®Ÿè¡Œ
        threads = []
        for i in range(10):
            thread = threading.Thread(target=concurrent_operation, args=(i,))
            threads.append(thread)
            thread.start()
        
        # å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã®å®Œäº†ã‚’å¾…æ©Ÿ
        for thread in threads:
            thread.join(timeout=5)
        
        # çµæœåˆ†æ
        logger.info(f"ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ: æˆåŠŸ={len(results)}, ã‚¨ãƒ©ãƒ¼={len(errors)}")
        
        # ä¸€éƒ¨ã‚¨ãƒ©ãƒ¼ã¯è¨±å®¹ï¼ˆãƒªã‚½ãƒ¼ã‚¹ç«¶åˆãªã©ï¼‰
        if len(errors) > len(results):
            logger.warning("ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹ã§å¤šæ•°ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

    def test_infinite_loop_prevention(self, smart_generator):
        """ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ãƒ†ã‚¹ãƒˆ"""
        # å¾ªç’°å‚ç…§ã‚’å«ã‚€æŒ‡æ¨™è¨­å®š
        circular_indicators = []
        for i in range(100):  # å¤§é‡ã®æŒ‡æ¨™
            from app.services.auto_strategy.models.gene_indicator import IndicatorGene
            indicator = IndicatorGene(
                type=f"CIRCULAR_{i}",
                enabled=True,
                parameters={"ref": f"CIRCULAR_{(i + 1) % 100}"}  # å¾ªç’°å‚ç…§
            )
            circular_indicators.append(indicator)
        
        start_time = time.time()
        
        try:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ
            result = smart_generator.generate_balanced_conditions(circular_indicators)
            
            execution_time = time.time() - start_time
            
            # å®Ÿè¡Œæ™‚é–“ãŒåˆç†çš„ãªç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆ30ç§’ä»¥ä¸‹ï¼‰
            assert execution_time < 30, f"å®Ÿè¡Œæ™‚é–“ãŒé•·ã™ãã¾ã™: {execution_time:.2f}ç§’"
            
            # çµæœãŒé©åˆ‡ã«ç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
            assert isinstance(result, tuple)
            assert len(result) == 3  # long_conditions, short_conditions, exit_conditions
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.info(f"å¾ªç’°å‚ç…§ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ï¼ˆå®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’ï¼‰: {e}")

    def test_network_timeout_simulation(self, ml_orchestrator):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        def slow_network_call(*args, **kwargs):
            time.sleep(5)  # 5ç§’ã®é…å»¶
            raise TimeoutError("Network timeout")
        
        with patch.object(ml_orchestrator, 'ml_training_service') as mock_service:
            mock_service.generate_signals.side_effect = slow_network_call
            
            start_time = time.time()
            
            try:
                sample_data = pd.DataFrame({
                    'timestamp': pd.date_range('2024-01-01', periods=10, freq='1H'),
                    'open': np.random.randn(10) * 1000 + 50000,
                    'close': np.random.randn(10) * 1000 + 50000,
                    'high': np.random.randn(10) * 1000 + 51000,
                    'low': np.random.randn(10) * 1000 + 49000,
                    'volume': np.random.randn(10) * 100 + 1000,
                })
                
                result = ml_orchestrator.calculate_ml_indicators(sample_data)
                
            except TimeoutError:
                logger.info("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ")
            except Exception as e:
                logger.info(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ: {e}")
            finally:
                execution_time = time.time() - start_time
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ãŒé©åˆ‡ã«è¡Œã‚ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
                assert execution_time < 10, f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ãŒé…ã™ãã¾ã™: {execution_time:.2f}ç§’"

    def test_database_connection_failure(self, auto_strategy_service):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—ãƒ†ã‚¹ãƒˆ"""
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        with patch.object(auto_strategy_service, 'db_session_factory') as mock_factory:
            mock_factory.side_effect = Exception("Database connection failed")
            
            try:
                auto_strategy_service._init_services()
                pytest.fail("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã›ã‚“ã§ã—ãŸ")
                
            except Exception as e:
                # é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒè¡Œã‚ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
                assert any(keyword in str(e).lower() for keyword in ['database', 'connection', 'failed'])

    def test_file_system_errors(self, ml_orchestrator):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿æ›¸ãã‚¨ãƒ©ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        with patch('builtins.open', side_effect=PermissionError("Permission denied")):
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚’å«ã‚€å‡¦ç†ã‚’å®Ÿè¡Œ
                sample_data = pd.DataFrame({
                    'timestamp': pd.date_range('2024-01-01', periods=5, freq='1H'),
                    'close': [50000, 50100, 49900, 50200, 50050]
                })
                
                result = ml_orchestrator.calculate_ml_indicators(sample_data)
                
            except PermissionError:
                logger.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ")
            except Exception as e:
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ: {e}")

    def test_unicode_and_encoding_issues(self, auto_strategy_service):
        """Unicodeãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œãƒ†ã‚¹ãƒˆ"""
        unicode_test_cases = [
            "å®Ÿé¨“å_æ—¥æœ¬èª",  # æ—¥æœ¬èª
            "Ğ¢ĞµÑÑ‚_ĞºĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ†Ğ°",  # ã‚­ãƒªãƒ«æ–‡å­—
            "æµ‹è¯•_ä¸­æ–‡",  # ä¸­å›½èª
            "ğŸš€ğŸ“ˆğŸ’°",  # çµµæ–‡å­—
            "test\x00null",  # Nullæ–‡å­—
            "test\uffff",  # ç„¡åŠ¹ãªUnicode
        ]
        
        for test_name in unicode_test_cases:
            try:
                with patch.object(auto_strategy_service, 'persistence_service'):
                    with patch.object(auto_strategy_service, 'experiment_manager'):
                        auto_strategy_service.start_strategy_generation(
                            experiment_id=str(uuid.uuid4()),
                            experiment_name=test_name,
                            ga_config_dict={
                                "population_size": 5,
                                "generations": 2,
                                "crossover_rate": 0.8,
                                "mutation_rate": 0.1,
                                "elite_size": 1,
                                "max_indicators": 2,
                                "allowed_indicators": ["SMA"]
                            },
                            backtest_config_dict={
                                "symbol": "BTC/USDT",
                                "timeframe": "1h",
                                "start_date": "2024-01-01",
                                "end_date": "2024-01-31",
                                "initial_capital": 100000,
                                "commission_rate": 0.00055
                            },
                            background_tasks=Mock()
                        )
                        
                logger.info(f"Unicodeæ–‡å­—åˆ— '{test_name}' ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ")
                
            except Exception as e:
                logger.info(f"Unicodeæ–‡å­—åˆ— '{test_name}' ã§ã‚¨ãƒ©ãƒ¼: {e}")

    def test_floating_point_precision_issues(self, auto_strategy_service):
        """æµ®å‹•å°æ•°ç‚¹ç²¾åº¦å•é¡Œãƒ†ã‚¹ãƒˆ"""
        precision_test_cases = [
            0.1 + 0.2,  # 0.30000000000000004
            1e-15,  # æ¥µå°å€¤
            1e15,  # æ¥µå¤§å€¤
            float('inf'),  # ç„¡é™å¤§
            float('-inf'),  # è² ã®ç„¡é™å¤§
            float('nan'),  # NaN
        ]
        
        for test_value in precision_test_cases:
            try:
                ga_config = {
                    "population_size": 10,
                    "generations": 5,
                    "crossover_rate": test_value if 0 <= test_value <= 1 else 0.8,
                    "mutation_rate": 0.1,
                    "elite_size": 2,
                    "max_indicators": 3,
                    "allowed_indicators": ["SMA"]
                }
                
                backtest_config = {
                    "symbol": "BTC/USDT",
                    "timeframe": "1h",
                    "start_date": "2024-01-01",
                    "end_date": "2024-01-31",
                    "initial_capital": test_value if test_value > 0 and test_value < 1e10 else 100000,
                    "commission_rate": 0.00055
                }
                
                from app.services.auto_strategy.models.ga_config import GAConfig
                config = GAConfig.from_dict(ga_config)
                is_valid, errors = config.validate()
                
                if not is_valid and any(str(test_value) in error for error in errors):
                    logger.info(f"æµ®å‹•å°æ•°ç‚¹å€¤ {test_value} ã§é©åˆ‡ã«ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ")
                    
            except Exception as e:
                logger.info(f"æµ®å‹•å°æ•°ç‚¹å€¤ {test_value} ã§ã‚¨ãƒ©ãƒ¼: {e}")

    def test_resource_cleanup_on_failure(self, auto_strategy_service):
        """å¤±æ•—æ™‚ã®ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ"""
        # ãƒªã‚½ãƒ¼ã‚¹ãƒªãƒ¼ã‚¯ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®ãƒ†ã‚¹ãƒˆ
        initial_thread_count = threading.active_count()
        
        try:
            # æ„å›³çš„ã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹
            with patch.object(auto_strategy_service, 'experiment_manager') as mock_manager:
                mock_manager.run_experiment.side_effect = Exception("Intentional failure")
                
                auto_strategy_service.start_strategy_generation(
                    experiment_id=str(uuid.uuid4()),
                    experiment_name="Resource Cleanup Test",
                    ga_config_dict={
                        "population_size": 10,
                        "generations": 5,
                        "crossover_rate": 0.8,
                        "mutation_rate": 0.1,
                        "elite_size": 2,
                        "max_indicators": 3,
                        "allowed_indicators": ["SMA"]
                    },
                    backtest_config_dict={
                        "symbol": "BTC/USDT",
                        "timeframe": "1h",
                        "start_date": "2024-01-01",
                        "end_date": "2024-01-31",
                        "initial_capital": 100000,
                        "commission_rate": 0.00055
                    },
                    background_tasks=Mock()
                )
                
        except Exception as e:
            logger.info(f"æ„å›³çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        
        # ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ç¢ºèª
        time.sleep(1)  # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ™‚é–“ã‚’å¾…æ©Ÿ
        final_thread_count = threading.active_count()
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ãŒå¤§å¹…ã«å¢—åŠ ã—ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
        thread_increase = final_thread_count - initial_thread_count
        assert thread_increase < 10, f"ã‚¹ãƒ¬ãƒƒãƒ‰ãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§: {thread_increase} å€‹ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒå¢—åŠ "


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
