"""
MLã‚·ã‚¹ãƒ†ãƒ ã®å•é¡Œç‚¹æ¤œè¨¼ãƒ†ã‚¹ãƒˆ

ç¾åœ¨ã®MLã‚·ã‚¹ãƒ†ãƒ ã§ç‰¹å®šã•ã‚ŒãŸå•é¡Œç‚¹ã‚’å®Ÿéš›ã«æ¤œè¨¼ã—ã€
å•é¡Œã®å½±éŸ¿ã‚’å®šé‡åŒ–ã™ã‚‹ãŸã‚ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã€‚
"""

import pytest
import pandas as pd
import numpy as np
import logging
from unittest.mock import Mock, patch
from datetime import datetime, timedelta
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.ml.feature_engineering.feature_engineering_service import (
    FeatureEngineeringService,
)
from app.utils.data_processing import DataProcessor
from app.utils.label_generation import LabelGenerator, ThresholdMethod
from app.services.ml.config.ml_config import TrainingConfig
from app.services.ml.ml_training_service import MLTrainingService

logger = logging.getLogger(__name__)


class TestMLSystemIssues:
    """MLã‚·ã‚¹ãƒ†ãƒ ã®å•é¡Œç‚¹æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    @pytest.fixture
    def sample_ohlcv_data(self):
        """ãƒ†ã‚¹ãƒˆç”¨ã®OHLCVãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        dates = pd.date_range(start="2023-01-01", periods=1000, freq="1H")
        np.random.seed(42)

        # ç¾å®Ÿçš„ãªä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        base_price = 50000
        price_changes = np.random.normal(0, 0.02, len(dates))  # 2%ã®æ¨™æº–åå·®
        prices = [base_price]

        for change in price_changes[1:]:
            new_price = prices[-1] * (1 + change)
            prices.append(new_price)

        prices = np.array(prices)

        # OHLCV ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        data = {
            "timestamp": dates,
            "Open": prices * np.random.uniform(0.995, 1.005, len(prices)),
            "High": prices * np.random.uniform(1.001, 1.02, len(prices)),
            "Low": prices * np.random.uniform(0.98, 0.999, len(prices)),
            "Close": prices,
            "Volume": np.random.uniform(100, 1000, len(prices)),
        }

        df = pd.DataFrame(data)
        df.set_index("timestamp", inplace=True)
        return df

    @pytest.fixture
    def sample_funding_rate_data(self, sample_ohlcv_data):
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        return pd.DataFrame(
            {
                "timestamp": sample_ohlcv_data.index,
                "funding_rate": np.random.normal(
                    0.0001, 0.0005, len(sample_ohlcv_data)
                ),
            }
        ).set_index("timestamp")

    @pytest.fixture
    def sample_open_interest_data(self, sample_ohlcv_data):
        """ãƒ†ã‚¹ãƒˆç”¨ã®å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        return pd.DataFrame(
            {
                "timestamp": sample_ohlcv_data.index,
                "open_interest": np.random.uniform(
                    1000000, 5000000, len(sample_ohlcv_data)
                ),
            }
        ).set_index("timestamp")

    def test_feature_scaling_disabled_issue(
        self, sample_ohlcv_data, sample_funding_rate_data, sample_open_interest_data
    ):
        """
        å•é¡Œ1: ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã‚‹å•é¡Œã‚’æ¤œè¨¼
        """
        logger.info("=== ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–å•é¡Œã®æ¤œè¨¼ ===")

        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
        feature_service = FeatureEngineeringService()

        # ç‰¹å¾´é‡ã‚’è¨ˆç®—
        features_df = feature_service.calculate_advanced_features(
            ohlcv_data=sample_ohlcv_data,
            funding_rate_data=sample_funding_rate_data,
            open_interest_data=sample_open_interest_data,
        )

        # æ•°å€¤ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ç¢ºèª
        numeric_columns = features_df.select_dtypes(include=[np.number]).columns
        feature_stats = {}

        for col in numeric_columns[:10]:  # æœ€åˆã®10å€‹ã®ç‰¹å¾´é‡ã‚’ãƒã‚§ãƒƒã‚¯
            if col not in ["Open", "High", "Low", "Close", "Volume"]:
                series = features_df[col].dropna()
                if len(series) > 0:
                    feature_stats[col] = {
                        "mean": series.mean(),
                        "std": series.std(),
                        "min": series.min(),
                        "max": series.max(),
                        "range": series.max() - series.min(),
                    }

        logger.info(f"ç‰¹å¾´é‡çµ±è¨ˆæƒ…å ±ï¼ˆæœ€åˆã®10å€‹ï¼‰:")
        for col, stats in feature_stats.items():
            logger.info(
                f"  {col}: å¹³å‡={stats['mean']:.4f}, æ¨™æº–åå·®={stats['std']:.4f}, ç¯„å›²={stats['range']:.4f}"
            )

        # ã‚¹ã‚±ãƒ¼ãƒ«ã®ä¸æ•´åˆã‚’æ¤œè¨¼
        ranges = [stats["range"] for stats in feature_stats.values()]
        if len(ranges) > 1:
            max_range = max(ranges)
            min_range = min(ranges)
            scale_ratio = max_range / min_range if min_range > 0 else float("inf")

            logger.info(f"ã‚¹ã‚±ãƒ¼ãƒ«æ¯”ç‡ï¼ˆæœ€å¤§ç¯„å›²/æœ€å°ç¯„å›²ï¼‰: {scale_ratio:.2f}")

            # ã‚¹ã‚±ãƒ¼ãƒ«æ¯”ç‡ãŒ100ä»¥ä¸Šã®å ´åˆã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå¿…è¦
            assert (
                scale_ratio > 100
            ), f"ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¸æ•´åˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆæ¯”ç‡: {scale_ratio:.2f}ï¼‰"
            logger.warning(
                f"âš ï¸ ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒç„¡åŠ¹ã®ãŸã‚ã€ã‚¹ã‚±ãƒ¼ãƒ«ä¸æ•´åˆãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ï¼ˆæ¯”ç‡: {scale_ratio:.2f}ï¼‰"
            )

        return feature_stats

    def test_zscore_outlier_detection_issue(self, sample_ohlcv_data):
        """
        å•é¡Œ2: Z-scoreãƒ™ãƒ¼ã‚¹ã®å¤–ã‚Œå€¤æ¤œå‡ºãŒé‡‘èãƒ‡ãƒ¼ã‚¿ã«ä¸é©åˆ‡ãªå•é¡Œã‚’æ¤œè¨¼
        """
        logger.info("=== Z-scoreå¤–ã‚Œå€¤æ¤œå‡ºå•é¡Œã®æ¤œè¨¼ ===")

        # é‡‘èãƒ‡ãƒ¼ã‚¿ã«å…¸å‹çš„ãªæ€¥æ¿€ãªä¾¡æ ¼å¤‰å‹•ã‚’è¿½åŠ 
        test_data = sample_ohlcv_data.copy()

        # æ„å›³çš„ã«æ€¥æ¿€ãªä¾¡æ ¼å¤‰å‹•ï¼ˆå¸‚å ´ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ï¼‰ã‚’è¿½åŠ 
        crash_index = len(test_data) // 2
        test_data.iloc[
            crash_index : crash_index + 5, test_data.columns.get_loc("Close")
        ] *= 0.8  # 20%ä¸‹è½

        # ä¾¡æ ¼å¤‰åŒ–ç‡ã‚’è¨ˆç®—
        price_changes = test_data["Close"].pct_change().dropna()

        # Z-scoreãƒ™ãƒ¼ã‚¹ã®å¤–ã‚Œå€¤æ¤œå‡ºã‚’å®Ÿè¡Œ
        preprocessor = DataPreprocessor()

        # å¤–ã‚Œå€¤æ¤œå‡ºå‰ã®ãƒ‡ãƒ¼ã‚¿æ•°
        original_count = len(price_changes)

        # Z-scoreãƒ™ãƒ¼ã‚¹ã®å¤–ã‚Œå€¤é™¤å»ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        mean_change = price_changes.mean()
        std_change = price_changes.std()
        z_scores = np.abs((price_changes - mean_change) / std_change)

        # é–¾å€¤3.0ã§å¤–ã‚Œå€¤ã‚’ç‰¹å®š
        outliers = z_scores > 3.0
        outlier_count = outliers.sum()
        outlier_percentage = (outlier_count / original_count) * 100

        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿æ•°: {original_count}")
        logger.info(f"Z-scoreå¤–ã‚Œå€¤æ•°: {outlier_count}")
        logger.info(f"å¤–ã‚Œå€¤å‰²åˆ: {outlier_percentage:.2f}%")

        # å¤–ã‚Œå€¤ã¨ã—ã¦æ¤œå‡ºã•ã‚ŒãŸä¾¡æ ¼å¤‰åŒ–ç‡ã‚’ç¢ºèª
        outlier_changes = price_changes[outliers]
        if len(outlier_changes) > 0:
            logger.info(f"å¤–ã‚Œå€¤ã¨ã—ã¦æ¤œå‡ºã•ã‚ŒãŸä¾¡æ ¼å¤‰åŒ–ç‡:")
            for i, change in enumerate(outlier_changes.head(5)):
                logger.info(f"  {i+1}: {change:.4f} ({change*100:.2f}%)")

        # é‡‘èãƒ‡ãƒ¼ã‚¿ã§ã¯5%ä»¥ä¸Šã®å¤‰å‹•ã‚‚æ­£å¸¸ãªç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        large_changes = price_changes[np.abs(price_changes) > 0.05]
        large_changes_count = len(large_changes)

        logger.info(f"5%ä»¥ä¸Šã®ä¾¡æ ¼å¤‰å‹•æ•°: {large_changes_count}")

        # Z-scoreãŒé‡è¦ãªå¸‚å ´ã‚·ã‚°ãƒŠãƒ«ã‚’å¤–ã‚Œå€¤ã¨ã—ã¦èª¤æ¤œå‡ºã—ã¦ã„ã‚‹ã“ã¨ã‚’æ¤œè¨¼
        if outlier_count > 0:
            logger.warning(
                f"âš ï¸ Z-scoreå¤–ã‚Œå€¤æ¤œå‡ºã«ã‚ˆã‚Š{outlier_count}å€‹ã®é‡è¦ãªå¸‚å ´ã‚·ã‚°ãƒŠãƒ«ãŒé™¤å»ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
            )

        return {
            "original_count": original_count,
            "outlier_count": outlier_count,
            "outlier_percentage": outlier_percentage,
            "large_changes_count": large_changes_count,
        }

    def test_fixed_threshold_label_generation_issue(self, sample_ohlcv_data):
        """
        å•é¡Œ3: å›ºå®šé–¾å€¤ãƒ©ãƒ™ãƒ«ç”ŸæˆãŒã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã‚’å¼•ãèµ·ã“ã™å•é¡Œã‚’æ¤œè¨¼
        """
        logger.info("=== å›ºå®šé–¾å€¤ãƒ©ãƒ™ãƒ«ç”Ÿæˆå•é¡Œã®æ¤œè¨¼ ===")

        # ç¾åœ¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ç¢ºèª
        config = TrainingConfig()
        fixed_threshold_up = config.THRESHOLD_UP  # 0.02 (2%)
        fixed_threshold_down = config.THRESHOLD_DOWN  # -0.02 (-2%)

        logger.info(
            f"ç¾åœ¨ã®å›ºå®šé–¾å€¤: ä¸Šæ˜‡={fixed_threshold_up}, ä¸‹è½={fixed_threshold_down}"
        )

        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆå™¨ã‚’åˆæœŸåŒ–
        label_generator = LabelGenerator()

        # å›ºå®šé–¾å€¤ã§ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ
        labels_fixed, threshold_info_fixed = label_generator.generate_labels(
            sample_ohlcv_data["Close"],
            method=ThresholdMethod.FIXED,
            threshold_up=fixed_threshold_up,
            threshold_down=fixed_threshold_down,
        )

        # å‹•çš„é–¾å€¤ã§ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆï¼ˆæ¯”è¼ƒç”¨ï¼‰
        labels_dynamic, threshold_info_dynamic = label_generator.generate_labels(
            sample_ohlcv_data["Close"],
            method=ThresholdMethod.STD_DEVIATION,
            std_multiplier=0.5,
        )

        # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã‚’åˆ†æ
        def analyze_label_distribution(labels, method_name):
            label_counts = labels.value_counts().sort_index()
            total = len(labels)

            distribution = {
                "down": label_counts.get(0, 0) / total,
                "range": label_counts.get(1, 0) / total,
                "up": label_counts.get(2, 0) / total,
            }

            logger.info(f"{method_name}ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
            logger.info(
                f"  ä¸‹è½: {distribution['down']:.3f} ({label_counts.get(0, 0)}å€‹)"
            )
            logger.info(
                f"  ãƒ¬ãƒ³ã‚¸: {distribution['range']:.3f} ({label_counts.get(1, 0)}å€‹)"
            )
            logger.info(
                f"  ä¸Šæ˜‡: {distribution['up']:.3f} ({label_counts.get(2, 0)}å€‹)"
            )

            # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã®åº¦åˆã„ã‚’è¨ˆç®—ï¼ˆæœ€å¤§ã‚¯ãƒ©ã‚¹ã¨æœ€å°ã‚¯ãƒ©ã‚¹ã®æ¯”ç‡ï¼‰
            ratios = [distribution["down"], distribution["range"], distribution["up"]]
            max_ratio = max(ratios)
            min_ratio = min([r for r in ratios if r > 0])
            imbalance_ratio = max_ratio / min_ratio if min_ratio > 0 else float("inf")

            logger.info(f"  ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯”ç‡: {imbalance_ratio:.2f}")

            return distribution, imbalance_ratio

        # å›ºå®šé–¾å€¤ã®åˆ†å¸ƒåˆ†æ
        fixed_dist, fixed_imbalance = analyze_label_distribution(
            labels_fixed, "å›ºå®šé–¾å€¤"
        )

        # å‹•çš„é–¾å€¤ã®åˆ†å¸ƒåˆ†æ
        dynamic_dist, dynamic_imbalance = analyze_label_distribution(
            labels_dynamic, "å‹•çš„é–¾å€¤"
        )

        # å•é¡Œã®æ¤œè¨¼
        logger.info(f"å›ºå®šé–¾å€¤ã®ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯”ç‡: {fixed_imbalance:.2f}")
        logger.info(f"å‹•çš„é–¾å€¤ã®ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯”ç‡: {dynamic_imbalance:.2f}")

        if fixed_imbalance > 3.0:  # 3å€ä»¥ä¸Šã®ä¸å‡è¡¡
            logger.warning(
                f"âš ï¸ å›ºå®šé–¾å€¤ã«ã‚ˆã‚Šæ·±åˆ»ãªã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ï¼ˆæ¯”ç‡: {fixed_imbalance:.2f}ï¼‰"
            )

        return {
            "fixed_distribution": fixed_dist,
            "dynamic_distribution": dynamic_dist,
            "fixed_imbalance_ratio": fixed_imbalance,
            "dynamic_imbalance_ratio": dynamic_imbalance,
            "threshold_info_fixed": threshold_info_fixed,
            "threshold_info_dynamic": threshold_info_dynamic,
        }

    def test_time_series_cv_missing_issue(self):
        """
        å•é¡Œ4: æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãŒä¸å‚™ãªå•é¡Œã‚’æ¤œè¨¼
        """
        logger.info("=== æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä¸å‚™å•é¡Œã®æ¤œè¨¼ ===")

        # ç¾åœ¨ã®MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®è¨­å®šã‚’ç¢ºèª
        config = TrainingConfig()
        cv_folds = config.CROSS_VALIDATION_FOLDS

        logger.info(f"ç¾åœ¨ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å‰²æ•°: {cv_folds}")

        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’è€ƒæ…®ã—ãªã„é€šå¸¸ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å•é¡Œã‚’èª¬æ˜
        logger.warning(
            "âš ï¸ ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        )
        logger.warning("   - æœªæ¥ã®æƒ…å ±ãŒéå»ã®äºˆæ¸¬ã«ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§ï¼ˆdata leakageï¼‰")
        logger.warning("   - ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šæ™‚ç³»åˆ—ã®é †åºãŒç ´å£Šã•ã‚Œã‚‹")
        logger.warning("   - å®Ÿéš›ã®å–å¼•ç’°å¢ƒã¨ç•°ãªã‚‹è©•ä¾¡çµæœ")

        # æ¨å¥¨ã•ã‚Œã‚‹æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•
        logger.info("æ¨å¥¨ã•ã‚Œã‚‹æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•:")
        logger.info("  1. Time Series Split: æ™‚ç³»åˆ—é †ã«åˆ†å‰²")
        logger.info("  2. Walk-Forward Analysis: æ®µéšçš„ã«å­¦ç¿’æœŸé–“ã‚’æ‹¡å¼µ")
        logger.info("  3. Purged Cross-Validation: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ãã‚®ãƒ£ãƒƒãƒ—è¨­å®š")

        return {
            "current_cv_folds": cv_folds,
            "has_time_series_cv": False,
            "data_leakage_risk": True,
            "recommended_methods": ["time_series_split", "walk_forward", "purged_cv"],
        }

    def test_overall_system_impact(
        self, sample_ohlcv_data, sample_funding_rate_data, sample_open_interest_data
    ):
        """
        å…¨ä½“çš„ãªã‚·ã‚¹ãƒ†ãƒ ã¸ã®å½±éŸ¿ã‚’æ¤œè¨¼
        """
        logger.info("=== å…¨ä½“çš„ãªã‚·ã‚¹ãƒ†ãƒ å½±éŸ¿ã®æ¤œè¨¼ ===")

        # å„å•é¡Œã®å½±éŸ¿ã‚’çµ±åˆçš„ã«è©•ä¾¡
        feature_stats = self.test_feature_scaling_disabled_issue(
            sample_ohlcv_data, sample_funding_rate_data, sample_open_interest_data
        )

        outlier_stats = self.test_zscore_outlier_detection_issue(sample_ohlcv_data)

        label_stats = self.test_fixed_threshold_label_generation_issue(
            sample_ohlcv_data
        )

        cv_stats = self.test_time_series_cv_missing_issue()

        # ç·åˆçš„ãªå•é¡Œã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        problem_score = 0

        # ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å•é¡Œã®ã‚¹ã‚³ã‚¢
        if len(feature_stats) > 1:
            ranges = [stats["range"] for stats in feature_stats.values()]
            max_range = max(ranges)
            min_range = min([r for r in ranges if r > 0])
            scale_ratio = max_range / min_range if min_range > 0 else 1
            if scale_ratio > 100:
                problem_score += 25  # 25ç‚¹æ¸›ç‚¹

        # å¤–ã‚Œå€¤æ¤œå‡ºå•é¡Œã®ã‚¹ã‚³ã‚¢
        if outlier_stats["outlier_percentage"] > 5:  # 5%ä»¥ä¸ŠãŒå¤–ã‚Œå€¤
            problem_score += 20  # 20ç‚¹æ¸›ç‚¹

        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆå•é¡Œã®ã‚¹ã‚³ã‚¢
        if label_stats["fixed_imbalance_ratio"] > 3:  # 3å€ä»¥ä¸Šã®ä¸å‡è¡¡
            problem_score += 30  # 30ç‚¹æ¸›ç‚¹

        # æ™‚ç³»åˆ—CVå•é¡Œã®ã‚¹ã‚³ã‚¢
        if cv_stats["data_leakage_risk"]:
            problem_score += 25  # 25ç‚¹æ¸›ç‚¹

        logger.info(f"ç·åˆå•é¡Œã‚¹ã‚³ã‚¢: {problem_score}/100")
        logger.info("ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©æ·±åˆ»ãªå•é¡ŒãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™")

        if problem_score >= 70:
            logger.error("ğŸš¨ æ·±åˆ»ãªå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ç·Šæ€¥ã®æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚")
        elif problem_score >= 40:
            logger.warning("âš ï¸ é‡è¦ãªå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ”¹å–„ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        else:
            logger.info("âœ… è»½å¾®ãªå•é¡Œã®ã¿ã§ã™ã€‚")

        return {
            "total_problem_score": problem_score,
            "feature_scaling_issues": feature_stats,
            "outlier_detection_issues": outlier_stats,
            "label_generation_issues": label_stats,
            "cross_validation_issues": cv_stats,
        }

    def generate_sample_ohlcv_data(self):
        """ãƒ†ã‚¹ãƒˆç”¨ã®OHLCVãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆéfixtureç‰ˆï¼‰"""
        dates = pd.date_range(start="2023-01-01", periods=1000, freq="1H")
        np.random.seed(42)

        # ç¾å®Ÿçš„ãªä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        base_price = 50000
        price_changes = np.random.normal(0, 0.02, len(dates))  # 2%ã®æ¨™æº–åå·®
        prices = [base_price]

        for change in price_changes[1:]:
            new_price = prices[-1] * (1 + change)
            prices.append(new_price)

        prices = np.array(prices)

        # OHLCV ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        data = {
            "timestamp": dates,
            "Open": prices * np.random.uniform(0.995, 1.005, len(prices)),
            "High": prices * np.random.uniform(1.001, 1.02, len(prices)),
            "Low": prices * np.random.uniform(0.98, 0.999, len(prices)),
            "Close": prices,
            "Volume": np.random.uniform(100, 1000, len(prices)),
        }

        df = pd.DataFrame(data)
        df.set_index("timestamp", inplace=True)
        return df

    def generate_sample_funding_rate_data(self, ohlcv_data):
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆéfixtureç‰ˆï¼‰"""
        return pd.DataFrame(
            {
                "timestamp": ohlcv_data.index,
                "funding_rate": np.random.normal(0.0001, 0.0005, len(ohlcv_data)),
            }
        ).set_index("timestamp")

    def generate_sample_open_interest_data(self, ohlcv_data):
        """ãƒ†ã‚¹ãƒˆç”¨ã®å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆéfixtureç‰ˆï¼‰"""
        return pd.DataFrame(
            {
                "timestamp": ohlcv_data.index,
                "open_interest": np.random.uniform(1000000, 5000000, len(ohlcv_data)),
            }
        ).set_index("timestamp")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆã‚’ç›´æ¥å®Ÿè¡Œã™ã‚‹å ´åˆ
    import logging

    logging.basicConfig(level=logging.INFO)

    test_instance = TestMLSystemIssues()

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    sample_data = test_instance.generate_sample_ohlcv_data()
    funding_data = test_instance.generate_sample_funding_rate_data(sample_data)
    oi_data = test_instance.generate_sample_open_interest_data(sample_data)

    # å…¨ä½“çš„ãªå½±éŸ¿ã‚’æ¤œè¨¼
    results = test_instance.test_overall_system_impact(
        sample_data, funding_data, oi_data
    )

    print(f"\n=== æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼ ===")
    print(f"ç·åˆå•é¡Œã‚¹ã‚³ã‚¢: {results['total_problem_score']}/100")
