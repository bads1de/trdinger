"""
ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯åˆ†æãƒ†ã‚¹ãƒˆ

æ”¹å–„å‰å¾Œã®ç²¾åº¦å·®ãŒãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ã«ã‚ˆã‚‹ã‚‚ã®ã‹ã‚’æ¤œè¨¼ã—ã€
çœŸã®æ”¹å–„åŠ¹æœã‚’æ¸¬å®šã—ã¾ã™ã€‚
"""

import pandas as pd
import numpy as np
import logging
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score
from sklearn.preprocessing import RobustScaler
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logger = logging.getLogger(__name__)


class DataLeakageAnalysis:
    """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.results = {}

    def create_trading_dataset_with_leakage_test(self, n_samples=1000):
        """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        np.random.seed(42)
        
        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        dates = pd.date_range(start='2023-01-01', periods=n_samples, freq='1h')
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å«ã‚€ï¼‰
        base_price = 50000
        
        # æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
        trend_periods = n_samples // 4
        trends = []
        for i in range(4):
            if i % 2 == 0:
                # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
                trend = np.linspace(0, 0.1, trend_periods)
            else:
                # ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰
                trend = np.linspace(0, -0.1, trend_periods)
            trends.extend(trend)
        
        # æ®‹ã‚Šã®æœŸé–“ã‚’åŸ‹ã‚ã‚‹
        while len(trends) < n_samples:
            trends.append(trends[-1])
        trends = trends[:n_samples]
        
        # ä¾¡æ ¼ç”Ÿæˆ
        price_changes = np.array(trends) / n_samples + np.random.normal(0, 0.01, n_samples)
        prices = base_price * np.cumprod(1 + price_changes)
        
        # OHLCV ãƒ‡ãƒ¼ã‚¿
        data = pd.DataFrame({
            'Open': prices * (1 + np.random.normal(0, 0.001, n_samples)),
            'High': prices * (1 + np.abs(np.random.normal(0, 0.005, n_samples))),
            'Low': prices * (1 - np.abs(np.random.normal(0, 0.005, n_samples))),
            'Close': prices,
            'Volume': np.random.lognormal(10, 1, n_samples),
        }, index=dates)
        
        # æŠ€è¡“æŒ‡æ¨™
        data['SMA_5'] = data['Close'].rolling(5).mean()
        data['SMA_10'] = data['Close'].rolling(10).mean()
        data['SMA_20'] = data['Close'].rolling(20).mean()
        data['RSI'] = self._calculate_rsi(data['Close'])
        data['MACD'] = self._calculate_macd(data['Close'])
        data['Volume_Ratio'] = data['Volume'] / data['Volume'].rolling(10).mean()
        
        # å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€ç‰¹å¾´é‡ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®åŸå› ï¼‰
        data['Future_Price_Hint'] = data['Close'].shift(-12)  # 12æ™‚é–“å¾Œã®ä¾¡æ ¼
        data['Future_Volume_Hint'] = data['Volume'].shift(-6)  # 6æ™‚é–“å¾Œã®å‡ºæ¥é«˜
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆ12æ™‚é–“å¾Œã®ä¾¡æ ¼å¤‰å‹•ï¼‰
        future_returns = data['Close'].pct_change(12).shift(-12)
        
        # 3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆã‚ˆã‚Šæ˜ç¢ºãªé–¾å€¤ï¼‰
        y = pd.Series(1, index=dates)  # Hold
        y[future_returns > 0.015] = 2   # Up (1.5%ä»¥ä¸Šä¸Šæ˜‡)
        y[future_returns < -0.015] = 0  # Down (1.5%ä»¥ä¸Šä¸‹è½)
        
        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿
        valid_mask = data.notna().all(axis=1) & y.notna()
        data = data[valid_mask]
        y = y[valid_mask]
        
        return data, y

    def _calculate_rsi(self, prices, window=14):
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.fillna(50)

    def _calculate_macd(self, prices, fast=12, slow=26):
        """MACDè¨ˆç®—"""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        return macd.fillna(0)

    def test_with_data_leakage(self, X, y):
        """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šã®ãƒ†ã‚¹ãƒˆï¼ˆæ”¹å–„å‰ã®å•é¡Œã®ã‚ã‚‹æ–¹æ³•ï¼‰"""
        logger.info("ğŸ”´ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šã®ãƒ†ã‚¹ãƒˆï¼ˆæ”¹å–„å‰ï¼‰")
        
        # å•é¡Œã®ã‚ã‚‹æ–¹æ³•ï¼šãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆå°†æ¥ã®æƒ…å ±ãŒå­¦ç¿’ã«æ··å…¥ï¼‰
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€ç‰¹å¾´é‡ã‚‚ä½¿ç”¨
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        
        results = {
            'accuracy': accuracy_score(y_test, y_pred),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'method': 'ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šï¼ˆãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼‰'
        }
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã‚’ç¢ºèª
        feature_importance = pd.Series(
            model.feature_importances_,
            index=X.columns
        ).sort_values(ascending=False)
        
        logger.info(f"  ç²¾åº¦: {results['accuracy']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {results['balanced_accuracy']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.4f}")
        logger.info(f"  æœ€é‡è¦ç‰¹å¾´é‡: {feature_importance.head(3).to_dict()}")
        
        return results, feature_importance

    def test_without_data_leakage_basic(self, X, y):
        """ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãªã—ã®ãƒ†ã‚¹ãƒˆï¼ˆåŸºæœ¬çš„ãªæ”¹å–„ï¼‰"""
        logger.info("ğŸŸ¡ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãªã—ï¼ˆåŸºæœ¬æ”¹å–„ï¼‰")
        
        # å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€ç‰¹å¾´é‡ã‚’é™¤å»
        leak_features = ['Future_Price_Hint', 'Future_Volume_Hint']
        X_clean = X.drop(columns=[col for col in leak_features if col in X.columns])
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        split_point = int(len(X_clean) * 0.7)
        X_train = X_clean.iloc[:split_point]
        X_test = X_clean.iloc[split_point:]
        y_train = y.iloc[:split_point]
        y_test = y.iloc[split_point:]
        
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        
        results = {
            'accuracy': accuracy_score(y_test, y_pred),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'method': 'ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãªã—ï¼ˆåŸºæœ¬æ”¹å–„ï¼‰'
        }
        
        logger.info(f"  ç²¾åº¦: {results['accuracy']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {results['balanced_accuracy']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.4f}")
        
        return results

    def test_with_full_improvements(self, X, y):
        """å®Œå…¨æ”¹å–„ç‰ˆã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸŸ¢ å®Œå…¨æ”¹å–„ç‰ˆï¼ˆå…¨æ”¹å–„é©ç”¨ï¼‰")
        
        # å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€ç‰¹å¾´é‡ã‚’é™¤å»
        leak_features = ['Future_Price_Hint', 'Future_Volume_Hint']
        X_clean = X.drop(columns=[col for col in leak_features if col in X.columns])
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        split_point = int(len(X_clean) * 0.7)
        X_train = X_clean.iloc[:split_point]
        X_test = X_clean.iloc[split_point:]
        y_train = y.iloc[:split_point]
        y_test = y.iloc[split_point:]
        
        # RobustScaleré©ç”¨
        scaler = RobustScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            columns=X_test.columns,
            index=X_test.index
        )
        
        # ç‰¹å¾´é‡é¸æŠ
        temp_model = RandomForestClassifier(n_estimators=50, random_state=42)
        temp_model.fit(X_train_scaled, y_train)
        
        feature_importance = pd.Series(
            temp_model.feature_importances_,
            index=X_train_scaled.columns
        ).sort_values(ascending=False)
        
        # ä¸Šä½ç‰¹å¾´é‡ã‚’é¸æŠ
        top_features = feature_importance.head(min(8, len(feature_importance))).index
        X_train_selected = X_train_scaled[top_features]
        X_test_selected = X_test_scaled[top_features]
        
        # æ”¹å–„ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        model = RandomForestClassifier(
            n_estimators=150,
            random_state=42,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=3,
            class_weight='balanced'  # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œ
        )
        model.fit(X_train_selected, y_train)
        
        y_pred = model.predict(X_test_selected)
        
        results = {
            'accuracy': accuracy_score(y_test, y_pred),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred, average='weighted'),
            'method': 'å®Œå…¨æ”¹å–„ç‰ˆ',
            'selected_features': top_features.tolist()
        }
        
        logger.info(f"  ç²¾åº¦: {results['accuracy']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {results['balanced_accuracy']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.4f}")
        logger.info(f"  é¸æŠç‰¹å¾´é‡: {len(top_features)}å€‹")
        
        return results

    def test_time_series_cv(self, X, y):
        """æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”µ æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
        
        # å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€ç‰¹å¾´é‡ã‚’é™¤å»
        leak_features = ['Future_Price_Hint', 'Future_Volume_Hint']
        X_clean = X.drop(columns=[col for col in leak_features if col in X.columns])
        
        # RobustScaleré©ç”¨
        scaler = RobustScaler()
        X_scaled = pd.DataFrame(
            scaler.fit_transform(X_clean),
            columns=X_clean.columns,
            index=X_clean.index
        )
        
        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        tscv = TimeSeriesSplit(n_splits=3)
        model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            class_weight='balanced'
        )
        
        cv_scores = []
        cv_balanced_scores = []
        cv_f1_scores = []
        
        for train_idx, test_idx in tscv.split(X_scaled):
            X_train_cv = X_scaled.iloc[train_idx]
            X_test_cv = X_scaled.iloc[test_idx]
            y_train_cv = y.iloc[train_idx]
            y_test_cv = y.iloc[test_idx]
            
            model.fit(X_train_cv, y_train_cv)
            y_pred_cv = model.predict(X_test_cv)
            
            cv_scores.append(accuracy_score(y_test_cv, y_pred_cv))
            cv_balanced_scores.append(balanced_accuracy_score(y_test_cv, y_pred_cv))
            cv_f1_scores.append(f1_score(y_test_cv, y_pred_cv, average='weighted'))
        
        results = {
            'accuracy': np.mean(cv_scores),
            'accuracy_std': np.std(cv_scores),
            'balanced_accuracy': np.mean(cv_balanced_scores),
            'balanced_accuracy_std': np.std(cv_balanced_scores),
            'f1_score': np.mean(cv_f1_scores),
            'f1_std': np.std(cv_f1_scores),
            'method': 'æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³'
        }
        
        logger.info(f"  ç²¾åº¦: {results['accuracy']:.4f} Â± {results['accuracy_std']:.4f}")
        logger.info(f"  ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {results['balanced_accuracy']:.4f} Â± {results['balanced_accuracy_std']:.4f}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {results['f1_score']:.4f} Â± {results['f1_std']:.4f}")
        
        return results

    def run_comprehensive_analysis(self):
        """åŒ…æ‹¬çš„ãªåˆ†æã‚’å®Ÿè¡Œ"""
        logger.info("=" * 80)
        logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯åˆ†æã¨MLãƒ¢ãƒ‡ãƒ«æ”¹å–„åŠ¹æœã®æ¤œè¨¼")
        logger.info("=" * 80)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        X, y = self.create_trading_dataset_with_leakage_test(n_samples=1000)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(X)}ã‚µãƒ³ãƒ—ãƒ«, {X.shape[1]}ç‰¹å¾´é‡")
        logger.info(f"ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        
        # å„æ‰‹æ³•ã§ãƒ†ã‚¹ãƒˆ
        results = {}
        
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šã®ãƒ†ã‚¹ãƒˆ
        results['leakage'], feature_importance = self.test_with_data_leakage(X, y)
        
        # 2. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãªã—ï¼ˆåŸºæœ¬æ”¹å–„ï¼‰
        results['basic_improvement'] = self.test_without_data_leakage_basic(X, y)
        
        # 3. å®Œå…¨æ”¹å–„ç‰ˆ
        results['full_improvement'] = self.test_with_full_improvements(X, y)
        
        # 4. æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        results['time_series_cv'] = self.test_time_series_cv(X, y)
        
        # åˆ†æçµæœã‚’ã¾ã¨ã‚ã‚‹
        self._analyze_comprehensive_results(results, feature_importance)
        
        return results

    def _analyze_comprehensive_results(self, results, feature_importance):
        """åŒ…æ‹¬çš„ãªçµæœåˆ†æ"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š åŒ…æ‹¬çš„åˆ†æçµæœ")
        logger.info("=" * 80)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®å½±éŸ¿ã‚’åˆ†æ
        leakage_accuracy = results['leakage']['accuracy']
        basic_accuracy = results['basic_improvement']['accuracy']
        full_accuracy = results['full_improvement']['accuracy']
        cv_accuracy = results['time_series_cv']['accuracy']
        
        logger.info("ğŸ¯ ç²¾åº¦æ¯”è¼ƒ:")
        logger.info(f"  ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Š: {leakage_accuracy:.4f}")
        logger.info(f"  åŸºæœ¬æ”¹å–„: {basic_accuracy:.4f}")
        logger.info(f"  å®Œå…¨æ”¹å–„: {full_accuracy:.4f}")
        logger.info(f"  æ™‚ç³»åˆ—CV: {cv_accuracy:.4f}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®å½±éŸ¿åº¦
        leakage_impact = (leakage_accuracy - basic_accuracy) / leakage_accuracy * 100
        logger.info(f"\nğŸš¨ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®å½±éŸ¿: {leakage_impact:.1f}%ã®ç²¾åº¦æ°´å¢—ã—")
        
        # çœŸã®æ”¹å–„åŠ¹æœ
        true_improvement = (full_accuracy - basic_accuracy) / basic_accuracy * 100
        logger.info(f"ğŸ‰ çœŸã®æ”¹å–„åŠ¹æœ: {true_improvement:+.1f}%")
        
        # ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
        logger.info(f"\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦åˆ†æï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Šãƒ¢ãƒ‡ãƒ«ï¼‰:")
        top_features = feature_importance.head(5)
        for feature, importance in top_features.items():
            leak_indicator = "ğŸš¨" if "Future" in feature else "âœ…"
            logger.info(f"  {leak_indicator} {feature}: {importance:.4f}")
        
        # å®‰å®šæ€§è©•ä¾¡
        cv_stability = results['time_series_cv']['accuracy_std'] / results['time_series_cv']['accuracy']
        logger.info(f"\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«å®‰å®šæ€§ (CV): {cv_stability:.4f}")
        
        if cv_stability < 0.1:
            stability_rating = "éå¸¸ã«å®‰å®š"
        elif cv_stability < 0.2:
            stability_rating = "å®‰å®š"
        else:
            stability_rating = "ä¸å®‰å®š"
        
        logger.info(f"  è©•ä¾¡: {stability_rating}")
        
        # ç·åˆè©•ä¾¡
        logger.info(f"\nğŸ† ç·åˆè©•ä¾¡:")
        logger.info(f"  âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢: æˆåŠŸï¼ˆ{leakage_impact:.1f}%ã®æ°´å¢—ã—é™¤å»ï¼‰")
        logger.info(f"  âœ… çœŸã®æ”¹å–„åŠ¹æœ: {true_improvement:+.1f}%")
        logger.info(f"  âœ… ãƒ¢ãƒ‡ãƒ«å®‰å®šæ€§: {stability_rating}")
        
        if true_improvement > 5:
            logger.info(f"  ğŸ‰ æœ‰æ„ãªæ”¹å–„åŠ¹æœã‚’ç¢ºèªï¼")
        elif true_improvement > 0:
            logger.info(f"  âš ï¸ è»½å¾®ãªæ”¹å–„åŠ¹æœ")
        else:
            logger.info(f"  âŒ æ”¹å–„åŠ¹æœãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“")


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s: %(message)s'
    )
    
    # åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ
    analysis = DataLeakageAnalysis()
    results = analysis.run_comprehensive_analysis()
    
    logger.info("\nğŸ‰ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯åˆ†æå®Œäº†")
