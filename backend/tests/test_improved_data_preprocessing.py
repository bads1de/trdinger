"""
æ”¹å–„ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆ

IQRãƒ™ãƒ¼ã‚¹ã®å¤–ã‚Œå€¤æ¤œå‡ºã«ã‚ˆã‚Š
é‡‘èæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é‡è¦ãªã‚·ã‚°ãƒŠãƒ«ãŒä¿æŒã•ã‚Œã‚‹ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
"""

import pytest
import pandas as pd
import numpy as np
import logging
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils.data_preprocessing import DataPreprocessor

logger = logging.getLogger(__name__)


class TestImprovedDataPreprocessing:
    """æ”¹å–„ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def generate_financial_data_with_events(self):
        """é‡‘èã‚¤ãƒ™ãƒ³ãƒˆã‚’å«ã‚€ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        dates = pd.date_range(start='2023-01-01', periods=1000, freq='h')
        np.random.seed(42)
        
        # åŸºæœ¬çš„ãªä¾¡æ ¼å¤‰å‹•
        base_returns = np.random.normal(0, 0.01, len(dates))  # 1%ã®æ¨™æº–åå·®
        
        # é‡è¦ãªé‡‘èã‚¤ãƒ™ãƒ³ãƒˆã‚’è¿½åŠ 
        # 1. å¸‚å ´ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ï¼ˆ-15%ã®æ€¥è½ï¼‰
        crash_index = 200
        base_returns[crash_index] = -0.15
        
        # 2. æ€¥æ¿€ãªå›å¾©ï¼ˆ+12%ã®ä¸Šæ˜‡ï¼‰
        recovery_index = 205
        base_returns[recovery_index] = 0.12
        
        # 3. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¹ãƒ‘ã‚¤ã‚¯ï¼ˆÂ±8%ã®å¤‰å‹•ï¼‰
        spike_indices = [400, 401, 402]
        base_returns[spike_indices] = [0.08, -0.08, 0.08]
        
        # 4. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆÂ±5%ã®å¤‰å‹•ï¼‰
        news_indices = [600, 700, 800]
        base_returns[news_indices] = [0.05, -0.05, 0.05]
        
        # ä¾¡æ ¼ã‚’è¨ˆç®—
        prices = [50000]
        for ret in base_returns[1:]:
            new_price = prices[-1] * (1 + ret)
            prices.append(new_price)
        
        # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        data = {
            'price_change': base_returns,
            'volume': np.random.lognormal(10, 1, len(dates)),  # å¯¾æ•°æ­£è¦åˆ†å¸ƒ
            'spread': np.random.exponential(0.001, len(dates)),  # æŒ‡æ•°åˆ†å¸ƒ
            'volatility': np.abs(base_returns) * np.random.uniform(0.8, 1.2, len(dates)),
            'momentum': pd.Series(base_returns).rolling(5).mean().fillna(0),
        }
        
        df = pd.DataFrame(data, index=dates)
        
        # é‡è¦ãªã‚¤ãƒ™ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
        important_events = {
            'crash': crash_index,
            'recovery': recovery_index,
            'volatility_spikes': spike_indices,
            'news_events': news_indices
        }
        
        return df, important_events

    def test_iqr_vs_zscore_outlier_detection(self):
        """IQRãƒ™ãƒ¼ã‚¹ã¨Z-scoreãƒ™ãƒ¼ã‚¹ã®å¤–ã‚Œå€¤æ¤œå‡ºã‚’æ¯”è¼ƒ"""
        logger.info("=== IQRãƒ™ãƒ¼ã‚¹ vs Z-scoreãƒ™ãƒ¼ã‚¹å¤–ã‚Œå€¤æ¤œå‡ºã®æ¯”è¼ƒ ===")
        
        # é‡‘èã‚¤ãƒ™ãƒ³ãƒˆã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        financial_data, important_events = self.generate_financial_data_with_events()
        
        preprocessor = DataPreprocessor()
        
        # Z-scoreãƒ™ãƒ¼ã‚¹ã®å¤–ã‚Œå€¤æ¤œå‡º
        data_zscore = preprocessor.preprocess_features(
            financial_data.copy(),
            remove_outliers=True,
            outlier_threshold=3.0,
            outlier_method="zscore",
            scale_features=False
        )
        
        # IQRãƒ™ãƒ¼ã‚¹ã®å¤–ã‚Œå€¤æ¤œå‡º
        data_iqr = preprocessor.preprocess_features(
            financial_data.copy(),
            remove_outliers=True,
            outlier_threshold=3.0,
            outlier_method="iqr",
            scale_features=False
        )
        
        # é‡è¦ãªã‚¤ãƒ™ãƒ³ãƒˆã®ä¿æŒç‡ã‚’è¨ˆç®—
        def calculate_event_preservation(original_data, processed_data, events):
            preservation_stats = {}
            
            for event_type, indices in events.items():
                if isinstance(indices, list):
                    preserved_count = 0
                    total_count = len(indices)
                    
                    for idx in indices:
                        if idx < len(processed_data):
                            # price_changeã‚«ãƒ©ãƒ ã§ç¢ºèª
                            if not pd.isna(processed_data.iloc[idx]['price_change']):
                                preserved_count += 1
                    
                    preservation_rate = preserved_count / total_count if total_count > 0 else 0
                    preservation_stats[event_type] = {
                        'preserved': preserved_count,
                        'total': total_count,
                        'rate': preservation_rate
                    }
                else:
                    # å˜ä¸€ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                    idx = indices
                    if idx < len(processed_data):
                        preserved = not pd.isna(processed_data.iloc[idx]['price_change'])
                        preservation_stats[event_type] = {
                            'preserved': 1 if preserved else 0,
                            'total': 1,
                            'rate': 1.0 if preserved else 0.0
                        }
            
            return preservation_stats
        
        # å„æ–¹æ³•ã§ã®é‡è¦ã‚¤ãƒ™ãƒ³ãƒˆä¿æŒç‡ã‚’è¨ˆç®—
        zscore_preservation = calculate_event_preservation(financial_data, data_zscore, important_events)
        iqr_preservation = calculate_event_preservation(financial_data, data_iqr, important_events)
        
        logger.info("é‡è¦ãªé‡‘èã‚¤ãƒ™ãƒ³ãƒˆã®ä¿æŒç‡:")
        logger.info("Z-scoreãƒ™ãƒ¼ã‚¹:")
        total_zscore_preserved = 0
        total_zscore_events = 0
        for event_type, stats in zscore_preservation.items():
            logger.info(f"  {event_type}: {stats['preserved']}/{stats['total']} ({stats['rate']*100:.1f}%)")
            total_zscore_preserved += stats['preserved']
            total_zscore_events += stats['total']
        
        logger.info("IQRãƒ™ãƒ¼ã‚¹:")
        total_iqr_preserved = 0
        total_iqr_events = 0
        for event_type, stats in iqr_preservation.items():
            logger.info(f"  {event_type}: {stats['preserved']}/{stats['total']} ({stats['rate']*100:.1f}%)")
            total_iqr_preserved += stats['preserved']
            total_iqr_events += stats['total']
        
        # å…¨ä½“çš„ãªä¿æŒç‡
        zscore_overall_rate = total_zscore_preserved / total_zscore_events if total_zscore_events > 0 else 0
        iqr_overall_rate = total_iqr_preserved / total_iqr_events if total_iqr_events > 0 else 0
        
        logger.info(f"å…¨ä½“çš„ãªé‡è¦ã‚¤ãƒ™ãƒ³ãƒˆä¿æŒç‡:")
        logger.info(f"  Z-score: {zscore_overall_rate*100:.1f}%")
        logger.info(f"  IQR: {iqr_overall_rate*100:.1f}%")
        
        # é€šå¸¸ãƒ‡ãƒ¼ã‚¿ã®é™¤å»ç‡ã‚‚ç¢ºèª
        original_count = len(financial_data)
        zscore_remaining = data_zscore.dropna().shape[0]
        iqr_remaining = data_iqr.dropna().shape[0]
        
        zscore_removal_rate = (original_count - zscore_remaining) / original_count
        iqr_removal_rate = (original_count - iqr_remaining) / original_count
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿é™¤å»ç‡:")
        logger.info(f"  Z-score: {zscore_removal_rate*100:.1f}%")
        logger.info(f"  IQR: {iqr_removal_rate*100:.1f}%")
        
        # IQRã®æ–¹ãŒé‡è¦ã‚¤ãƒ™ãƒ³ãƒˆã‚’ä¿æŒã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
        if iqr_overall_rate > zscore_overall_rate:
            logger.info("âœ… IQRãƒ™ãƒ¼ã‚¹ã®å¤–ã‚Œå€¤æ¤œå‡ºã«ã‚ˆã‚Šã€é‡è¦ãªé‡‘èã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ˆã‚Šå¤šãä¿æŒã•ã‚Œã¾ã—ãŸ")
        else:
            logger.warning("âš ï¸ IQRãƒ™ãƒ¼ã‚¹ã®æ”¹å–„åŠ¹æœãŒæœŸå¾…ã‚ˆã‚Šä½ã„ã§ã™")
        
        return {
            'zscore_preservation': zscore_preservation,
            'iqr_preservation': iqr_preservation,
            'zscore_overall_rate': zscore_overall_rate,
            'iqr_overall_rate': iqr_overall_rate,
            'zscore_removal_rate': zscore_removal_rate,
            'iqr_removal_rate': iqr_removal_rate
        }

    def test_financial_data_characteristics(self):
        """é‡‘èãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã«é©ã—ãŸå‰å‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== é‡‘èãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«é©ã—ãŸå‰å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ ===")
        
        # ç•°ãªã‚‹åˆ†å¸ƒã‚’æŒã¤é‡‘èãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        np.random.seed(42)
        n_samples = 1000
        
        financial_features = pd.DataFrame({
            # æ­£è¦åˆ†å¸ƒã«è¿‘ã„ç‰¹å¾´é‡ï¼ˆä¾¡æ ¼å¤‰åŒ–ç‡ï¼‰
            'returns': np.random.normal(0, 0.02, n_samples),
            
            # å¯¾æ•°æ­£è¦åˆ†å¸ƒï¼ˆå‡ºæ¥é«˜ï¼‰
            'volume': np.random.lognormal(10, 1, n_samples),
            
            # æŒ‡æ•°åˆ†å¸ƒï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼‰
            'spread': np.random.exponential(0.001, n_samples),
            
            # é‡ã„å°¾ã‚’æŒã¤åˆ†å¸ƒï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
            'volatility': np.random.standard_t(3, n_samples) * 0.01,
            
            # æ­ªã‚“ã åˆ†å¸ƒï¼ˆå»ºç‰æ®‹é«˜å¤‰åŒ–ï¼‰
            'oi_change': np.random.gamma(2, 0.01, n_samples) - 0.02,
        })
        
        preprocessor = DataPreprocessor()
        
        # ç•°ãªã‚‹å¤–ã‚Œå€¤æ¤œå‡ºæ–¹æ³•ã§ãƒ†ã‚¹ãƒˆ
        methods = ['iqr', 'zscore']
        results = {}
        
        for method in methods:
            processed_data = preprocessor.preprocess_features(
                financial_features.copy(),
                remove_outliers=True,
                outlier_threshold=3.0,
                outlier_method=method,
                scale_features=False
            )
            
            # å„ç‰¹å¾´é‡ã®ä¿æŒç‡ã‚’è¨ˆç®—
            preservation_rates = {}
            for col in financial_features.columns:
                original_count = financial_features[col].notna().sum()
                preserved_count = processed_data[col].notna().sum()
                preservation_rate = preserved_count / original_count if original_count > 0 else 0
                preservation_rates[col] = preservation_rate
            
            results[method] = {
                'data': processed_data,
                'preservation_rates': preservation_rates,
                'overall_preservation': np.mean(list(preservation_rates.values()))
            }
            
            logger.info(f"{method.upper()}æ³•ã®ç‰¹å¾´é‡ä¿æŒç‡:")
            for col, rate in preservation_rates.items():
                logger.info(f"  {col}: {rate*100:.1f}%")
            logger.info(f"  å…¨ä½“å¹³å‡: {results[method]['overall_preservation']*100:.1f}%")
        
        # IQRã®æ–¹ãŒé‡‘èãƒ‡ãƒ¼ã‚¿ã«é©ã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
        iqr_preservation = results['iqr']['overall_preservation']
        zscore_preservation = results['zscore']['overall_preservation']
        
        if iqr_preservation > zscore_preservation:
            logger.info("âœ… IQRãƒ™ãƒ¼ã‚¹ã®æ–¹ãŒé‡‘èãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã«é©ã—ã¦ã„ã¾ã™")
        else:
            logger.warning("âš ï¸ æœŸå¾…ã•ã‚ŒãŸæ”¹å–„åŠ¹æœãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“")
        
        return results

    def test_data_quality_monitoring(self):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ ===")
        
        # å“è³ªå•é¡Œã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        np.random.seed(42)
        problematic_data = pd.DataFrame({
            'normal_feature': np.random.normal(0, 1, 100),
            'missing_heavy': np.concatenate([np.random.normal(0, 1, 50), [np.nan] * 50]),
            'outlier_heavy': np.concatenate([np.random.normal(0, 1, 90), [100, -100] * 5]),
            'zero_variance': [1.0] * 100,
            'infinite_values': np.concatenate([np.random.normal(0, 1, 95), [np.inf, -np.inf, np.nan, np.inf, -np.inf]])
        })
        
        preprocessor = DataPreprocessor()
        
        # å‰å‡¦ç†å‰ã®å“è³ªãƒã‚§ãƒƒã‚¯
        def analyze_data_quality(df, label):
            logger.info(f"{label}ã®ãƒ‡ãƒ¼ã‚¿å“è³ª:")
            
            for col in df.columns:
                series = df[col]
                total_count = len(series)
                
                missing_count = series.isna().sum()
                infinite_count = np.isinf(series).sum()
                zero_var = series.var() == 0 if series.notna().sum() > 1 else True
                
                logger.info(f"  {col}:")
                logger.info(f"    æ¬ æå€¤: {missing_count}/{total_count} ({missing_count/total_count*100:.1f}%)")
                logger.info(f"    ç„¡é™å€¤: {infinite_count}")
                logger.info(f"    åˆ†æ•£ã‚¼ãƒ­: {zero_var}")
                
                if series.notna().sum() > 0:
                    logger.info(f"    ç¯„å›²: {series.min():.3f} - {series.max():.3f}")
        
        analyze_data_quality(problematic_data, "å‰å‡¦ç†å‰")
        
        # IQRãƒ™ãƒ¼ã‚¹ã§å‰å‡¦ç†
        processed_data = preprocessor.preprocess_features(
            problematic_data.copy(),
            remove_outliers=True,
            outlier_threshold=3.0,
            outlier_method="iqr",
            scale_features=True,
            scaling_method="robust"
        )
        
        analyze_data_quality(processed_data, "å‰å‡¦ç†å¾Œ")
        
        # å“è³ªæ”¹å–„ã®è©•ä¾¡
        quality_improvements = {}
        
        for col in problematic_data.columns:
            if col in processed_data.columns:
                # æ¬ æå€¤ã®æ”¹å–„
                original_missing = problematic_data[col].isna().sum()
                processed_missing = processed_data[col].isna().sum()
                
                # ç„¡é™å€¤ã®æ”¹å–„
                original_infinite = np.isinf(problematic_data[col]).sum()
                processed_infinite = np.isinf(processed_data[col]).sum()
                
                quality_improvements[col] = {
                    'missing_improved': original_missing > processed_missing,
                    'infinite_improved': original_infinite > processed_infinite,
                    'has_finite_values': processed_data[col].notna().sum() > 0
                }
        
        # å…¨ä½“çš„ãªå“è³ªæ”¹å–„ã‚’è©•ä¾¡
        total_improvements = sum([
            sum([
                improvements['missing_improved'],
                improvements['infinite_improved'],
                improvements['has_finite_values']
            ]) for improvements in quality_improvements.values()
        ])
        
        max_possible_improvements = len(quality_improvements) * 3
        improvement_rate = total_improvements / max_possible_improvements if max_possible_improvements > 0 else 0
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ç‡: {improvement_rate*100:.1f}%")
        
        if improvement_rate > 0.7:
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã¾ã—ãŸ")
        elif improvement_rate > 0.5:
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªãŒæ”¹å–„ã•ã‚Œã¾ã—ãŸ")
        else:
            logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿å“è³ªã®æ”¹å–„ãŒé™å®šçš„ã§ã™")
        
        return {
            'original_data': problematic_data,
            'processed_data': processed_data,
            'quality_improvements': quality_improvements,
            'improvement_rate': improvement_rate
        }

    def test_overall_preprocessing_improvement(self):
        """å…¨ä½“çš„ãªå‰å‡¦ç†æ”¹å–„ã‚’ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== å…¨ä½“çš„ãªå‰å‡¦ç†æ”¹å–„ã®æ¤œè¨¼ ===")
        
        # å„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        outlier_results = self.test_iqr_vs_zscore_outlier_detection()
        financial_results = self.test_financial_data_characteristics()
        quality_results = self.test_data_quality_monitoring()
        
        # æ”¹å–„ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        improvement_score = 0
        
        # é‡è¦ã‚¤ãƒ™ãƒ³ãƒˆä¿æŒï¼ˆæœ€å¤§40ç‚¹ï¼‰
        iqr_event_preservation = outlier_results['iqr_overall_rate']
        zscore_event_preservation = outlier_results['zscore_overall_rate']
        
        if iqr_event_preservation > zscore_event_preservation:
            improvement_score += 40
        elif iqr_event_preservation >= zscore_event_preservation * 0.9:
            improvement_score += 30
        elif iqr_event_preservation >= zscore_event_preservation * 0.8:
            improvement_score += 20
        
        # é‡‘èãƒ‡ãƒ¼ã‚¿é©å¿œæ€§ï¼ˆæœ€å¤§30ç‚¹ï¼‰
        iqr_financial_preservation = financial_results['iqr']['overall_preservation']
        zscore_financial_preservation = financial_results['zscore']['overall_preservation']
        
        if iqr_financial_preservation > zscore_financial_preservation:
            improvement_score += 30
        elif iqr_financial_preservation >= zscore_financial_preservation * 0.95:
            improvement_score += 20
        elif iqr_financial_preservation >= zscore_financial_preservation * 0.9:
            improvement_score += 10
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ï¼ˆæœ€å¤§30ç‚¹ï¼‰
        quality_improvement_rate = quality_results['improvement_rate']
        if quality_improvement_rate > 0.8:
            improvement_score += 30
        elif quality_improvement_rate > 0.6:
            improvement_score += 20
        elif quality_improvement_rate > 0.4:
            improvement_score += 10
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†æ”¹å–„ã‚¹ã‚³ã‚¢: {improvement_score}/100")
        
        if improvement_score >= 80:
            logger.info("ğŸ‰ å„ªç§€ãªæ”¹å–„åŠ¹æœãŒç¢ºèªã•ã‚Œã¾ã—ãŸ")
        elif improvement_score >= 60:
            logger.info("âœ… è‰¯å¥½ãªæ”¹å–„åŠ¹æœãŒç¢ºèªã•ã‚Œã¾ã—ãŸ")
        elif improvement_score >= 40:
            logger.info("âš ï¸ éƒ¨åˆ†çš„ãªæ”¹å–„åŠ¹æœãŒç¢ºèªã•ã‚Œã¾ã—ãŸ")
        else:
            logger.warning("âŒ æ”¹å–„åŠ¹æœãŒä¸ååˆ†ã§ã™")
        
        return {
            'improvement_score': improvement_score,
            'outlier_results': outlier_results,
            'financial_results': financial_results,
            'quality_results': quality_results
        }


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆã‚’ç›´æ¥å®Ÿè¡Œã™ã‚‹å ´åˆ
    import logging
    logging.basicConfig(level=logging.INFO)
    
    test_instance = TestImprovedDataPreprocessing()
    
    # å…¨ä½“çš„ãªæ”¹å–„åŠ¹æœã‚’æ¤œè¨¼
    results = test_instance.test_overall_preprocessing_improvement()
    
    print(f"\n=== ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†æ”¹å–„çµæœã‚µãƒãƒªãƒ¼ ===")
    print(f"æ”¹å–„ã‚¹ã‚³ã‚¢: {results['improvement_score']}/100")
    print(f"é‡è¦ã‚¤ãƒ™ãƒ³ãƒˆä¿æŒç‡ï¼ˆIQRï¼‰: {results['outlier_results']['iqr_overall_rate']*100:.1f}%")
    print(f"é‡è¦ã‚¤ãƒ™ãƒ³ãƒˆä¿æŒç‡ï¼ˆZ-scoreï¼‰: {results['outlier_results']['zscore_overall_rate']*100:.1f}%")
    print(f"ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ç‡: {results['quality_results']['improvement_rate']*100:.1f}%")
