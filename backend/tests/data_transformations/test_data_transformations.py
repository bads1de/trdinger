"""
ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†ã®æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ

ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã€ãƒ‡ãƒ¼ã‚¿çµåˆã®æ­£ç¢ºæ€§ã‚’æ¤œè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã€‚
ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†ã®ä¿¡é ¼æ€§ã¨ä¸€è²«æ€§ã‚’åŒ…æ‹¬çš„ã«æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import pytest
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Any
from datetime import datetime, timedelta
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils.data_processing import DataProcessor

logger = logging.getLogger(__name__)


class TestDataTransformations:
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†ã®æ•´åˆæ€§ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def sample_ohlcv_data(self) -> pd.DataFrame:
        """ãƒ†ã‚¹ãƒˆç”¨ã®OHLCVãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=100, freq='1H')

        base_price = 50000
        returns = np.random.normal(0, 0.02, 100)
        prices = [base_price]

        for ret in returns[1:]:
            prices.append(prices[-1] * (1 + ret))

        return pd.DataFrame({
            'timestamp': dates,
            'Open': prices,
            'High': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
            'Low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
            'Close': [p * (1 + np.random.normal(0, 0.005)) for p in prices],
            'Volume': np.random.lognormal(10, 1, 100)
        })

    def mixed_type_data(self) -> pd.DataFrame:
        """æ··åˆãƒ‡ãƒ¼ã‚¿å‹ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿"""
        return pd.DataFrame({
            'int_col': [1, 2, 3, 4, 5],
            'float_col': [1.1, 2.2, 3.3, 4.4, 5.5],
            'str_col': ['a', 'b', 'c', 'd', 'e'],
            'bool_col': [True, False, True, False, True],
            'datetime_col': pd.date_range('2023-01-01', periods=5, freq='D')
        })

    @pytest.fixture
    def funding_rate_data(self) -> pd.DataFrame:
        """ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿"""
        dates = pd.date_range('2023-01-01', periods=50, freq='8H')
        return pd.DataFrame({
            'timestamp': dates,
            'funding_rate': np.random.normal(0.0001, 0.0005, 50),
            'symbol': ['BTC'] * 50
        })

    @pytest.fixture
    def open_interest_data(self) -> pd.DataFrame:
        """å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿"""
        dates = pd.date_range('2023-01-01', periods=50, freq='1H')
        return pd.DataFrame({
            'timestamp': dates,
            'open_interest': np.random.lognormal(15, 0.5, 50),
            'symbol': ['BTC'] * 50
        })

    def test_data_type_optimization(self, mixed_type_data):
        """ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = mixed_type_data.copy()
        
        # å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’è¨˜éŒ²
        original_dtypes = data.dtypes.to_dict()
        
        # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã‚’å®Ÿè¡Œ
        optimized_data = processor.optimize_dtypes(data)
        
        # æœ€é©åŒ–å¾Œã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèª
        optimized_dtypes = optimized_data.dtypes.to_dict()
        
        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å€¤ãŒå¤‰ã‚ã£ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
        try:
            pd.testing.assert_series_equal(
                data['int_col'].astype(optimized_dtypes['int_col']),
                optimized_data['int_col'],
                check_names=False
            )
        except AssertionError:
            raise AssertionError("æ•´æ•°ã‚«ãƒ©ãƒ ã®å€¤ãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸ")

        try:
            pd.testing.assert_series_equal(
                data['float_col'].astype(optimized_dtypes['float_col']),
                optimized_data['float_col'],
                check_names=False
            )
        except AssertionError:
            raise AssertionError("æµ®å‹•å°æ•°ç‚¹ã‚«ãƒ©ãƒ ã®å€¤ãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸ")

        # æ–‡å­—åˆ—ã¨ãƒ–ãƒ¼ãƒ«å€¤ã¯å¤‰æ›´ã•ã‚Œãªã„ã“ã¨ã‚’ç¢ºèª
        try:
            pd.testing.assert_series_equal(
                data['str_col'],
                optimized_data['str_col']
            )
        except AssertionError:
            raise AssertionError("æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸ")

        try:
            pd.testing.assert_series_equal(
                data['bool_col'],
                optimized_data['bool_col']
            )
        except AssertionError:
            raise AssertionError("ãƒ–ãƒ¼ãƒ«ã‚«ãƒ©ãƒ ãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸ")
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_time_series_index_handling(self, sample_ohlcv_data):
        """æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = sample_ohlcv_data.copy()
        
        # timestampã‚«ãƒ©ãƒ ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
        data_with_index = data.set_index('timestamp')
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿè¡Œ
        processed_data = processor.clean_and_validate_data(
            data_with_index,
            required_columns=['Open', 'High', 'Low', 'Close', 'Volume']
        )
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ™‚ç³»åˆ—é †ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert processed_data.index.is_monotonic_increasing, "æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ˜‡é †ã§ã‚ã‚Šã¾ã›ã‚“"
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒDatetimeIndexã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert isinstance(processed_data.index, pd.DatetimeIndex), "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒDatetimeIndexã§ã¯ã‚ã‚Šã¾ã›ã‚“"
        
        # ãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°ãŒä¿æŒã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert len(processed_data) == len(data_with_index), "ãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°ãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸ"
        
        logger.info("âœ… æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_data_interpolation_accuracy(self, sample_ohlcv_data):
        """ãƒ‡ãƒ¼ã‚¿è£œé–“ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿è£œé–“ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = sample_ohlcv_data.copy()
        
        # æ„å›³çš„ã«NaNå€¤ã‚’æŒ¿å…¥
        data.loc[10:15, 'Close'] = np.nan
        data.loc[30:32, 'Volume'] = np.nan
        
        # è£œé–“å‰ã®NaNæ•°ã‚’è¨˜éŒ²
        nan_count_before = data.isna().sum().sum()
        
        # ãƒ‡ãƒ¼ã‚¿è£œé–“ã‚’å®Ÿè¡Œ
        interpolated_data = processor.interpolate_all_data(data)
        
        # è£œé–“å¾Œã®NaNæ•°ã‚’ç¢ºèª
        nan_count_after = interpolated_data.isna().sum().sum()
        
        # NaNæ•°ãŒæ¸›å°‘ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert nan_count_after < nan_count_before, "ãƒ‡ãƒ¼ã‚¿è£œé–“ã§NaNæ•°ãŒæ¸›å°‘ã—ã¦ã„ã¾ã›ã‚“"
        
        # è£œé–“ã•ã‚ŒãŸå€¤ãŒåˆç†çš„ãªç¯„å›²å†…ã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        original_close_mean = data['Close'].mean()
        interpolated_close_mean = interpolated_data['Close'].mean()
        
        # å¹³å‡å€¤ã®å¤‰åŒ–ãŒ10%ä»¥å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        mean_change_ratio = abs(interpolated_close_mean - original_close_mean) / original_close_mean
        assert mean_change_ratio < 0.1, f"è£œé–“å¾Œã®å¹³å‡å€¤å¤‰åŒ–ãŒå¤§ãã™ãã¾ã™: {mean_change_ratio:.4f}"
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿è£œé–“ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_data_merging_accuracy(self, sample_ohlcv_data, funding_rate_data, open_interest_data):
        """ãƒ‡ãƒ¼ã‚¿çµåˆã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿çµåˆã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        
        # OHLCVãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æº–å‚™
        ohlcv = sample_ohlcv_data.set_index('timestamp')
        funding = funding_rate_data.set_index('timestamp')
        oi = open_interest_data.set_index('timestamp')
        
        # ãƒ‡ãƒ¼ã‚¿çµåˆã‚’å®Ÿè¡Œï¼ˆæ™‚é–“è»¸ã§ã®çµåˆï¼‰
        merged_data = pd.merge(ohlcv, funding, left_index=True, right_index=True, how='left')
        merged_data = pd.merge(merged_data, oi, left_index=True, right_index=True, how='left')
        
        # çµåˆçµæœã®æ¤œè¨¼
        assert len(merged_data) == len(ohlcv), "çµåˆå¾Œã®è¡Œæ•°ãŒãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ç•°ãªã‚Šã¾ã™"
        
        # å…ƒã®OHLCVãƒ‡ãƒ¼ã‚¿ãŒä¿æŒã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        pd.testing.assert_series_equal(
            ohlcv['Close'],
            merged_data['Close'],
            msg="çµåˆå¾Œã«OHLCVãƒ‡ãƒ¼ã‚¿ãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸ"
        )
        
        # ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¨å»ºç‰æ®‹é«˜ã®ã‚«ãƒ©ãƒ ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert 'funding_rate' in merged_data.columns, "ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        assert 'open_interest' in merged_data.columns, "å»ºç‰æ®‹é«˜ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿çµåˆã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_data_validation_consistency(self, sample_ohlcv_data):
        """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = sample_ohlcv_data.set_index('timestamp')
        
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        
        # åŒã˜ãƒ‡ãƒ¼ã‚¿ã§è¤‡æ•°å›æ¤œè¨¼ã‚’å®Ÿè¡Œ
        validated_data1 = processor.clean_and_validate_data(data.copy(), required_columns)
        validated_data2 = processor.clean_and_validate_data(data.copy(), required_columns)
        
        # çµæœãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        pd.testing.assert_frame_equal(
            validated_data1, validated_data2,
            check_exact=False,
            rtol=1e-10,
            msg="åŒã˜ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼çµæœãŒä¸€è‡´ã—ã¾ã›ã‚“"
        )
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_data_sorting_stability(self, sample_ohlcv_data):
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ãƒˆã®å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ãƒˆã®å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = sample_ohlcv_data.copy()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸¦ã³æ›¿ãˆ
        shuffled_data = data.sample(frac=1, random_state=42).reset_index(drop=True)
        shuffled_data = shuffled_data.set_index('timestamp')
        
        # ã‚½ãƒ¼ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        sorted_data = processor.clean_and_validate_data(
            shuffled_data,
            required_columns=['Open', 'High', 'Low', 'Close', 'Volume']
        )
        
        # ã‚½ãƒ¼ãƒˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒæ™‚ç³»åˆ—é †ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert sorted_data.index.is_monotonic_increasing, "ã‚½ãƒ¼ãƒˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒæ™‚ç³»åˆ—é †ã§ã‚ã‚Šã¾ã›ã‚“"
        
        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜å€¤ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆé †åºã¯ç•°ãªã‚‹å¯èƒ½æ€§ï¼‰
        original_sorted = data.set_index('timestamp').sort_index()
        
        pd.testing.assert_frame_equal(
            original_sorted,
            sorted_data,
            check_exact=False,
            rtol=1e-10,
            msg="ã‚½ãƒ¼ãƒˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒå…ƒã®ãƒ‡ãƒ¼ã‚¿ã¨ä¸€è‡´ã—ã¾ã›ã‚“"
        )
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ãƒˆã®å®‰å®šæ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_memory_efficiency(self, sample_ohlcv_data):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        data = sample_ohlcv_data.copy()
        
        # å‡¦ç†å‰ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        memory_before = data.memory_usage(deep=True).sum()
        
        # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã‚’å®Ÿè¡Œ
        optimized_data = processor.optimize_dtypes(data)
        
        # å‡¦ç†å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        memory_after = optimized_data.memory_usage(deep=True).sum()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå‰Šæ¸›ã•ã‚Œã¦ã„ã‚‹ã‹ã€å°‘ãªãã¨ã‚‚å¢—åŠ ã—ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
        assert memory_after <= memory_before, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¾ã—ãŸ: {memory_before} -> {memory_after}"
        
        # ãƒ‡ãƒ¼ã‚¿ã®å€¤ãŒä¿æŒã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            np.testing.assert_array_almost_equal(
                data[col].values,
                optimized_data[col].values,
                decimal=10,
                err_msg=f"ã‚«ãƒ©ãƒ  {col} ã®å€¤ãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸ"
            )
        
        logger.info(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_before} -> {memory_after} bytes")
        logger.info("âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_edge_case_handling(self):
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®å‡¦ç†ãƒ†ã‚¹ãƒˆ ===")
        
        processor = DataProcessor()
        
        # ç©ºã®DataFrameã®ãƒ†ã‚¹ãƒˆ
        empty_df = pd.DataFrame()
        processed_empty = processor.optimize_dtypes(empty_df)
        assert len(processed_empty) == 0, "ç©ºã®DataFrameã®å‡¦ç†ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
        
        # å˜ä¸€è¡Œã®DataFrameã®ãƒ†ã‚¹ãƒˆ
        single_row_df = pd.DataFrame({'A': [1], 'B': [2.0], 'C': ['test']})
        processed_single = processor.optimize_dtypes(single_row_df)
        assert len(processed_single) == 1, "å˜ä¸€è¡ŒDataFrameã®å‡¦ç†ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
        
        # å…¨ã¦NaNã®ã‚«ãƒ©ãƒ ã‚’å«ã‚€DataFrameã®ãƒ†ã‚¹ãƒˆ
        all_nan_df = pd.DataFrame({
            'normal_col': [1, 2, 3],
            'all_nan_col': [np.nan, np.nan, np.nan]
        })
        processed_nan = processor.optimize_dtypes(all_nan_df)
        assert 'all_nan_col' in processed_nan.columns, "å…¨NaNã‚«ãƒ©ãƒ ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ"
        
        logger.info("âœ… ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®å‡¦ç†ãƒ†ã‚¹ãƒˆå®Œäº†")


def run_all_data_transformation_tests():
    """ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    logger.info("ğŸ”„ ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†æ•´åˆæ€§ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’é–‹å§‹")

    test_instance = TestDataTransformations()

    try:
        # åŸºæœ¬çš„ãªãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œï¼ˆç°¡ç•¥åŒ–ç‰ˆï¼‰
        logger.info("ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        mixed_type_data = test_instance.mixed_type_data()
        test_instance.test_data_type_optimization(mixed_type_data)

        logger.info("æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        sample_ohlcv_data = test_instance.sample_ohlcv_data()
        test_instance.test_time_series_index_handling(sample_ohlcv_data)

        logger.info("ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å‡¦ç†ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        test_instance.test_edge_case_handling()
        
        logger.info("ğŸ‰ ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†æ•´åˆæ€§ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†æ•´åˆæ€§ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    success = run_all_data_transformation_tests()
    sys.exit(0 if success else 1)
