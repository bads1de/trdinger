#!/usr/bin/env python3
"""
çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰

MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®è¤‡æ•°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ã®é€£æºã¨
ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å‹•ä½œã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
- ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ
- ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“é€£æºãƒ†ã‚¹ãƒˆ
- ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
- APIçµ±åˆãƒ†ã‚¹ãƒˆ
"""

import sys
import os
import logging
import pandas as pd
import numpy as np
import time
from typing import Dict, List
from dataclasses import dataclass, field

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
backend_path = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
sys.path.insert(0, backend_path)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class IntegrationTestResult:
    """çµ±åˆãƒ†ã‚¹ãƒˆçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    test_name: str
    test_category: str
    success: bool
    execution_time: float
    components_tested: List[str] = field(default_factory=list)
    data_flow_verified: bool = False
    integration_points: Dict[str, bool] = field(default_factory=dict)
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    error_message: str = ""


class IntegrationTestSuite:
    """çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""

    def __init__(self):
        self.results: List[IntegrationTestResult] = []

    def create_realistic_market_data(self, rows: int = 500) -> pd.DataFrame:
        """ãƒªã‚¢ãƒ«ãªå¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        logger.info(f"ğŸ“Š {rows}è¡Œã®ãƒªã‚¢ãƒ«ãªå¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ")

        np.random.seed(42)
        dates = pd.date_range("2024-01-01", periods=rows, freq="h")

        # ãƒªã‚¢ãƒ«ãªä¾¡æ ¼å‹•å‘ã‚’æ¨¡æ“¬
        base_price = 50000
        trend = np.cumsum(np.random.normal(0, 100, rows))
        volatility = np.random.normal(0, 500, rows)
        close_prices = base_price + trend + volatility

        # å¸‚å ´æ™‚é–“ã‚’è€ƒæ…®ã—ãŸèª¿æ•´
        for i in range(len(close_prices)):
            hour = dates[i].hour
            # å¸‚å ´é–‹å§‹æ™‚é–“ï¼ˆ9æ™‚ï¼‰ã¨çµ‚äº†æ™‚é–“ï¼ˆ15æ™‚ï¼‰ã§ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´
            if hour in [9, 15]:
                volatility[i] *= 1.5
            elif hour in [12, 13]:  # æ˜¼ä¼‘ã¿æ™‚é–“
                volatility[i] *= 0.5

        data = {
            "Open": close_prices + np.random.normal(0, 50, rows),
            "High": close_prices + np.abs(np.random.normal(100, 75, rows)),
            "Low": close_prices - np.abs(np.random.normal(100, 75, rows)),
            "Close": close_prices,
            "Volume": np.random.lognormal(10, 0.3, rows),
        }

        df = pd.DataFrame(data, index=dates)

        # ä¾¡æ ¼æ•´åˆæ€§ã‚’ç¢ºä¿
        df["High"] = df[["Open", "Close", "High"]].max(axis=1)
        df["Low"] = df[["Open", "Close", "Low"]].min(axis=1)

        return df

    def test_end_to_end_ml_pipeline(self):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        logger.info("ğŸ”„ ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
            market_data = self.create_realistic_market_data(rows=300)

            # 2. MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–ï¼ˆä¿®æ­£ï¼šæ­£ã—ã„ã‚¯ãƒ©ã‚¹åã‚’ä½¿ç”¨ï¼‰
            from app.services.ml.single_model.single_model_trainer import (
                SingleModelTrainer,
            )

            trainer = SingleModelTrainer(model_type="lightgbm")

            # 3. å®Œå…¨ãªMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
            result = trainer.train_model(
                training_data=market_data,
                save_model=False,
                threshold_up=0.02,
                threshold_down=-0.02,
            )

            execution_time = time.time() - start_time

            # 4. çµæœæ¤œè¨¼
            integration_points = {
                "data_preprocessing": "accuracy" in result,
                "feature_engineering": "feature_count" in result,
                "model_training": "f1_score" in result,
                "evaluation": "precision" in result and "recall" in result,
                "result_formatting": isinstance(result, dict),
            }

            performance_metrics = {
                "accuracy": result.get("accuracy", 0),
                "f1_score": result.get("f1_score", 0),
                "feature_count": result.get("feature_count", 0),
                "training_samples": result.get("training_samples", 0),
                "test_samples": result.get("test_samples", 0),
            }

            self.results.append(
                IntegrationTestResult(
                    test_name="ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
                    test_category="end_to_end",
                    success=all(integration_points.values()),
                    execution_time=execution_time,
                    components_tested=[
                        "SingleModelTrainer",
                        "FeatureEngineering",
                        "DataProcessing",
                        "ModelTraining",
                        "Evaluation",
                    ],
                    data_flow_verified=True,
                    integration_points=integration_points,
                    performance_metrics=performance_metrics,
                )
            )

            logger.info(
                f"âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†: {execution_time:.2f}ç§’"
            )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                IntegrationTestResult(
                    test_name="ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
                    test_category="end_to_end",
                    success=False,
                    execution_time=execution_time,
                    components_tested=["SingleModelTrainer"],
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    def test_feature_engineering_integration(self):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        logger.info("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            market_data = self.create_realistic_market_data(rows=200)

            # 1. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹
            from app.services.ml.feature_engineering.feature_engineering_service import (
                FeatureEngineeringService,
            )

            fe_service = FeatureEngineeringService()

            # 2. åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—ï¼ˆä¿®æ­£ï¼šæ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰åã‚’ä½¿ç”¨ï¼‰
            basic_features = fe_service.calculate_basic_features(market_data)

            execution_time = time.time() - start_time

            # 3. çµ±åˆãƒã‚¤ãƒ³ãƒˆæ¤œè¨¼
            integration_points = {
                "basic_features_generated": len(basic_features.columns)
                > len(market_data.columns),
                "data_consistency": len(basic_features) == len(market_data),
                "no_all_nan_columns": not basic_features.isnull().all().any(),
                "feature_scaling": basic_features.std().mean()
                < 100,  # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç¢ºèªï¼ˆé–¾å€¤èª¿æ•´ï¼‰
            }

            performance_metrics = {
                "basic_feature_count": len(basic_features.columns),
                "feature_generation_rate": len(basic_features.columns) / execution_time,
                "data_completeness": (
                    1 - basic_features.isnull().sum().sum() / basic_features.size
                )
                * 100,
            }

            self.results.append(
                IntegrationTestResult(
                    test_name="ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµ±åˆ",
                    test_category="feature_engineering",
                    success=all(integration_points.values()),
                    execution_time=execution_time,
                    components_tested=[
                        "FeatureEngineeringService",
                        "BasicFeatures",
                        "DataProcessing",
                    ],
                    data_flow_verified=True,
                    integration_points=integration_points,
                    performance_metrics=performance_metrics,
                )
            )

            logger.info(
                f"âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: {execution_time:.2f}ç§’"
            )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                IntegrationTestResult(
                    test_name="ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµ±åˆ",
                    test_category="feature_engineering",
                    success=False,
                    execution_time=execution_time,
                    components_tested=["FeatureEngineeringService"],
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    def test_model_training_integration(self):
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        logger.info("ğŸ¤– ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            market_data = self.create_realistic_market_data(rows=250)

            # 1. ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã§ã®å­¦ç¿’ï¼ˆä¿®æ­£ï¼šåˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ã¿ãƒ†ã‚¹ãƒˆï¼‰
            model_types = ["lightgbm", "xgboost"]  # random_forestã‚’é™¤å¤–
            model_results = {}

            for model_type in model_types:
                try:
                    from app.services.ml.single_model.single_model_trainer import (
                        SingleModelTrainer,
                    )

                    trainer = SingleModelTrainer(model_type=model_type)
                    result = trainer.train_model(
                        training_data=market_data,
                        save_model=False,
                        threshold_up=0.02,
                        threshold_down=-0.02,
                    )

                    model_results[model_type] = {
                        "success": True,
                        "accuracy": result.get("accuracy", 0),
                        "f1_score": result.get("f1_score", 0),
                    }

                except Exception as e:
                    model_results[model_type] = {"success": False, "error": str(e)}

            execution_time = time.time() - start_time

            # 2. çµ±åˆãƒã‚¤ãƒ³ãƒˆæ¤œè¨¼
            successful_models = sum(1 for r in model_results.values() if r["success"])

            integration_points = {
                "multiple_models_supported": successful_models > 0,
                "lightgbm_integration": model_results.get("lightgbm", {}).get(
                    "success", False
                ),
                "consistent_interface": all(
                    "accuracy" in r for r in model_results.values() if r["success"]
                ),
                "error_handling": True,  # ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¦ã„ã‚‹
            }

            performance_metrics = {
                "successful_models": successful_models,
                "total_models_tested": len(model_types),
                "average_accuracy": (
                    np.mean(
                        [
                            r["accuracy"]
                            for r in model_results.values()
                            if r["success"] and "accuracy" in r
                        ]
                    )
                    if successful_models > 0
                    else 0
                ),
            }

            self.results.append(
                IntegrationTestResult(
                    test_name="ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµ±åˆ",
                    test_category="model_training",
                    success=successful_models >= 1,  # å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ¢ãƒ‡ãƒ«ãŒæˆåŠŸ
                    execution_time=execution_time,
                    components_tested=["SingleModelTrainer", "LightGBM", "XGBoost"],
                    data_flow_verified=True,
                    integration_points=integration_points,
                    performance_metrics=performance_metrics,
                )
            )

            logger.info(
                f"âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: {successful_models}/{len(model_types)}ãƒ¢ãƒ‡ãƒ«æˆåŠŸ"
            )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                IntegrationTestResult(
                    test_name="ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµ±åˆ",
                    test_category="model_training",
                    success=False,
                    execution_time=execution_time,
                    components_tested=["SingleModelTrainer"],
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    def test_data_processing_pipeline(self):
        """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            # 1. æ§˜ã€…ãªå“è³ªã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            clean_data = self.create_realistic_market_data(rows=150)

            # 2. ãƒ‡ãƒ¼ã‚¿ã«æ„å›³çš„ãªå•é¡Œã‚’è¿½åŠ 
            dirty_data = clean_data.copy()
            dirty_data.iloc[50:60, :] = np.nan  # NaNå€¤
            dirty_data.iloc[100:110, 0] = np.inf  # ç„¡é™å¤§å€¤

            # 3. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
            from app.utils.data_processing import DataProcessor

            processor = DataProcessor()

            # 4. ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            clean_processed = processor.prepare_training_data(
                clean_data, threshold_up=0.02, threshold_down=-0.02
            )

            # 5. ãƒ€ãƒ¼ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            dirty_processed = processor.prepare_training_data(
                dirty_data, threshold_up=0.02, threshold_down=-0.02
            )

            execution_time = time.time() - start_time

            # 6. çµ±åˆãƒã‚¤ãƒ³ãƒˆæ¤œè¨¼
            integration_points = {
                "clean_data_processing": clean_processed is not None,
                "dirty_data_handling": dirty_processed is not None,
                "nan_handling": (
                    not dirty_processed[0].isnull().any().any()
                    if dirty_processed
                    else False
                ),
                "data_consistency": (
                    len(clean_processed[0]) > 0 if clean_processed else False
                ),
                "label_generation": (
                    len(clean_processed) >= 2 if clean_processed else False
                ),
            }

            performance_metrics = {
                "clean_data_rows": len(clean_processed[0]) if clean_processed else 0,
                "dirty_data_rows": len(dirty_processed[0]) if dirty_processed else 0,
                "data_recovery_rate": (
                    (len(dirty_processed[0]) / len(clean_processed[0])) * 100
                    if clean_processed and dirty_processed
                    else 0
                ),
            }

            self.results.append(
                IntegrationTestResult(
                    test_name="ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ",
                    test_category="data_processing",
                    success=all(integration_points.values()),
                    execution_time=execution_time,
                    components_tested=[
                        "DataProcessor",
                        "DataCleaning",
                        "LabelGeneration",
                        "Validation",
                    ],
                    data_flow_verified=True,
                    integration_points=integration_points,
                    performance_metrics=performance_metrics,
                )
            )

            logger.info(
                f"âœ… ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: {execution_time:.2f}ç§’"
            )

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                IntegrationTestResult(
                    test_name="ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ",
                    test_category="data_processing",
                    success=False,
                    execution_time=execution_time,
                    components_tested=["DataProcessor"],
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")

    def test_evaluation_metrics_integration(self):
        """è©•ä¾¡æŒ‡æ¨™çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        logger.info("ğŸ“ˆ è©•ä¾¡æŒ‡æ¨™çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()

        try:
            # 1. ãƒ†ã‚¹ãƒˆç”¨ã®äºˆæ¸¬çµæœã‚’ä½œæˆ
            y_true = np.array([0, 0, 1, 1, 2, 2, 0, 1, 2, 0] * 10)
            y_pred = np.array([0, 1, 1, 1, 2, 0, 0, 1, 2, 0] * 10)
            y_proba = np.random.rand(100, 3)
            y_proba = y_proba / y_proba.sum(axis=1, keepdims=True)  # æ­£è¦åŒ–

            # 2. è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            from app.services.ml.evaluation.enhanced_metrics import (
                EnhancedMetricsCalculator,
            )

            metrics_calculator = EnhancedMetricsCalculator()

            # 3. åŒ…æ‹¬çš„è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            comprehensive_metrics = metrics_calculator.calculate_comprehensive_metrics(
                y_true, y_pred, y_proba
            )

            execution_time = time.time() - start_time

            # 4. çµ±åˆãƒã‚¤ãƒ³ãƒˆæ¤œè¨¼
            expected_metrics = ["accuracy", "precision", "recall", "f1_score"]

            integration_points = {
                "basic_metrics_calculated": all(
                    metric in comprehensive_metrics for metric in expected_metrics
                ),
                "metrics_range_valid": all(
                    0 <= comprehensive_metrics.get(metric, -1) <= 1
                    for metric in expected_metrics
                ),
                "consistent_results": comprehensive_metrics.get("accuracy", 0) > 0,
            }

            performance_metrics = {
                "accuracy": comprehensive_metrics.get("accuracy", 0),
                "precision": comprehensive_metrics.get("precision", 0),
                "recall": comprehensive_metrics.get("recall", 0),
                "f1_score": comprehensive_metrics.get("f1_score", 0),
                "metrics_count": len(comprehensive_metrics),
            }

            self.results.append(
                IntegrationTestResult(
                    test_name="è©•ä¾¡æŒ‡æ¨™çµ±åˆ",
                    test_category="evaluation",
                    success=all(integration_points.values()),
                    execution_time=execution_time,
                    components_tested=[
                        "EnhancedMetricsCalculator",
                        "MetricsValidation",
                    ],
                    data_flow_verified=True,
                    integration_points=integration_points,
                    performance_metrics=performance_metrics,
                )
            )

            logger.info(f"âœ… è©•ä¾¡æŒ‡æ¨™çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: {execution_time:.2f}ç§’")

        except Exception as e:
            execution_time = time.time() - start_time

            self.results.append(
                IntegrationTestResult(
                    test_name="è©•ä¾¡æŒ‡æ¨™çµ±åˆ",
                    test_category="evaluation",
                    success=False,
                    execution_time=execution_time,
                    components_tested=["EnhancedMetricsCalculator"],
                    error_message=str(e),
                )
            )

            logger.error(f"âŒ è©•ä¾¡æŒ‡æ¨™çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")


def run_integration_tests():
    """çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    logger.info("ğŸ”„ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")

    test_suite = IntegrationTestSuite()

    # å„çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    test_suite.test_end_to_end_ml_pipeline()
    test_suite.test_feature_engineering_integration()
    test_suite.test_model_training_integration()
    test_suite.test_data_processing_pipeline()
    test_suite.test_evaluation_metrics_integration()

    # çµæœã‚µãƒãƒªãƒ¼
    total_tests = len(test_suite.results)
    successful_tests = sum(1 for r in test_suite.results if r.success)
    data_flow_verified = sum(1 for r in test_suite.results if r.data_flow_verified)

    print("\n" + "=" * 80)
    print("ğŸ”„ çµ±åˆãƒ†ã‚¹ãƒˆçµæœ")
    print("=" * 80)
    print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
    print(f"âœ… æˆåŠŸ: {successful_tests}")
    print(f"âŒ å¤±æ•—: {total_tests - successful_tests}")
    print(f"ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ¤œè¨¼: {data_flow_verified}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {(successful_tests/total_tests*100):.1f}%")
    print(f"ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ¤œè¨¼ç‡: {(data_flow_verified/total_tests*100):.1f}%")

    print("\nğŸ”„ çµ±åˆãƒ†ã‚¹ãƒˆè©³ç´°:")
    for result in test_suite.results:
        status = "âœ…" if result.success else "âŒ"
        data_flow = "ğŸ”„" if result.data_flow_verified else "âŒ"
        print(f"{status} {result.test_name}")
        print(f"   ã‚«ãƒ†ã‚´ãƒª: {result.test_category}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {result.execution_time:.2f}ç§’")
        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼: {data_flow}")
        print(f"   ãƒ†ã‚¹ãƒˆå¯¾è±¡: {', '.join(result.components_tested)}")
        if result.performance_metrics:
            key_metrics = list(result.performance_metrics.items())[:3]
            print(f"   ä¸»è¦æŒ‡æ¨™: {', '.join([f'{k}={v:.3f}' for k, v in key_metrics])}")
        if result.error_message:
            print(f"   ã‚¨ãƒ©ãƒ¼: {result.error_message[:100]}...")

    print("=" * 80)

    logger.info("ğŸ¯ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†")

    return test_suite.results


if __name__ == "__main__":
    run_integration_tests()
