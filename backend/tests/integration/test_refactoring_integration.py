"""
ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆ

ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŽé›†æ©Ÿèƒ½ã®é‡è¤‡è§£æ¶ˆã¨å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã®çµ±åˆã®ãƒ†ã‚¹ãƒˆ
"""

import logging
import numpy as np
import pandas as pd
import pytest
from datetime import datetime

from app.services.ml.common.metrics_constants import (
    StandardMetricNames,
    MetricValidation,
    StandardMetricDefinitions,
)
from app.services.ml.common.unified_metrics_manager import (
    UnifiedMetricsManager,
    unified_metrics_manager,
)
from app.services.ml.common.trainer_factory import (
    TrainerFactory,
    TrainerConfig,
    TrainerType,
    create_single_model_trainer,
    create_ensemble_trainer,
    trainer_factory,
)
from app.services.ml.evaluation.enhanced_metrics import (
    EnhancedMetricsCalculator,
    MetricsConfig,
)

logger = logging.getLogger(__name__)


class TestMetricsIntegration:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±åˆæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""

    def test_metrics_constants(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šæ•°ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šæ•°ãƒ†ã‚¹ãƒˆ ===")

        # æ¨™æº–ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ç¢ºèª
        assert StandardMetricNames.ACCURACY == "accuracy"
        assert StandardMetricNames.F1_SCORE == "f1_score"
        assert StandardMetricNames.BALANCED_ACCURACY == "balanced_accuracy"

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©ã®ç¢ºèª
        accuracy_def = StandardMetricDefinitions.get_definition(StandardMetricNames.ACCURACY)
        assert accuracy_def is not None
        assert accuracy_def.range_min == 0.0
        assert accuracy_def.range_max == 1.0
        assert accuracy_def.higher_is_better is True

        logger.info("âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šæ•°ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_metric_validation(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œè¨¼ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œè¨¼ãƒ†ã‚¹ãƒˆ ===")

        # æœ‰åŠ¹ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ãƒ†ã‚¹ãƒˆ
        assert MetricValidation.is_valid_metric_name("accuracy") is True
        assert MetricValidation.is_valid_metric_name("invalid_metric") is False

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã®æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
        assert MetricValidation.validate_metric_value("accuracy", 0.85) is True
        assert MetricValidation.validate_metric_value("accuracy", 1.5) is False  # ç¯„å›²å¤–
        assert MetricValidation.validate_metric_value("accuracy", -0.1) is False  # ç¯„å›²å¤–

        logger.info("âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_unified_metrics_manager(self):
        """çµ±ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç®¡ç†ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== çµ±ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç®¡ç†ãƒ†ã‚¹ãƒˆ ===")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        np.random.seed(42)
        y_true = np.random.choice([0, 1, 2], size=100, p=[0.4, 0.4, 0.2])
        y_pred = np.random.choice([0, 1, 2], size=100, p=[0.3, 0.5, 0.2])
        y_proba = np.random.dirichlet([1, 1, 1], size=100)

        # çµ±ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç®¡ç†ã§ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        manager = UnifiedMetricsManager()
        evaluation_result = manager.evaluate_and_record_model(
            model_name="test_model",
            model_type="test_type",
            y_true=y_true,
            y_pred=y_pred,
            y_proba=y_proba,
            class_names=["Down", "Hold", "Up"],
            dataset_info={"samples": len(y_true)},
            training_params={"test": True},
        )

        # çµæžœã®ç¢ºèª
        assert "accuracy" in evaluation_result
        assert "f1_score" in evaluation_result
        assert "balanced_accuracy" in evaluation_result
        assert isinstance(evaluation_result["accuracy"], float)

        # åŒ…æ‹¬çš„ã‚µãƒžãƒªãƒ¼ã®å–å¾—
        summary = manager.get_comprehensive_summary(time_window_minutes=60)
        assert "model_evaluation_metrics" in summary
        assert "system_metrics" in summary

        logger.info("âœ… çµ±ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç®¡ç†ãƒ†ã‚¹ãƒˆå®Œäº†")


class TestTrainerFactory:
    """ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""

    def test_trainer_config(self):
        """ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šãƒ†ã‚¹ãƒˆ ===")

        # å˜ä¸€ãƒ¢ãƒ‡ãƒ«è¨­å®š
        single_config = TrainerConfig(
            trainer_type=TrainerType.SINGLE_MODEL,
            model_type="lightgbm",
            automl_config=None,
        )
        assert single_config.trainer_type == TrainerType.SINGLE_MODEL
        assert single_config.model_type == "lightgbm"

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š
        ensemble_config = TrainerConfig(
            trainer_type=TrainerType.ENSEMBLE,
            model_type="bagging",
            ensemble_config={
                "method": "bagging",
                "bagging_params": {"n_estimators": 3}
            }
        )
        assert ensemble_config.trainer_type == TrainerType.ENSEMBLE
        assert ensemble_config.ensemble_config["method"] == "bagging"

        logger.info("âœ… ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_trainer_factory_creation(self):
        """ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ä½œæˆã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ ===")

        factory = TrainerFactory()

        # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¿ã‚¤ãƒ—ã®ç¢ºèª
        supported_trainer_types = factory.get_supported_trainer_types()
        assert "single_model" in supported_trainer_types
        assert "ensemble" in supported_trainer_types

        supported_model_types = factory.get_supported_model_types()
        assert "lightgbm" in supported_model_types
        assert "bagging" in supported_model_types

        logger.info("âœ… ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_single_model_trainer_creation(self):
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ ===")

        # ä¾¿åˆ©é–¢æ•°ã‚’ä½¿ç”¨
        trainer = create_single_model_trainer(
            model_type="lightgbm",
            automl_config=None,
        )

        assert trainer is not None
        assert hasattr(trainer, 'model_type')
        assert trainer.model_type == "lightgbm"

        logger.info("âœ… å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_ensemble_trainer_creation(self):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ ===")

        # ä¾¿åˆ©é–¢æ•°ã‚’ä½¿ç”¨
        trainer = create_ensemble_trainer(
            ensemble_method="bagging",
            automl_config=None,
        )

        assert trainer is not None
        assert hasattr(trainer, 'ensemble_method')
        assert trainer.ensemble_method == "bagging"

        logger.info("âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆå®Œäº†")


class TestIntegrationWorkflow:
    """çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""

    def test_end_to_end_workflow(self):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ ===")

        # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        np.random.seed(42)
        n_samples = 50
        n_features = 5

        # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        X = pd.DataFrame(
            np.random.randn(n_samples, n_features),
            columns=[f"feature_{i}" for i in range(n_features)]
        )
        
        # ãƒ©ãƒ™ãƒ«ä½œæˆ
        y = pd.Series(np.random.choice([0, 1, 2], size=n_samples))

        # 2. TrainerFactoryã§ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆ
        trainer = create_single_model_trainer(model_type="lightgbm")

        # 3. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        # 4. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        try:
            # å®Ÿéš›ã®å­¦ç¿’ã¯è¤‡é›‘ãªã®ã§ã€ã“ã“ã§ã¯çµ±ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡ã®ã¿ãƒ†ã‚¹ãƒˆ
            y_pred = np.random.choice([0, 1, 2], size=len(y_test))
            y_proba = np.random.dirichlet([1, 1, 1], size=len(y_test))

            # 5. çµ±ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡
            evaluation_result = unified_metrics_manager.evaluate_and_record_model(
                model_name="integration_test_model",
                model_type="lightgbm",
                y_true=y_test.values,
                y_pred=y_pred,
                y_proba=y_proba,
                dataset_info={"train_samples": len(X_train), "test_samples": len(X_test)},
                training_params={"integration_test": True},
            )

            # 6. çµæžœæ¤œè¨¼
            assert "accuracy" in evaluation_result
            assert "f1_score" in evaluation_result
            assert evaluation_result["accuracy"] >= 0.0
            assert evaluation_result["accuracy"] <= 1.0

            logger.info(f"çµ±åˆãƒ†ã‚¹ãƒˆçµæžœ: accuracy={evaluation_result['accuracy']:.4f}")

        except Exception as e:
            logger.warning(f"çµ±åˆãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ã“ã‚Œã¯æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œã§ã™: {e}")

        logger.info("âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆå®Œäº†")


def test_metrics_integration():
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    test_class = TestMetricsIntegration()
    test_class.test_metrics_constants()
    test_class.test_metric_validation()
    test_class.test_unified_metrics_manager()


def test_trainer_factory():
    """ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    test_class = TestTrainerFactory()
    test_class.test_trainer_config()
    test_class.test_trainer_factory_creation()
    test_class.test_single_model_trainer_creation()
    test_class.test_ensemble_trainer_creation()


def test_integration_workflow():
    """çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    test_class = TestIntegrationWorkflow()
    test_class.test_end_to_end_workflow()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("ðŸš€ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    test_metrics_integration()
    test_trainer_factory()
    test_integration_workflow()
    
    print("âœ… ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
