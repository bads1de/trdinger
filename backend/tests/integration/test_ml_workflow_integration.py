"""
MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ

ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ã®ç›¸äº’ä½œç”¨ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®çµ±åˆå‹•ä½œã‚’æ¤œè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã€‚
"""

import numpy as np
import pandas as pd
import logging
import time
import psutil
import os
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils.data_processing import DataProcessor
from app.utils.label_generation import LabelGenerator, ThresholdMethod
from app.services.ml.feature_engineering.feature_engineering_service import FeatureEngineeringService
from app.services.ml.ml_training_service import MLTrainingService

logger = logging.getLogger(__name__)


class TestMLWorkflowIntegration:
    """MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def create_realistic_market_data(self, size: int = 1000) -> pd.DataFrame:
        """ç¾å®Ÿçš„ãªå¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=size, freq='1h')
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªä¾¡æ ¼å‹•ä½œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        base_price = 50000
        volatility = 0.02
        trend = 0.0001  # å¾®å°ãªä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
        
        prices = [base_price]
        for i in range(1, size):
            # ãƒˆãƒ¬ãƒ³ãƒ‰ + ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ + å¹³å‡å›å¸°
            trend_component = trend * i
            random_component = np.random.normal(0, volatility)
            mean_reversion = -0.001 * (prices[-1] - base_price) / base_price
            
            change = trend_component + random_component + mean_reversion
            new_price = prices[-1] * (1 + change)
            prices.append(max(new_price, base_price * 0.5))  # ä¾¡æ ¼ä¸‹é™è¨­å®š
        
        # OHLCVç”Ÿæˆ
        opens = prices
        highs = [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices]
        lows = [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices]
        closes = [p * (1 + np.random.normal(0, 0.002)) for p in prices]
        volumes = np.random.lognormal(10, 0.5, size)
        
        return pd.DataFrame({
            'timestamp': dates,
            'Open': opens,
            'High': highs,
            'Low': lows,
            'Close': closes,
            'Volume': volumes
        }).set_index('timestamp')

    def test_module_interaction_pipeline(self):
        """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ç›¸äº’ä½œç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ç›¸äº’ä½œç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ ===")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        raw_data = self.create_realistic_market_data(500)
        
        # Step 1: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        processor = DataProcessor()
        processed_data = processor.preprocess_features(
            raw_data[['Close', 'Volume']].copy(),
            scale_features=False,  # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å‰ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãªã„
            remove_outliers=True
        )
        
        logger.info(f"å‰å‡¦ç†å®Œäº†: {len(processed_data)}è¡Œ â†’ {processed_data.shape}")
        
        # Step 2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        fe_service = FeatureEngineeringService()
        
        # å‰å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã®OHLCVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æˆ»ã™
        ohlcv_data = raw_data.loc[processed_data.index].copy()
        
        features = fe_service.calculate_advanced_features(ohlcv_data)
        
        logger.info(f"ç‰¹å¾´é‡è¨ˆç®—å®Œäº†: {features.shape[1]}å€‹ã®ç‰¹å¾´é‡")
        
        # Step 3: ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
        label_generator = LabelGenerator()
        price_series = ohlcv_data['Close']
        
        labels, threshold_info = label_generator.generate_labels(
            price_series,
            method=ThresholdMethod.FIXED,
            threshold_up=0.02,
            threshold_down=-0.02
        )
        
        logger.info(f"ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†: {len(labels)}å€‹ã®ãƒ©ãƒ™ãƒ«")
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æ¤œè¨¼
        assert len(features) > 0, "ç‰¹å¾´é‡ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
        assert len(labels) > 0, "ãƒ©ãƒ™ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ•´åˆæ€§ç¢ºèª
        common_index = features.index.intersection(labels.index)
        assert len(common_index) > len(features) * 0.8, "ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•´åˆæ€§ãŒä½ã™ãã¾ã™"
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª
        assert features.select_dtypes(include=[np.number]).shape[1] == features.shape[1], \
            "æ•°å€¤ä»¥å¤–ã®ç‰¹å¾´é‡ãŒå«ã¾ã‚Œã¦ã„ã¾ã™"
        
        logger.info("âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ç›¸äº’ä½œç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_end_to_end_ml_workflow(self):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ ===")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        raw_data = self.create_realistic_market_data(1000)
        
        try:
            # MLTrainingServiceã‚’ä½¿ç”¨ã—ãŸå®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
            ml_service = MLTrainingService()
            
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            ohlcv_data = raw_data.copy()
            
            # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®ç”Ÿæˆ
            fe_service = FeatureEngineeringService()
            features = fe_service.calculate_advanced_features(ohlcv_data)
            
            label_generator = LabelGenerator()
            labels, _ = label_generator.generate_labels(
                ohlcv_data['Close'],
                method=ThresholdMethod.FIXED,
                threshold_up=0.02,
                threshold_down=-0.02
            )
            
            # ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ç¢ºèª
            common_index = features.index.intersection(labels.index)
            aligned_features = features.loc[common_index]
            aligned_labels = labels.loc[common_index]
            
            logger.info(f"æ•´åˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: ç‰¹å¾´é‡{aligned_features.shape}, ãƒ©ãƒ™ãƒ«{len(aligned_labels)}")
            
            # æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿è¦ä»¶ç¢ºèª
            assert len(aligned_features) >= 100, "è¨“ç·´ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
            assert aligned_features.shape[1] >= 10, "ç‰¹å¾´é‡æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            
            # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®ç¢ºèª
            label_counts = aligned_labels.value_counts()
            assert len(label_counts) >= 2, "ãƒ©ãƒ™ãƒ«ã®å¤šæ§˜æ€§ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            
            logger.info(f"ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {label_counts.to_dict()}")
            logger.info("âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆå®Œäº†")
            
        except Exception as e:
            logger.warning(f"MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿï¼ˆæœŸå¾…ã•ã‚Œã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™ï¼‰: {e}")
            # ä¸€éƒ¨ã®ã‚¨ãƒ©ãƒ¼ã¯è¨­å®šä¸è¶³ç­‰ã§ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€è­¦å‘Šã¨ã—ã¦å‡¦ç†

    def test_large_dataset_integration(self):
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆãƒ†ã‚¹ãƒˆ"""
        logger.info("=== å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆãƒ†ã‚¹ãƒˆ ===")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ10ä¸‡è¡Œï¼‰
        large_data = self.create_realistic_market_data(100000)
        
        start_time = time.time()
        
        try:
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
            processor = DataProcessor()
            
            # ãƒãƒƒãƒå‡¦ç†ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å‘ä¸Š
            batch_size = 10000
            processed_batches = []
            
            for i in range(0, len(large_data), batch_size):
                batch = large_data.iloc[i:i+batch_size]
                processed_batch = processor.preprocess_features(
                    batch[['Close', 'Volume']].copy(),
                    scale_features=True,
                    remove_outliers=True
                )
                processed_batches.append(processed_batch)
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                current_memory = process.memory_info().rss
                memory_increase = current_memory - initial_memory
                
                if memory_increase > 1e9:  # 1GBä»¥ä¸Šã®å¢—åŠ 
                    logger.warning(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§å¹…ã«å¢—åŠ : {memory_increase/1e6:.1f}MB")
            
            # ãƒãƒƒãƒçµåˆ
            final_processed = pd.concat(processed_batches, ignore_index=False)
            
            processing_time = time.time() - start_time
            final_memory = process.memory_info().rss
            total_memory_increase = final_memory - initial_memory
            
            logger.info(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†:")
            logger.info(f"  - å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
            logger.info(f"  - ãƒ¡ãƒ¢ãƒªå¢—åŠ : {total_memory_increase/1e6:.1f}MB")
            logger.info(f"  - å‡¦ç†åŠ¹ç‡: {len(large_data)/processing_time:.0f}è¡Œ/ç§’")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ã®ç¢ºèª
            assert processing_time < 300, f"å‡¦ç†æ™‚é–“ãŒé•·ã™ãã¾ã™: {processing_time:.2f}ç§’"
            assert total_memory_increase < 2e9, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã™ãã¾ã™: {total_memory_increase/1e6:.1f}MB"
            
            logger.info("âœ… å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
            
        except MemoryError:
            logger.error("âŒ ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            raise
        except Exception as e:
            logger.error(f"âŒ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            raise

    def test_data_consistency_across_modules(self):
        """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ã§ã®ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ ===")
        
        # åŒã˜ãƒ‡ãƒ¼ã‚¿ã§è¤‡æ•°å›å‡¦ç†
        raw_data = self.create_realistic_market_data(500)
        
        # è¤‡æ•°å›ã®å‡¦ç†çµæœã‚’æ¯”è¼ƒ
        results = []
        for i in range(3):
            processor = DataProcessor()
            processed = processor.preprocess_features(
                raw_data[['Close', 'Volume']].copy(),
                scale_features=True,
                remove_outliers=True
            )
            results.append(processed)
        
        # çµæœã®ä¸€è²«æ€§ç¢ºèª
        for i in range(1, len(results)):
            try:
                pd.testing.assert_frame_equal(
                    results[0], results[i],
                    check_exact=False,
                    rtol=1e-10
                )
            except AssertionError as e:
                logger.error(f"å‡¦ç†çµæœã®ä¸€è²«æ€§ã‚¨ãƒ©ãƒ¼ï¼ˆå®Ÿè¡Œ{i}ï¼‰: {e}")
                raise
        
        logger.info("âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆå®Œäº†")

    def test_error_propagation(self):
        """ã‚¨ãƒ©ãƒ¼ä¼æ’­ãƒ†ã‚¹ãƒˆ"""
        logger.info("=== ã‚¨ãƒ©ãƒ¼ä¼æ’­ãƒ†ã‚¹ãƒˆ ===")
        
        # ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿ã§ã‚¨ãƒ©ãƒ¼ä¼æ’­ã‚’ç¢ºèª
        invalid_data = pd.DataFrame({
            'Close': [np.nan, np.inf, -np.inf, 0, 1],
            'Volume': [0, -1, np.nan, np.inf, 1]
        })
        
        processor = DataProcessor()
        
        try:
            # ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã‚‹ã‹ç¢ºèª
            result = processor.preprocess_features(
                invalid_data,
                scale_features=True,
                remove_outliers=True
            )
            
            # çµæœãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            assert not result.isnull().all().all(), "ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒNaNã«ãªã‚Šã¾ã—ãŸ"
            assert np.isfinite(result.select_dtypes(include=[np.number])).all().all(), \
                "ç„¡é™å¤§å€¤ãŒæ®‹ã£ã¦ã„ã¾ã™"
            
            logger.info("âœ… ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ")
            
        except Exception as e:
            logger.info(f"âœ… æœŸå¾…é€šã‚Šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        
        logger.info("âœ… ã‚¨ãƒ©ãƒ¼ä¼æ’­ãƒ†ã‚¹ãƒˆå®Œäº†")


def run_all_integration_tests():
    """ã™ã¹ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    logger.info("ğŸ”— MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’é–‹å§‹")
    
    test_instance = TestMLWorkflowIntegration()
    
    try:
        test_instance.test_module_interaction_pipeline()
        test_instance.test_end_to_end_ml_workflow()
        test_instance.test_large_dataset_integration()
        test_instance.test_data_consistency_across_modules()
        test_instance.test_error_propagation()
        
        logger.info("ğŸ‰ ã™ã¹ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ çµ±åˆãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    success = run_all_integration_tests()
    sys.exit(0 if success else 1)
