"""
å…¨ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMã€XGBoostï¼‰ã§ã®ç‰¹å¾´é‡æ€§èƒ½æ¤œè¨¼çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ—¢å­˜ã®93ç‰¹å¾´é‡ã‚’3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡ã—ã€
å‰Šæ¸›å¯èƒ½ãªç‰¹å¾´é‡ã‚’ç‰¹å®šã—ã¾ã™ã€‚

TimeSeriesSplitã‚’ä½¿ç”¨ã—ãŸæ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€
æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸè©•ä¾¡ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

å®Ÿè¡Œæ–¹æ³•:
    cd backend
    python -m scripts.feature_evaluation.evaluate_feature_performance
    python -m scripts.feature_evaluation.evaluate_feature_performance --models lightgbm
    python -m scripts.feature_evaluation.evaluate_feature_performance \
        --models lightgbm xgboost
    python -m scripts.feature_evaluation.evaluate_feature_performance --models all

è¨­å®š:
    - TimeSeriesSplitåˆ†å‰²æ•°: ml_config.training.cv_folds (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)
    - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°: 3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆ0=DOWN, 1=RANGE, 2=UPï¼‰

æ³¨æ„:
    ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å€‹åˆ¥ã«å®Ÿè¡Œå¯èƒ½ã§ã™ãŒã€çµ±åˆåˆ†æã®ãŸã‚
    run_unified_analysis.py ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
    çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä¸€è²«ã—ãŸãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‚’ä½¿ç”¨ã—ã€
    3ã¤ã®åˆ†æã‚’çµ±åˆã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import argparse
import json
import logging
import sys
import time
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    f1_score,
    precision_score,
    recall_score,
)
from sklearn.model_selection import TimeSeriesSplit

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from app.config.unified_config import unified_config
from app.services.optimization.ensemble_parameter_space import EnsembleParameterSpace
from app.services.optimization.optuna_optimizer import (
    OptunaOptimizer,
    ParameterSpace,
)
from app.utils.label_generation.enums import ThresholdMethod
from app.utils.label_generation.main import LabelGenerator
from scripts.feature_evaluation.common_feature_evaluator import (
    CommonFeatureEvaluator,
    EvaluationData,
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class BaseFeatureEvaluator(ABC):  # TODO: å¾Œç¶šã§CommonFeatureEvaluatorã«å®Œå…¨çµ±åˆäºˆå®š
    """ç‰¹å¾´é‡è©•ä¾¡ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, model_name: str):
        """
        åˆæœŸåŒ–

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
        """
        self.model_name = model_name
        self.common = CommonFeatureEvaluator()
        self.results = {}

    def __enter__(self):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼: å…¥å ´"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼: é€€å ´"""
        self.common.close()

    def fetch_data(
        self, symbol: str = "BTC/USDT:USDT", limit: int = 2000
    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.DataFrame]]:
        """
        DBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            symbol: å–å¼•ãƒšã‚¢
            limit: å–å¾—ä»¶æ•°

        Returns:
            (OHLCV, FR, OI)ã®ã‚¿ãƒ—ãƒ«
        """
        data = self.common.fetch_data(symbol=symbol, timeframe="1h", limit=limit)
        return data.ohlcv, data.fr, data.oi

    def calculate_features(
        self,
        ohlcv_df: pd.DataFrame,
        fr_df: Optional[pd.DataFrame],
        oi_df: Optional[pd.DataFrame],
    ) -> pd.DataFrame:
        """
        ç‰¹å¾´é‡è¨ˆç®—ï¼ˆãƒ¡ã‚¤ãƒ³MLã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜å®Œå…¨ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ï¼‰

        Args:
            ohlcv_df: OHLCVãƒ‡ãƒ¼ã‚¿
            fr_df: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
            oi_df: ã‚ªãƒ¼ãƒ—ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¬ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿

        Returns:
            ç‰¹å¾´é‡DataFrame
        """
        logger.info(f"[{self.model_name}] ç‰¹å¾´é‡è¨ˆç®—é–‹å§‹ï¼ˆå®Œå…¨ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆï¼‰")

        try:
            # ãƒ¡ã‚¤ãƒ³MLã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜å®Œå…¨ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’è¨ˆç®—
            # - åŸºæœ¬ç‰¹å¾´é‡
            # - æš—å·é€šè²¨ç‰¹åŒ–ç‰¹å¾´é‡ï¼ˆCryptoFeaturesï¼‰
            # - é«˜åº¦ãªç‰¹å¾´é‡ï¼ˆAdvancedFeatureEngineerï¼‰
            data = EvaluationData(ohlcv=ohlcv_df, fr=fr_df, oi=oi_df)
            features_df = self.common.build_basic_features(
                data=data,
                skip_crypto_and_advanced=False,  # ãƒ¡ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜ãå…¨ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
            )
            result_df = self.common.drop_ohlcv_columns(
                features_df,
                keep_close=True,
            )
            logger.info(f"ç‰¹å¾´é‡è¨ˆç®—å®Œäº†: {len(result_df.columns)}å€‹ã®ç‰¹å¾´é‡ï¼ˆå®Œå…¨ã‚»ãƒƒãƒˆï¼‰")
            return result_df

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def create_target(self, df: pd.DataFrame, periods: int = 1) -> pd.Series:
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰

        ãƒ¡ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã®LabelGeneratorã‚’ä½¿ç”¨ã—ã¦ã€ä¾¡æ ¼å¤‰åŒ–ã‹ã‚‰
        3ã‚¯ãƒ©ã‚¹åˆ†é¡ãƒ©ãƒ™ãƒ«ï¼ˆ0=DOWN, 1=RANGE, 2=UPï¼‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            df: closeã‚«ãƒ©ãƒ ã‚’å«ã‚€DataFrame
            periods: å…ˆèª­ã¿æœŸé–“ï¼ˆä½¿ç”¨ã—ãªã„ï¼šäº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰

        Returns:
            ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡: 0=DOWN, 1=RANGE, 2=UPï¼‰
        """
        if "close" not in df.columns:
            raise ValueError("closeã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # ãƒ¡ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã®LabelGeneratorã‚’ä½¿ç”¨
        label_generator = LabelGenerator()

        # æ¨™æº–åå·®æ³•ã§ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®std_multiplier=0.5ã‚’ä½¿ç”¨ï¼‰
        labels, threshold_info = label_generator.generate_labels(
            price_data=df["close"],
            method=ThresholdMethod.STD_DEVIATION,
            std_multiplier=0.5,
        )

        logger.info(
            f"ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†: "
            f"UP={threshold_info['up_count']}"
            f"({threshold_info['up_ratio']*100:.1f}%), "
            f"DOWN={threshold_info['down_count']}"
            f"({threshold_info['down_ratio']*100:.1f}%), "
            f"RANGE={threshold_info['range_count']}"
            f"({threshold_info['range_ratio']*100:.1f}%)"
        )

        return labels

    @abstractmethod
    def evaluate_model_cv(
        self, X: pd.DataFrame, y: pd.Series, n_splits: Optional[int] = None
    ) -> Dict[str, float]:
        """
        TimeSeriesSplitã§ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡

        æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’è€ƒæ…®ã—ã€éå»ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ã¦æœªæ¥ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã—ã¾ã™ã€‚
        åˆ†å‰²æ•°ã¯ml_configã‹ã‚‰èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚

        Args:
            X: ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            n_splits: åˆ†å‰²æ•°ï¼ˆNoneã®å ´åˆã¯ml_configã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        pass

    @abstractmethod
    def get_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—

        Args:
            X: ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ

        Returns:
            ç‰¹å¾´é‡é‡è¦åº¦ã®è¾æ›¸
        """
        pass

    def load_unified_scores(
        self, json_path: str = "../../feature_importance_analysis.json"
    ) -> Dict:
        """
        çµ±åˆã‚¹ã‚³ã‚¢ã‚’JSONã‹ã‚‰èª­ã¿è¾¼ã¿

        Args:
            json_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            çµ±åˆã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿
        """
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            logger.info(f"çµ±åˆã‚¹ã‚³ã‚¢èª­ã¿è¾¼ã¿å®Œäº†: {json_path}")
            return data.get("feature_importance", {})
        except Exception as e:
            logger.warning(f"çµ±åˆã‚¹ã‚³ã‚¢èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def select_features_by_score(
        self, features: List[str], unified_scores: Dict, bottom_pct: float
    ) -> Tuple[List[str], List[str]]:
        """
        çµ±åˆã‚¹ã‚³ã‚¢ä¸‹ä½N%ã®ç‰¹å¾´é‡ã‚’é¸æŠ

        Args:
            features: å…¨ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
            unified_scores: çµ±åˆã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿
            bottom_pct: ä¸‹ä½ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« (0.1 = 10%)

        Returns:
            (å‰Šé™¤ã™ã‚‹ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ, ä¿æŒã™ã‚‹ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ)
        """
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        scored_features = []
        for feat in features:
            if feat in unified_scores:
                score = unified_scores[feat].get("combined_score", 0.0)
                scored_features.append((feat, score))
            else:
                # ã‚¹ã‚³ã‚¢ãŒãªã„å ´åˆã¯ä¿æŒ
                scored_features.append((feat, 1.0))

        sorted_features = sorted(scored_features, key=lambda x: x[1])

        # ä¸‹ä½N%ã‚’è¨ˆç®—
        n_remove = max(1, int(len(sorted_features) * bottom_pct))

        to_remove = [feat for feat, _ in sorted_features[:n_remove]]
        to_keep = [feat for feat, _ in sorted_features[n_remove:]]

        return to_remove, to_keep

    def run_scenario(
        self,
        scenario_name: str,
        X: pd.DataFrame,
        y: pd.Series,
        features_to_use: List[str],
        removed_features: List[str] = None,
    ) -> Dict:
        """
        1ã¤ã®ã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ

        Args:
            scenario_name: ã‚·ãƒŠãƒªã‚ªå
            X: å…¨ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
            features_to_use: ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
            removed_features: å‰Šé™¤ã—ãŸç‰¹å¾´é‡ãƒªã‚¹ãƒˆ

        Returns:
            ã‚·ãƒŠãƒªã‚ªçµæœ
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"[{self.model_name}] ã‚·ãƒŠãƒªã‚ª: {scenario_name}")
        logger.info(f"{'='*80}")
        logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(features_to_use)}")

        if removed_features:
            logger.info(f"å‰Šé™¤ç‰¹å¾´é‡æ•°: {len(removed_features)}")
            features_preview = ", ".join(removed_features[:10])
            suffix = "..." if len(removed_features) > 10 else ""
            logger.info(f"å‰Šé™¤ç‰¹å¾´é‡: {features_preview}{suffix}")

        # ç‰¹å¾´é‡é¸æŠ
        X_selected = X[features_to_use]

        # NaNé™¤å»
        valid_idx = ~(X_selected.isna().any(axis=1) | y.isna())
        X_clean = X_selected[valid_idx]
        y_clean = y[valid_idx]

        logger.info(f"æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_clean)}è¡Œ")

        if len(X_clean) < 100:
            logger.warning("ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³")
            return {}

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
        cv_results = self.evaluate_model_cv(X_clean, y_clean)

        if not cv_results:
            return {}

        # ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
        feature_importance = self.get_feature_importance(X_clean, y_clean)
        top_features = sorted(
            feature_importance.items(), key=lambda x: x[1], reverse=True
        )[:10]

        result = {
            "n_features": len(features_to_use),
            "removed_features": removed_features or [],
            **cv_results,
            "feature_importance_top10": [
                {"feature": feat, "importance": float(imp)}
                for feat, imp in top_features
            ],
        }

        logger.info(
            f"CV Accuracy: {cv_results['cv_accuracy']:.4f} "
            f"(Â±{cv_results['cv_accuracy_std']:.4f})"
        )
        logger.info(
            f"CV F1 (Weighted): {cv_results['cv_f1_weighted']:.4f} "
            f"(Â±{cv_results['cv_f1_weighted_std']:.4f})"
        )
        logger.info(
            f"CV Balanced Accuracy: {cv_results['cv_balanced_accuracy']:.4f} "
            f"(Â±{cv_results['cv_balanced_accuracy_std']:.4f})"
        )
        logger.info(f"å­¦ç¿’æ™‚é–“: {cv_results['train_time_sec']:.2f}ç§’")

        return result

    def generate_recommendation(self, results: Dict) -> Dict:
        """
        æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆï¼ˆåˆ†é¡å•é¡Œç”¨ï¼‰

        Args:
            results: å„ã‚·ãƒŠãƒªã‚ªã®çµæœ

        Returns:
            æ¨å¥¨äº‹é …è¾æ›¸
        """
        if not results.get("baseline"):
            return {
                "message": "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ãŒå¤±æ•—ã—ãŸãŸã‚ã€æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“"
            }

        # è¨±å®¹ç¯„å›²ï¼ˆAccuracyå¤‰åŒ– < 2%ï¼‰ã§æœ€ã‚‚å¤šãå‰Šæ¸›ã§ãã‚‹ã‚·ãƒŠãƒªã‚ªã‚’æ¢ã™
        acceptable_scenarios = []

        for key, result in results.items():
            if key == "baseline" or not result:
                continue

            change_pct = result.get("performance_change_pct", 100)
            if abs(change_pct) < 2.0:  # 2%ä»¥å†…ã®å¤‰åŒ–ï¼ˆåˆ†é¡å•é¡Œã§ã¯å°‘ã—ç·©ã‚ã«ï¼‰
                acceptable_scenarios.append(
                    {
                        "scenario": key,
                        "n_features": result["n_features"],
                        "removed_count": len(result["removed_features"]),
                        "change_pct": change_pct,
                        "removed_features": result["removed_features"],
                    }
                )

        if acceptable_scenarios:
            # å‰Šæ¸›æ•°ãŒæœ€å¤§ã®ã‚·ãƒŠãƒªã‚ªã‚’é¸æŠ
            best = max(acceptable_scenarios, key=lambda x: x["removed_count"])
            return {
                "recommended_scenario": best["scenario"],
                "recommended_features_to_remove": best["removed_features"],
                "features_count_after": best["n_features"],
                "features_removed_count": best["removed_count"],
                "performance_change_pct": best["change_pct"],
                "message": f"æ€§èƒ½åŠ£åŒ–ãŒ2%æœªæº€ã§{best['removed_count']}å€‹ã®ç‰¹å¾´é‡å‰Šæ¸›ãŒå¯èƒ½ã§ã™",
            }
        else:
            return {
                "recommended_scenario": "baseline",
                "message": "æ€§èƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰å‰Šæ¸›ã§ãã‚‹ç‰¹å¾´é‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
            }


class OptunaEnabledEvaluator(BaseFeatureEvaluator):
    """Optunaæœ€é©åŒ–ã‚’æœ‰åŠ¹ã«ã—ãŸè©•ä¾¡å™¨åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        model_name: str,
        enable_optuna: bool = False,
        n_trials: int = 50,
        timeout: Optional[int] = None,
    ):
        """
        åˆæœŸåŒ–

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            enable_optuna: Optunaæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–
            n_trials: Optunaã®è©¦è¡Œå›æ•°
            timeout: Optunaæœ€é©åŒ–ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
        """
        super().__init__(model_name)
        self.enable_optuna = enable_optuna
        self.n_trials = n_trials
        self.timeout = timeout
        self.best_params: Optional[Dict] = None
        self.optimization_history: List[Dict] = []

    @abstractmethod
    def get_parameter_space(self) -> Dict[str, ParameterSpace]:
        """
        ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å–å¾—

        Returns:
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®è¾æ›¸
        """
        pass

    def optimize_hyperparameters(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        parameter_space: Dict[str, ParameterSpace],
    ) -> Dict[str, Any]:
        """
        Optunaã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆåˆ†é¡å•é¡Œç”¨ï¼‰

        Args:
            X_train: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
            y_train: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
            parameter_space: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“

        Returns:
            æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        logger.info(
            f"ğŸš€ [{self.model_name}] Optunaæœ€é©åŒ–ã‚’é–‹å§‹: è©¦è¡Œå›æ•°={self.n_trials}"
        )

        optimizer = OptunaOptimizer()

        def objective_function(params: Dict[str, Any]) -> float:
            """æœ€é©åŒ–ç›®çš„é–¢æ•°ï¼ˆAccuracyã‚’æœ€å¤§åŒ–ï¼‰"""
            try:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
                temp_result = self._evaluate_with_params(X_train, y_train, params)
                # Accuracyã‚’æœ€å¤§åŒ–
                return temp_result.get("cv_accuracy", 0.0)
            except Exception as e:
                logger.warning(f"ç›®çš„é–¢æ•°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                return 0.0

        try:
            result = optimizer.optimize(
                objective_function=objective_function,
                parameter_space=parameter_space,
                n_calls=self.n_trials,
            )

            self.best_params = result.best_params
            self.optimization_history = [
                {"trial": i + 1, "value": trial.value, "params": trial.params}
                for i, trial in enumerate(result.study.trials)
                if trial.value is not None
            ]

            logger.info(
                f"âœ… [{self.model_name}] æœ€é©åŒ–å®Œäº†: ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢(Accuracy)={result.best_score:.4f}"
            )
            logger.info(f"âš™ï¸  æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {result.best_params}")

            return result.best_params

        except Exception as e:
            logger.error(f"[{self.model_name}] æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    @abstractmethod
    def _evaluate_with_params(
        self, X: pd.DataFrame, y: pd.Series, params: Dict[str, Any]
    ) -> Dict[str, float]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è©•ä¾¡

        Args:
            X: ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            params: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        pass

    def evaluate_model_cv_with_optuna(
        self, X: pd.DataFrame, y: pd.Series, n_splits: Optional[int] = None
    ) -> Dict[str, float]:
        """
        Optunaæœ€é©åŒ–+TimeSeriesSplitè©•ä¾¡

        Args:
            X: ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            n_splits: åˆ†å‰²æ•°

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        if n_splits is None:
            n_splits = unified_config.ml.training.cv_folds

        logger.info(f"[{self.model_name}] Optunaæœ€é©åŒ–+CVè©•ä¾¡é–‹å§‹")

        # TimeSeriesSplitã§å­¦ç¿’/æ¤œè¨¼ã«åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=n_splits)
        train_idx, _ = list(tscv.split(X))[-1]  # æœ€å¾Œã®åˆ†å‰²ã‚’ä½¿ç”¨

        X_train = X.iloc[train_idx]
        y_train = y.iloc[train_idx]

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å–å¾—
        parameter_space = self.get_parameter_space()

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        best_params = self.optimize_hyperparameters(X_train, y_train, parameter_space)

        if not best_params:
            logger.warning("æœ€é©åŒ–å¤±æ•—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è©•ä¾¡")
            return self.evaluate_model_cv(X, y, n_splits)

        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦CVè©•ä¾¡
        result = self._evaluate_with_params(X, y, best_params)

        # Optunaæƒ…å ±ã‚’è¿½åŠ 
        result["optuna_enabled"] = True
        result["best_params"] = best_params
        result["n_trials"] = self.n_trials
        result["optimization_history"] = self.optimization_history[:10]  # ä¸Šä½10ä»¶ã®ã¿

        return result


class LightGBMEvaluator(OptunaEnabledEvaluator):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã§ã®ç‰¹å¾´é‡æ€§èƒ½è©•ä¾¡ã‚¯ãƒ©ã‚¹ï¼ˆåˆ†é¡å•é¡Œãƒ»Optunaå¯¾å¿œï¼‰"""

    def __init__(
        self,
        enable_optuna: bool = False,
        n_trials: int = 50,
        timeout: Optional[int] = None,
    ):
        """åˆæœŸåŒ–"""
        super().__init__("LightGBM", enable_optuna, n_trials, timeout)

        # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
        self.model_params = {
            "objective": "multiclass",
            "num_class": 3,
            "metric": "multi_logloss",
            "boosting_type": "gbdt",
            "num_leaves": 31,
            "learning_rate": 0.05,
            "feature_fraction": 0.9,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
            "verbose": -1,
            "random_state": 42,
        }

    def get_parameter_space(self) -> Dict[str, ParameterSpace]:
        """LightGBMç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å–å¾—"""
        return EnsembleParameterSpace.get_lightgbm_parameter_space()

    def _evaluate_with_params(
        self, X: pd.DataFrame, y: pd.Series, params: Dict[str, Any]
    ) -> Dict[str, float]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§LightGBMè©•ä¾¡ï¼ˆåˆ†é¡å•é¡Œï¼‰

        Args:
            X: ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
            params: LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        import lightgbm as lgb

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã®å¤‰æ›ï¼ˆlgb_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤ï¼‰
        lgb_params = {
            k.replace("lgb_", ""): v for k, v in params.items() if k.startswith("lgb_")
        }

        # ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸
        model_params = {**self.model_params, **lgb_params}

        # TimeSeriesSplitã§CV
        n_splits = unified_config.ml.training.cv_folds
        tscv = TimeSeriesSplit(n_splits=n_splits)

        accuracy_scores = []
        balanced_accuracy_scores = []
        f1_macro_scores = []
        f1_weighted_scores = []
        precision_scores = []
        recall_scores = []
        train_times = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
            try:
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

                start_time = time.time()

                train_data = lgb.Dataset(X_train, label=y_train)
                model = lgb.train(
                    model_params,
                    train_data,
                    num_boost_round=100,
                    valid_sets=[train_data],
                    callbacks=[
                        lgb.early_stopping(stopping_rounds=10),
                        lgb.log_evaluation(0),
                    ],
                )

                train_time = time.time() - start_time
                train_times.append(train_time)

                # ç¢ºç‡äºˆæ¸¬ã‚’å–å¾—ã—ã¦argmaxã§ã‚¯ãƒ©ã‚¹äºˆæ¸¬
                y_pred_proba = model.predict(X_test)
                y_pred = np.argmax(y_pred_proba, axis=1)

                # åˆ†é¡æŒ‡æ¨™ã‚’è¨ˆç®—
                accuracy = accuracy_score(y_test, y_pred)
                balanced_acc = balanced_accuracy_score(y_test, y_pred)
                f1_macro = f1_score(y_test, y_pred, average="macro", zero_division=0)
                f1_weighted = f1_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )
                precision = precision_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )
                recall = recall_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )

                accuracy_scores.append(accuracy)
                balanced_accuracy_scores.append(balanced_acc)
                f1_macro_scores.append(f1_macro)
                f1_weighted_scores.append(f1_weighted)
                precision_scores.append(precision)
                recall_scores.append(recall)

            except Exception as e:
                logger.warning(f"Fold {fold}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        if not accuracy_scores:
            return {}

        return {
            "cv_accuracy": float(np.mean(accuracy_scores)),
            "cv_accuracy_std": float(np.std(accuracy_scores)),
            "cv_balanced_accuracy": float(np.mean(balanced_accuracy_scores)),
            "cv_balanced_accuracy_std": float(np.std(balanced_accuracy_scores)),
            "cv_f1_macro": float(np.mean(f1_macro_scores)),
            "cv_f1_macro_std": float(np.std(f1_macro_scores)),
            "cv_f1_weighted": float(np.mean(f1_weighted_scores)),
            "cv_f1_weighted_std": float(np.std(f1_weighted_scores)),
            "cv_precision": float(np.mean(precision_scores)),
            "cv_precision_std": float(np.std(precision_scores)),
            "cv_recall": float(np.mean(recall_scores)),
            "cv_recall_std": float(np.std(recall_scores)),
            "train_time_sec": float(np.mean(train_times)),
        }

    def evaluate_model_cv(
        self, X: pd.DataFrame, y: pd.Series, n_splits: Optional[int] = None
    ) -> Dict[str, float]:
        """
        TimeSeriesSplitã§ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ï¼ˆåˆ†é¡å•é¡Œãƒ»Optunaå¯¾å¿œï¼‰

        Args:
            X: ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
            n_splits: åˆ†å‰²æ•°ï¼ˆNoneã®å ´åˆã¯ml_configã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        # Optunaæœ€é©åŒ–ãŒæœ‰åŠ¹ãªå ´åˆ
        if self.enable_optuna:
            return self.evaluate_model_cv_with_optuna(X, y, n_splits)

        # å¾“æ¥ã®å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©•ä¾¡
        import lightgbm as lgb

        if n_splits is None:
            n_splits = unified_config.ml.training.cv_folds

        logger.info(f"TimeSeriesSplitä½¿ç”¨: n_splits={n_splits}")
        tscv = TimeSeriesSplit(n_splits=n_splits)

        accuracy_scores = []
        balanced_accuracy_scores = []
        f1_macro_scores = []
        f1_weighted_scores = []
        precision_scores = []
        recall_scores = []
        train_times = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
            try:
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

                # å­¦ç¿’æ™‚é–“è¨ˆæ¸¬
                start_time = time.time()

                # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
                train_data = lgb.Dataset(X_train, label=y_train)

                # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                model = lgb.train(
                    self.model_params,
                    train_data,
                    num_boost_round=100,
                    valid_sets=[train_data],
                    callbacks=[
                        lgb.early_stopping(stopping_rounds=10),
                        lgb.log_evaluation(0),
                    ],
                )

                train_time = time.time() - start_time
                train_times.append(train_time)

                # äºˆæ¸¬ï¼ˆç¢ºç‡â†’ã‚¯ãƒ©ã‚¹ï¼‰
                y_pred_proba = model.predict(X_test)
                y_pred = np.argmax(y_pred_proba, axis=1)

                # åˆ†é¡æŒ‡æ¨™ã‚’è¨ˆç®—
                accuracy = accuracy_score(y_test, y_pred)
                balanced_acc = balanced_accuracy_score(y_test, y_pred)
                f1_macro = f1_score(y_test, y_pred, average="macro", zero_division=0)
                f1_weighted = f1_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )
                precision = precision_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )
                recall = recall_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )

                accuracy_scores.append(accuracy)
                balanced_accuracy_scores.append(balanced_acc)
                f1_macro_scores.append(f1_macro)
                f1_weighted_scores.append(f1_weighted)
                precision_scores.append(precision)
                recall_scores.append(recall)

                logger.info(
                    f"Fold {fold}: Accuracy={accuracy:.4f}, "
                    f"F1(Weighted)={f1_weighted:.4f}, "
                    f"Balanced Acc={balanced_acc:.4f}, "
                    f"Time={train_time:.2f}s"
                )

            except Exception as e:
                logger.warning(f"Fold {fold}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        if not accuracy_scores:
            return {}

        return {
            "cv_accuracy": float(np.mean(accuracy_scores)),
            "cv_accuracy_std": float(np.std(accuracy_scores)),
            "cv_balanced_accuracy": float(np.mean(balanced_accuracy_scores)),
            "cv_balanced_accuracy_std": float(np.std(balanced_accuracy_scores)),
            "cv_f1_macro": float(np.mean(f1_macro_scores)),
            "cv_f1_macro_std": float(np.std(f1_macro_scores)),
            "cv_f1_weighted": float(np.mean(f1_weighted_scores)),
            "cv_f1_weighted_std": float(np.std(f1_weighted_scores)),
            "cv_precision": float(np.mean(precision_scores)),
            "cv_precision_std": float(np.std(precision_scores)),
            "cv_recall": float(np.mean(recall_scores)),
            "cv_recall_std": float(np.std(recall_scores)),
            "train_time_sec": float(np.mean(train_times)),
        }

    def get_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """LightGBMã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—"""
        import lightgbm as lgb

        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            train_data = lgb.Dataset(X, label=y)

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            model = lgb.train(
                self.model_params,
                train_data,
                num_boost_round=100,
                valid_sets=[train_data],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=10),
                    lgb.log_evaluation(0),
                ],
            )

            # é‡è¦åº¦å–å¾—ï¼ˆgainï¼‰
            importance = model.feature_importance(importance_type="gain")

            # æ­£è¦åŒ–
            if importance.sum() > 0:
                importance = importance / importance.sum()

            return dict(zip(X.columns, importance))

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}


class XGBoostEvaluator(OptunaEnabledEvaluator):
    """XGBoostãƒ¢ãƒ‡ãƒ«ã§ã®ç‰¹å¾´é‡æ€§èƒ½è©•ä¾¡ã‚¯ãƒ©ã‚¹ï¼ˆåˆ†é¡å•é¡Œãƒ»Optunaå¯¾å¿œï¼‰"""

    def __init__(
        self,
        enable_optuna: bool = False,
        n_trials: int = 50,
        timeout: Optional[int] = None,
    ):
        """åˆæœŸåŒ–"""
        super().__init__("XGBoost", enable_optuna, n_trials, timeout)

        # XGBoostãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
        self.model_params = {
            "objective": "multi:softprob",
            "num_class": 3,
            "eval_metric": "mlogloss",
            "booster": "gbtree",
            "max_depth": 6,
            "learning_rate": 0.05,
            "subsample": 0.8,
            "colsample_bytree": 0.9,
            "min_child_weight": 1,
            "random_state": 42,
            "verbosity": 0,
        }

    def get_parameter_space(self) -> Dict[str, ParameterSpace]:
        """XGBoostç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å–å¾—"""
        return EnsembleParameterSpace.get_xgboost_parameter_space()

    def _evaluate_with_params(
        self, X: pd.DataFrame, y: pd.Series, params: Dict[str, Any]
    ) -> Dict[str, float]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§XGBoostè©•ä¾¡ï¼ˆåˆ†é¡å•é¡Œï¼‰

        Args:
            X: ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
            params: XGBoostãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        import xgboost as xgb

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã®å¤‰æ›ï¼ˆxgb_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤ï¼‰
        xgb_params = {
            k.replace("xgb_", ""): v for k, v in params.items() if k.startswith("xgb_")
        }

        # ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸
        model_params = {**self.model_params, **xgb_params}

        # TimeSeriesSplitã§CV
        n_splits = unified_config.ml.training.cv_folds
        tscv = TimeSeriesSplit(n_splits=n_splits)

        accuracy_scores = []
        balanced_accuracy_scores = []
        f1_macro_scores = []
        f1_weighted_scores = []
        precision_scores = []
        recall_scores = []
        train_times = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
            try:
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

                start_time = time.time()

                dtrain = xgb.DMatrix(X_train, label=y_train)
                dtest = xgb.DMatrix(X_test, label=y_test)

                model = xgb.train(
                    model_params,
                    dtrain,
                    num_boost_round=100,
                    evals=[(dtrain, "train")],
                    early_stopping_rounds=10,
                    verbose_eval=False,
                )

                train_time = time.time() - start_time
                train_times.append(train_time)

                # ç¢ºç‡äºˆæ¸¬ã‚’å–å¾—ã—ã¦argmaxã§ã‚¯ãƒ©ã‚¹äºˆæ¸¬
                y_pred_proba = model.predict(dtest)
                y_pred = np.argmax(y_pred_proba, axis=1)

                # åˆ†é¡æŒ‡æ¨™ã‚’è¨ˆç®—
                accuracy = accuracy_score(y_test, y_pred)
                balanced_acc = balanced_accuracy_score(y_test, y_pred)
                f1_macro = f1_score(y_test, y_pred, average="macro", zero_division=0)
                f1_weighted = f1_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )
                precision = precision_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )
                recall = recall_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )

                accuracy_scores.append(accuracy)
                balanced_accuracy_scores.append(balanced_acc)
                f1_macro_scores.append(f1_macro)
                f1_weighted_scores.append(f1_weighted)
                precision_scores.append(precision)
                recall_scores.append(recall)

            except Exception as e:
                logger.warning(f"Fold {fold}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        if not accuracy_scores:
            return {}

        return {
            "cv_accuracy": float(np.mean(accuracy_scores)),
            "cv_accuracy_std": float(np.std(accuracy_scores)),
            "cv_balanced_accuracy": float(np.mean(balanced_accuracy_scores)),
            "cv_balanced_accuracy_std": float(np.std(balanced_accuracy_scores)),
            "cv_f1_macro": float(np.mean(f1_macro_scores)),
            "cv_f1_macro_std": float(np.std(f1_macro_scores)),
            "cv_f1_weighted": float(np.mean(f1_weighted_scores)),
            "cv_f1_weighted_std": float(np.std(f1_weighted_scores)),
            "cv_precision": float(np.mean(precision_scores)),
            "cv_precision_std": float(np.std(precision_scores)),
            "cv_recall": float(np.mean(recall_scores)),
            "cv_recall_std": float(np.std(recall_scores)),
            "train_time_sec": float(np.mean(train_times)),
        }

    def evaluate_model_cv(
        self, X: pd.DataFrame, y: pd.Series, n_splits: Optional[int] = None
    ) -> Dict[str, float]:
        """
        TimeSeriesSplitã§ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ï¼ˆåˆ†é¡å•é¡Œãƒ»Optunaå¯¾å¿œï¼‰

        Args:
            X: ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
            n_splits: åˆ†å‰²æ•°ï¼ˆNoneã®å ´åˆã¯ml_configã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        # Optunaæœ€é©åŒ–ãŒæœ‰åŠ¹ãªå ´åˆ
        if self.enable_optuna:
            return self.evaluate_model_cv_with_optuna(X, y, n_splits)

        # å¾“æ¥ã®å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©•ä¾¡
        import xgboost as xgb

        if n_splits is None:
            n_splits = unified_config.ml.training.cv_folds

        logger.info(f"TimeSeriesSplitä½¿ç”¨: n_splits={n_splits}")
        tscv = TimeSeriesSplit(n_splits=n_splits)

        accuracy_scores = []
        balanced_accuracy_scores = []
        f1_macro_scores = []
        f1_weighted_scores = []
        precision_scores = []
        recall_scores = []
        train_times = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
            try:
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

                # å­¦ç¿’æ™‚é–“è¨ˆæ¸¬
                start_time = time.time()

                # XGBoostãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
                dtrain = xgb.DMatrix(X_train, label=y_train)
                dtest = xgb.DMatrix(X_test, label=y_test)

                # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                model = xgb.train(
                    self.model_params,
                    dtrain,
                    num_boost_round=100,
                    evals=[(dtrain, "train")],
                    early_stopping_rounds=10,
                    verbose_eval=False,
                )

                train_time = time.time() - start_time
                train_times.append(train_time)

                # äºˆæ¸¬ï¼ˆç¢ºç‡â†’ã‚¯ãƒ©ã‚¹ï¼‰
                y_pred_proba = model.predict(dtest)
                y_pred = np.argmax(y_pred_proba, axis=1)

                # åˆ†é¡æŒ‡æ¨™ã‚’è¨ˆç®—
                accuracy = accuracy_score(y_test, y_pred)
                balanced_acc = balanced_accuracy_score(y_test, y_pred)
                f1_macro = f1_score(y_test, y_pred, average="macro", zero_division=0)
                f1_weighted = f1_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )
                precision = precision_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )
                recall = recall_score(
                    y_test, y_pred, average="weighted", zero_division=0
                )

                accuracy_scores.append(accuracy)
                balanced_accuracy_scores.append(balanced_acc)
                f1_macro_scores.append(f1_macro)
                f1_weighted_scores.append(f1_weighted)
                precision_scores.append(precision)
                recall_scores.append(recall)

                logger.info(
                    f"Fold {fold}: Accuracy={accuracy:.4f}, "
                    f"F1(Weighted)={f1_weighted:.4f}, "
                    f"Balanced Acc={balanced_acc:.4f}, "
                    f"Time={train_time:.2f}s"
                )

            except Exception as e:
                logger.warning(f"Fold {fold}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        if not accuracy_scores:
            return {}

        return {
            "cv_accuracy": float(np.mean(accuracy_scores)),
            "cv_accuracy_std": float(np.std(accuracy_scores)),
            "cv_balanced_accuracy": float(np.mean(balanced_accuracy_scores)),
            "cv_balanced_accuracy_std": float(np.std(balanced_accuracy_scores)),
            "cv_f1_macro": float(np.mean(f1_macro_scores)),
            "cv_f1_macro_std": float(np.std(f1_macro_scores)),
            "cv_f1_weighted": float(np.mean(f1_weighted_scores)),
            "cv_f1_weighted_std": float(np.std(f1_weighted_scores)),
            "cv_precision": float(np.mean(precision_scores)),
            "cv_precision_std": float(np.std(precision_scores)),
            "cv_recall": float(np.mean(recall_scores)),
            "cv_recall_std": float(np.std(recall_scores)),
            "train_time_sec": float(np.mean(train_times)),
        }

    def get_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """XGBoostã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—"""
        import xgboost as xgb

        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            dtrain = xgb.DMatrix(X, label=y)

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            model = xgb.train(
                self.model_params,
                dtrain,
                num_boost_round=100,
                evals=[(dtrain, "train")],
                early_stopping_rounds=10,
                verbose_eval=False,
            )

            # é‡è¦åº¦å–å¾—ï¼ˆgainï¼‰
            importance_dict = model.get_score(importance_type="gain")

            # å…¨ç‰¹å¾´é‡ã«å¯¾ã—ã¦é‡è¦åº¦ã‚’è¨­å®šï¼ˆæœªä½¿ç”¨ã¯0ï¼‰
            result = {col: 0.0 for col in X.columns}
            result.update(importance_dict)

            # æ­£è¦åŒ–
            total = sum(result.values())
            if total > 0:
                result = {k: v / total for k, v in result.items()}

            return result

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}


class MultiModelFeatureEvaluator:
    """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ã®ç‰¹å¾´é‡è©•ä¾¡ã‚’çµ±åˆç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        models: List[str],
        enable_optuna: bool = False,
        n_trials: int = 50,
        timeout: Optional[int] = None,
    ):
        """
        åˆæœŸåŒ–

        Args:
            models: è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ ['lightgbm', 'xgboost']
            enable_optuna: Optunaæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–
            n_trials: Optunaã®è©¦è¡Œå›æ•°
            timeout: Optunaæœ€é©åŒ–ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
        """
        self.models = models
        self.evaluators = {}
        self.all_results = {}
        self.enable_optuna = enable_optuna

        # è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–
        if "lightgbm" in models:
            self.evaluators["lightgbm"] = LightGBMEvaluator(
                enable_optuna, n_trials, timeout
            )
        if "xgboost" in models:
            self.evaluators["xgboost"] = XGBoostEvaluator(
                enable_optuna, n_trials, timeout
            )

    def run_evaluation(self, symbol: str = "BTC/USDT:USDT", limit: int = 2000) -> Dict:
        """
        å…¨ãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡ã‚’å®Ÿè¡Œ

        Args:
            symbol: åˆ†æå¯¾è±¡ã‚·ãƒ³ãƒœãƒ«
            limit: ãƒ‡ãƒ¼ã‚¿å–å¾—ä»¶æ•°

        Returns:
            å…¨ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡çµæœ
        """
        start_time = time.time()
        logger.info("=" * 80)
        logger.info("å…¨ãƒ¢ãƒ‡ãƒ«ç‰¹å¾´é‡æ€§èƒ½è©•ä¾¡é–‹å§‹")
        logger.info(f"è©•ä¾¡ãƒ¢ãƒ‡ãƒ«: {', '.join([m.upper() for m in self.models])}")
        logger.info("=" * 80)

        # å…±é€šãƒ‡ãƒ¼ã‚¿ã‚’1å›ã ã‘å–å¾—
        logger.info("å…±é€šãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
        first_evaluator = list(self.evaluators.values())[0]
        ohlcv_df, fr_df, oi_df = first_evaluator.fetch_data(symbol, limit)

        if ohlcv_df.empty:
            logger.error("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return {}

        # ç‰¹å¾´é‡è¨ˆç®—ï¼ˆ1å›ã®ã¿ï¼‰
        features_df = first_evaluator.calculate_features(ohlcv_df, fr_df, oi_df)
        target = first_evaluator.create_target(features_df, periods=1)

        # closeã‚’é™¤å¤–
        feature_cols = [col for col in features_df.columns if col != "close"]
        X = features_df[feature_cols]

        # NaNé™¤å»
        combined_df = pd.concat([X, target.rename("target")], axis=1).dropna()
        X = combined_df[feature_cols]
        y = combined_df["target"]

        logger.info(f"\nåˆ†æå¯¾è±¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}è¡Œ")
        logger.info(f"å…¨ç‰¹å¾´é‡æ•°: {len(X.columns)}å€‹")

        # çµ±åˆã‚¹ã‚³ã‚¢èª­ã¿è¾¼ã¿
        unified_scores = first_evaluator.load_unified_scores()

        # å„ãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡å®Ÿè¡Œ
        for model_name, evaluator in self.evaluators.items():
            logger.info(f"\n{'='*80}")
            logger.info(f"{model_name.upper()}ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹")
            logger.info(f"{'='*80}")

            try:
                model_results = self._run_model_scenarios(
                    evaluator, X, y, unified_scores
                )
                self.all_results[model_name] = {
                    "evaluation_date": datetime.now().isoformat(),
                    "model_name": model_name,
                    "data_samples": len(X),
                    "symbol": symbol,
                    "target": "3class_classification",
                    "model_params": evaluator.model_params,
                    "scenarios": model_results["scenarios"],
                    "recommendation": model_results["recommendation"],
                }

                # å€‹åˆ¥çµæœã‚’ä¿å­˜
                self._save_individual_results(model_name, self.all_results[model_name])

            except Exception as e:
                logger.error(f"{model_name}è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback

                traceback.print_exc()
                continue

        # çµ±åˆçµæœã‚’ä¿å­˜
        self._save_integrated_results()

        # çµ±åˆã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
        self._print_integrated_summary()

        elapsed_time = time.time() - start_time
        logger.info(f"\nå…¨è©•ä¾¡å®Œäº†ï¼ˆå‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’ï¼‰")

        return self.all_results

    def _run_model_scenarios(
        self,
        evaluator: BaseFeatureEvaluator,
        X: pd.DataFrame,
        y: pd.Series,
        unified_scores: Dict,
    ) -> Dict:
        """
        1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§å…¨ã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ

        Args:
            evaluator: è©•ä¾¡å™¨
            X: ç‰¹å¾´é‡
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            unified_scores: çµ±åˆã‚¹ã‚³ã‚¢

        Returns:
            ã‚·ãƒŠãƒªã‚ªçµæœ
        """
        all_features = list(X.columns)
        results = {}

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå…¨ç‰¹å¾´é‡ï¼‰
        results["baseline"] = evaluator.run_scenario(
            "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (93ç‰¹å¾´é‡ã™ã¹ã¦)", X, y, all_features
        )

        # ã‚·ãƒŠãƒªã‚ª2: ä¸‹ä½10%å‰Šé™¤
        to_remove_10, to_keep_10 = evaluator.select_features_by_score(
            all_features, unified_scores, 0.10
        )
        results["scenario_remove_10pct"] = evaluator.run_scenario(
            "ã‚·ãƒŠãƒªã‚ª2: çµ±åˆã‚¹ã‚³ã‚¢ä¸‹ä½10%å‰Šé™¤", X, y, to_keep_10, to_remove_10
        )

        # ã‚·ãƒŠãƒªã‚ª3: ä¸‹ä½20%å‰Šé™¤
        to_remove_20, to_keep_20 = evaluator.select_features_by_score(
            all_features, unified_scores, 0.20
        )
        results["scenario_remove_20pct"] = evaluator.run_scenario(
            "ã‚·ãƒŠãƒªã‚ª3: çµ±åˆã‚¹ã‚³ã‚¢ä¸‹ä½20%å‰Šé™¤", X, y, to_keep_20, to_remove_20
        )

        # ã‚·ãƒŠãƒªã‚ª4: ä¸‹ä½30%å‰Šé™¤
        to_remove_30, to_keep_30 = evaluator.select_features_by_score(
            all_features, unified_scores, 0.30
        )
        results["scenario_remove_30pct"] = evaluator.run_scenario(
            "ã‚·ãƒŠãƒªã‚ª4: çµ±åˆã‚¹ã‚³ã‚¢ä¸‹ä½30%å‰Šé™¤", X, y, to_keep_30, to_remove_30
        )

        # ã‚·ãƒŠãƒªã‚ª5: ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ç‰¹å¾´é‡é‡è¦åº¦ãƒ™ãƒ¼ã‚¹
        if results["baseline"]:
            model_importance = evaluator.get_feature_importance(X, y)
            sorted_importance = sorted(model_importance.items(), key=lambda x: x[1])
            n_remove = max(1, int(len(sorted_importance) * 0.20))
            to_remove_model = [feat for feat, _ in sorted_importance[:n_remove]]
            to_keep_model = [
                feat for feat in all_features if feat not in to_remove_model
            ]

            results[f"scenario_{evaluator.model_name.lower()}_importance"] = (
                evaluator.run_scenario(
                    f"ã‚·ãƒŠãƒªã‚ª5: {evaluator.model_name}é‡è¦åº¦ä¸‹ä½20%å‰Šé™¤",
                    X,
                    y,
                    to_keep_model,
                    to_remove_model,
                )
            )

        # æ€§èƒ½å¤‰åŒ–ã‚’è¨ˆç®—ï¼ˆAccuracyãƒ™ãƒ¼ã‚¹ï¼‰
        if results["baseline"]:
            baseline_accuracy = results["baseline"]["cv_accuracy"]
            for key in results:
                if key != "baseline" and results[key]:
                    scenario_accuracy = results[key]["cv_accuracy"]
                    change_pct = (
                        (scenario_accuracy - baseline_accuracy) / baseline_accuracy
                    ) * 100
                    results[key]["performance_change_pct"] = float(change_pct)

        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendation = evaluator.generate_recommendation(results)

        return {"scenarios": results, "recommendation": recommendation}

    def _save_individual_results(self, model_name: str, results: Dict):
        """
        å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’ä¿å­˜

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            results: è©•ä¾¡çµæœ
        """
        try:
            # results/feature_analysisãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ï¼ˆbackendãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹ï¼‰
            output_dir = Path(__file__).parent.parent.parent / "results" / "feature_analysis"
            output_dir.mkdir(parents=True, exist_ok=True)

            # JSONä¿å­˜
            json_path = output_dir / f"{model_name}_feature_performance_evaluation.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"[{model_name.upper()}] JSONä¿å­˜å®Œäº†: {json_path}")

            # CSVä¿å­˜
            csv_path = output_dir / f"{model_name}_performance_comparison.csv"
            scenarios_data = []
            for key, scenario in results.get("scenarios", {}).items():
                if scenario:
                    row = {
                        "scenario": key,
                        "n_features": scenario.get("n_features"),
                        "cv_accuracy": scenario.get("cv_accuracy"),
                        "cv_f1_weighted": scenario.get("cv_f1_weighted"),
                        "cv_balanced_accuracy": scenario.get("cv_balanced_accuracy"),
                        "train_time_sec": scenario.get("train_time_sec"),
                        "performance_change_pct": scenario.get(
                            "performance_change_pct", 0.0
                        ),
                        "removed_count": len(scenario.get("removed_features", [])),
                    }
                    scenarios_data.append(row)

            if scenarios_data:
                df = pd.DataFrame(scenarios_data)
                df.to_csv(csv_path, index=False)
                logger.info(f"[{model_name.upper()}] CSVä¿å­˜å®Œäº†: {csv_path}")

        except Exception as e:
            logger.error(f"[{model_name}] çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _save_integrated_results(self):
        """çµ±åˆçµæœã‚’ä¿å­˜"""
        try:
            # results/feature_analysisãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ï¼ˆbackendãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹ï¼‰
            output_dir = Path(__file__).parent.parent.parent / "results" / "feature_analysis"
            output_dir.mkdir(parents=True, exist_ok=True)

            # çµ±åˆJSONä¿å­˜
            integrated_json = {
                "evaluation_date": datetime.now().isoformat(),
                "evaluated_models": list(self.all_results.keys()),
                "models_results": self.all_results,
            }

            json_path = output_dir / "all_models_feature_performance_evaluation.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(integrated_json, f, indent=2, ensure_ascii=False)
            logger.info(f"çµ±åˆJSONä¿å­˜å®Œäº†: {json_path}")

            # ãƒ¢ãƒ‡ãƒ«é–“æ€§èƒ½æ¯”è¼ƒCSV
            comparison_data = []
            for model_name, model_result in self.all_results.items():
                for scenario_key, scenario in model_result.get("scenarios", {}).items():
                    if scenario:
                        row = {
                            "model": model_name.upper(),
                            "scenario": scenario_key,
                            "n_features": scenario.get("n_features"),
                            "cv_accuracy": scenario.get("cv_accuracy"),
                            "cv_f1_weighted": scenario.get("cv_f1_weighted"),
                            "cv_balanced_accuracy": scenario.get(
                                "cv_balanced_accuracy"
                            ),
                            "train_time_sec": scenario.get("train_time_sec"),
                            "performance_change_pct": scenario.get(
                                "performance_change_pct", 0.0
                            ),
                            "removed_count": len(scenario.get("removed_features", [])),
                        }
                        comparison_data.append(row)

            if comparison_data:
                df = pd.DataFrame(comparison_data)
                csv_path = output_dir / "all_models_performance_comparison.csv"
                df.to_csv(csv_path, index=False)
                logger.info(f"çµ±åˆCSVä¿å­˜å®Œäº†: {csv_path}")

        except Exception as e:
            logger.error(f"çµ±åˆçµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _print_integrated_summary(self):
        """çµ±åˆçµæœã‚µãƒãƒªãƒ¼ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›"""
        print("\n" + "=" * 80)
        print("å…¨ãƒ¢ãƒ‡ãƒ«ç‰¹å¾´é‡æ€§èƒ½è©•ä¾¡çµæœ")
        print("=" * 80)

        if not self.all_results:
            print("è©•ä¾¡çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return

        # å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’æ¯”è¼ƒ
        print("\n" + "-" * 80)
        print("ã€ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¯”è¼ƒã€‘")
        print("-" * 80)
        print(
            f"{'ãƒ¢ãƒ‡ãƒ«':<15} {'Accuracy':<12} {'F1(Weight)':<12} "
            f"{'Bal.Acc':<10} {'å­¦ç¿’æ™‚é–“(ç§’)':<15}"
        )
        print("-" * 80)

        for model_name, result in self.all_results.items():
            baseline = result.get("scenarios", {}).get("baseline", {})
            if baseline:
                print(
                    f"{model_name.upper():<15} "
                    f"{baseline['cv_accuracy']:<12.4f} "
                    f"{baseline['cv_f1_weighted']:<12.4f} "
                    f"{baseline['cv_balanced_accuracy']:<10.4f} "
                    f"{baseline['train_time_sec']:<15.2f}"
                )

        # å„ãƒ¢ãƒ‡ãƒ«ã®æ¨å¥¨äº‹é …ã‚’æ¯”è¼ƒ
        print("\n" + "-" * 80)
        print("ã€ãƒ¢ãƒ‡ãƒ«åˆ¥æ¨å¥¨äº‹é …ã€‘")
        print("-" * 80)

        best_reduction = None
        best_model = None
        best_scenario = None

        for model_name, result in self.all_results.items():
            recommendation = result.get("recommendation", {})
            print(f"\n[{model_name.upper()}]")
            print(recommendation.get("message", "æ¨å¥¨äº‹é …ãªã—"))

            if "recommended_features_to_remove" in recommendation:
                removed_count = recommendation.get("features_removed_count", 0)
                if best_reduction is None or removed_count > best_reduction:
                    best_reduction = removed_count
                    best_model = model_name
                    best_scenario = recommendation

        # ç·åˆæ¨å¥¨
        print("\n" + "-" * 80)
        print("ã€ç·åˆæ¨å¥¨äº‹é …ã€‘")
        print("-" * 80)

        if best_model and best_scenario:
            print(
                f"æœ€ã‚‚åŠ¹æœçš„ãªå‰Šæ¸›: {best_model.upper()}ãƒ¢ãƒ‡ãƒ«ã§"
                f"{best_reduction}å€‹ã®ç‰¹å¾´é‡å‰Šæ¸›ãŒå¯èƒ½"
            )
            print(f"æ€§èƒ½å¤‰åŒ–: {best_scenario.get('performance_change_pct', 0):.2f}%")
            print(f"å‰Šæ¸›å¾Œã®ç‰¹å¾´é‡æ•°: {best_scenario.get('features_count_after')}å€‹")

            removed_features = best_scenario.get("recommended_features_to_remove", [])
            if removed_features:
                print(f"\nå‰Šé™¤æ¨å¥¨ç‰¹å¾´é‡ï¼ˆ{len(removed_features)}å€‹ï¼‰:")
                for i, feat in enumerate(removed_features, 1):
                    print(f"  {i:2}. {feat}")
        else:
            print(
                "å…¨ãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰" "å‰Šæ¸›ã§ãã‚‹ç‰¹å¾´é‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
            )

        print("\n" + "=" * 80 + "\n")


def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(description="å…¨ãƒ¢ãƒ‡ãƒ«ã§ã®ç‰¹å¾´é‡æ€§èƒ½è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument(
        "--models",
        nargs="+",
        choices=["lightgbm", "xgboost", "all"],
        default=["all"],
        help="è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: all)",
    )
    parser.add_argument(
        "--symbol",
        type=str,
        default="BTC/USDT:USDT",
        help="åˆ†æå¯¾è±¡ã‚·ãƒ³ãƒœãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: BTC/USDT:USDT)",
    )
    parser.add_argument(
        "--limit", type=int, default=2000, help="ãƒ‡ãƒ¼ã‚¿å–å¾—ä»¶æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2000)"
    )

    # Optunaé–¢é€£ã®å¼•æ•°
    parser.add_argument(
        "--enable-optuna",
        action="store_true",
        help="Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–",
    )
    parser.add_argument(
        "--n-trials",
        type=int,
        default=50,
        help="Optunaã®è©¦è¡Œå›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ï¼‰",
    )
    parser.add_argument(
        "--optuna-timeout",
        type=int,
        default=None,
        help="Optunaæœ€é©åŒ–ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰",
    )

    return parser.parse_args()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹
        args = parse_arguments()

        # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’æ±ºå®š
        if "all" in args.models:
            models = ["lightgbm", "xgboost"]
        else:
            models = args.models

        logger.info(f"è©•ä¾¡å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {', '.join([m.upper() for m in models])}")

        # Optunaæœ‰åŠ¹æ™‚ã®ãƒ­ã‚°å‡ºåŠ›
        if args.enable_optuna:
            logger.info("=" * 80)
            logger.info("ğŸš€ Optunaæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
            logger.info(f"è©¦è¡Œå›æ•°: {args.n_trials}")
            if args.optuna_timeout:
                logger.info(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {args.optuna_timeout}ç§’")
            logger.info("=" * 80)

        # è©•ä¾¡å®Ÿè¡Œ
        evaluator = MultiModelFeatureEvaluator(
            models=models,
            enable_optuna=args.enable_optuna,
            n_trials=args.n_trials,
            timeout=args.optuna_timeout,
        )
        evaluator.run_evaluation(symbol=args.symbol, limit=args.limit)

    except Exception as e:
        logger.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
