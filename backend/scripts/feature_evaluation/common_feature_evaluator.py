"""å…±é€šã®ç‰¹å¾´é‡è©•ä¾¡ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

DBã‹ã‚‰ã®OHLCV/FR/OIå–å¾—ã€FeatureEngineeringServiceã«ã‚ˆã‚‹ç‰¹å¾´é‡ç”Ÿæˆã€
forwardãƒªã‚¿ãƒ¼ãƒ³ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆã€TimeSeriesSplitã«ã‚ˆã‚‹CVè©•ä¾¡ãªã©ã€
feature_evaluationã‚¹ã‚¯ãƒªãƒ—ãƒˆé–“ã§å…±é€šã™ã‚‹å‡¦ç†ã‚’é›†ç´„ã™ã‚‹ã€‚
"""

from __future__ import annotations

import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆç›´å®Ÿè¡Œå¯¾å¿œï¼‰
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from app.config.unified_config import unified_config  # type: ignore
from app.services.ml.feature_engineering.feature_engineering_service import (  # type: ignore  # noqa: E501
    FeatureEngineeringService,
)
from app.services.ml.label_generation.presets import (  # type: ignore
    apply_preset_by_name,
)
from database.connection import SessionLocal  # type: ignore
from database.repositories.funding_rate_repository import (  # type: ignore
    FundingRateRepository,
)
from database.repositories.ohlcv_repository import OHLCVRepository  # type: ignore
from database.repositories.open_interest_repository import (  # type: ignore
    OpenInterestRepository,
)
from app.services.ml.cross_validation.purged_kfold import PurgedKFold
from app.services.ml.label_cache import LabelCache
from sklearn.metrics import classification_report

logger = logging.getLogger(__name__)


@dataclass
class EvaluationData:
    ohlcv: pd.DataFrame
    fr: Optional[pd.DataFrame]
    oi: Optional[pd.DataFrame]


class CommonFeatureEvaluator:
    """ç‰¹å¾´é‡è©•ä¾¡ç”¨ã®å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹"""

    def __init__(self) -> None:
        self.db = SessionLocal()
        self.ohlcv_repo = OHLCVRepository(self.db)
        self.fr_repo = FundingRateRepository(self.db)
        self.oi_repo = OpenInterestRepository(self.db)
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¯calculate_advanced_features()ã®profileå¼•æ•°ã§æŒ‡å®šå¯èƒ½
        self.feature_service = FeatureEngineeringService()
        self.label_cache: Optional[LabelCache] = None

    def close(self) -> None:
        self.db.close()

    # ---- ãƒ‡ãƒ¼ã‚¿å–å¾— ----

    def fetch_data(
        self,
        symbol: str = "BTC/USDT:USDT",
        timeframe: str = "1h",
        limit: int = 200000,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
    ) -> EvaluationData:
        logger.info(
            f"[Common] ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {symbol}, timeframe={timeframe}, limit={limit}, start={start_date}, end={end_date}"
        )

        # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        from datetime import datetime

        start_time = datetime.fromisoformat(start_date) if start_date else None
        end_time = datetime.fromisoformat(end_date) if end_date else None

        ohlcv_df = self.ohlcv_repo.get_ohlcv_dataframe(
            symbol=symbol,
            timeframe=timeframe,
            limit=limit,
            start_time=start_time,
            end_time=end_time,
        )
        if ohlcv_df.empty:
            logger.warning("OHLCVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return EvaluationData(pd.DataFrame(), None, None)

        start_time = ohlcv_df.index.min()
        end_time = ohlcv_df.index.max()

        try:
            fr_records = self.fr_repo.get_funding_rate_data(
                symbol=symbol,
                start_time=start_time,
                end_time=end_time,
            )
            fr_df: Optional[pd.DataFrame]
            if fr_records:
                fr_df = self.fr_repo.to_dataframe(
                    records=fr_records,
                    column_mapping={
                        "funding_timestamp": "timestamp",  # timestampã«çµ±ä¸€
                        "funding_rate": "funding_rate",
                    },
                    index_column="timestamp",  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã‚‚çµ±ä¸€
                )
                # funding_timestampã‚«ãƒ©ãƒ ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤
                if "funding_timestamp" in fr_df.columns:
                    fr_df = fr_df.drop(columns=["funding_timestamp"])
            else:
                fr_df = None
        except Exception as e:  # pragma: no cover - ãƒ­ãƒã‚¹ãƒˆãƒã‚¹ç›®çš„
            logger.warning(f"FRå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            fr_df = None

        try:
            oi_records = self.oi_repo.get_open_interest_data(
                symbol=symbol,
                start_time=start_time,
                end_time=end_time,
            )
            logger.info(f"OI records retrieved: {len(oi_records) if oi_records else 0}")
            oi_df: Optional[pd.DataFrame]
            if oi_records:
                oi_df = pd.DataFrame(
                    [
                        {
                            "timestamp": r.data_timestamp,
                            "open_interest_value": r.open_interest_value,
                        }
                        for r in oi_records
                    ]
                )
                oi_df.set_index("timestamp", inplace=True)
                logger.info(f"OI DataFrame created with shape: {oi_df.shape}")
            else:
                logger.warning("No OI records found for the specified period")
                oi_df = None
        except Exception as e:  # pragma: no cover
            logger.error(f"OIå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            oi_df = None

        return EvaluationData(ohlcv=ohlcv_df, fr=fr_df, oi=oi_df)

    # ---- ç‰¹å¾´é‡ç”Ÿæˆ ----

    def build_basic_features(
        self,
        data: EvaluationData,
        skip_crypto_and_advanced: bool = False,
    ) -> pd.DataFrame:
        """FeatureEngineeringServiceã‚’ä½¿ã£ã¦ç‰¹å¾´é‡ã‚’ç”Ÿæˆã™ã‚‹å…±é€šå‡¦ç†ã€‚"""
        if data.ohlcv.empty:
            return pd.DataFrame()

        # ãƒ‡ãƒ¼ã‚¿å­˜åœ¨ç¢ºèªã®ãƒ­ã‚°
        logger.info(f"OHLCV data shape: {data.ohlcv.shape}")
        if data.fr is not None:
            logger.info(
                f"FR data shape: {data.fr.shape}, columns: {data.fr.columns.tolist()}"
            )
        else:
            logger.warning("FR data is None")

        if data.oi is not None:
            logger.info(
                f"OI data shape: {data.oi.shape}, columns: {data.oi.columns.tolist()}"
            )
        else:
            logger.warning("OI data is None")

        original_crypto = self.feature_service.crypto_features
        # original_advanced = self.feature_service.advanced_features

        try:
            if skip_crypto_and_advanced:
                self.feature_service.crypto_features = None
                # self.feature_service.advanced_features = None

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦æ–°ã—ã„ç‰¹å¾´é‡è¨ˆç®—ã‚’å¼·åˆ¶
            self.feature_service.clear_cache()
            logger.info("Feature service cache cleared")

            features_df = self.feature_service.calculate_advanced_features(
                ohlcv_data=data.ohlcv,
                funding_rate_data=data.fr,
                open_interest_data=data.oi,
            )

            logger.info(f"Generated features columns: {features_df.columns.tolist()}")
        finally:
            self.feature_service.crypto_features = original_crypto
            # self.feature_service.advanced_features = original_advanced

        return features_df

    def drop_ohlcv_columns(
        self,
        features_df: pd.DataFrame,
        keep_close: bool = False,
    ) -> pd.DataFrame:
        if features_df.empty:
            return features_df

        drop_cols: List[str] = ["open", "high", "low", "volume"]
        if not keep_close:
            drop_cols.append("close")

        return features_df[[c for c in features_df.columns if c not in drop_cols]]

    # ---- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆ ----

    @staticmethod
    def create_forward_return_target(
        close: pd.Series,
        periods: int = 1,
    ) -> pd.Series:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®æ—§ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆéæ¨å¥¨ï¼‰

        æ³¨æ„: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã•ã‚Œã¦ã„ã¾ã™ãŒã€
        æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã¯ create_labels_from_config() ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        """
        if close.empty:
            raise ValueError("closeã‚·ãƒªãƒ¼ã‚ºãŒç©ºã§ã™")
        return close.pct_change(periods).shift(-periods)

    def create_labels_from_config(
        self,
        ohlcv_df: pd.DataFrame,
        preset_name: Optional[str] = None,
        price_column: str = "close",
    ) -> pd.Series:
        """çµ±ä¸€è¨­å®šã‹ã‚‰åˆ†é¡ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ

        unified_configã®label_generationè¨­å®šã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        ãƒ—ãƒªã‚»ãƒƒãƒˆåã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€è¨­å®šã‚’ä¸Šæ›¸ãã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

        Args:
            ohlcv_df: OHLCVãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            preset_name: ãƒ—ãƒªã‚»ãƒƒãƒˆåï¼ˆNoneã®å ´åˆã¯è¨­å®šã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
            price_column: ä¾¡æ ¼ã‚«ãƒ©ãƒ åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: "close"ï¼‰

        Returns:
            pd.Series: "UP" / "RANGE" / "DOWN" ã®åˆ†é¡ãƒ©ãƒ™ãƒ«

        Raises:
            ValueError: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒä¸æ­£ãªå ´åˆ

        Example:
            >>> evaluator = CommonFeatureEvaluator()
            >>> labels = evaluator.create_labels_from_config(ohlcv_df)
            >>> # ã¾ãŸã¯ç‰¹å®šã®ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
            >>> labels = evaluator.create_labels_from_config(
            ...     ohlcv_df, preset_name="1h_4bars"
            ... )
        """
        if ohlcv_df.empty:
            raise ValueError("OHLCVãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç©ºã§ã™")

        if price_column not in ohlcv_df.columns:
            raise ValueError(
                f"ä¾¡æ ¼ã‚«ãƒ©ãƒ  '{price_column}' ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“"
            )

        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‚’å–å¾—
        label_config = unified_config.ml.training.label_generation

        # ãƒ—ãƒªã‚»ãƒƒãƒˆä½¿ç”¨ã®å ´åˆ
        if label_config.use_preset or preset_name is not None:
            preset_to_use = preset_name or label_config.default_preset
            logger.info(f"ãƒ—ãƒªã‚»ãƒƒãƒˆ '{preset_to_use}' ã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ™ãƒ«ç”Ÿæˆ")

            labels, preset_info = apply_preset_by_name(
                df=ohlcv_df,
                preset_name=preset_to_use,
                price_column=price_column,
            )

            logger.info(f"ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†: {preset_info['description']}")
            return labels

    def get_label_config_info(self) -> Dict[str, any]:
        """ç¾åœ¨ã®ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šæƒ…å ±ã‚’å–å¾—

        Returns:
            Dict: ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã®è©³ç´°æƒ…å ±
        """
        label_config = unified_config.ml.training.label_generation

        info = {
            "use_preset": label_config.use_preset,
            "timeframe": label_config.timeframe,
            "horizon_n": label_config.horizon_n,
            "threshold": label_config.threshold,
            "price_column": label_config.price_column,
            "threshold_method": label_config.threshold_method,
        }

        if label_config.use_preset:
            info["preset_name"] = label_config.default_preset

        return info

    def generate_pipeline_compatible_labels(
        self,
        ohlcv_df: pd.DataFrame,
        horizon_n: int = 4,
        pt_factor: float = 1.0,
        sl_factor: float = 1.0,
        use_atr: bool = True,
        atr_period: int = 14,
        binary_label: bool = True,
        timeframe: str = "1h",
    ) -> pd.Series:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³äº’æ›ã®ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆLabelCacheä½¿ç”¨ï¼‰

        Args:
            ohlcv_df: OHLCVãƒ‡ãƒ¼ã‚¿
            horizon_n: ãƒ›ãƒ©ã‚¤ã‚ºãƒ³
            pt_factor: åˆ©ç¢ºä¿‚æ•°
            sl_factor: æåˆ‡ä¿‚æ•°
            use_atr: ATRä½¿ç”¨ãƒ•ãƒ©ã‚°
            atr_period: ATRæœŸé–“
            binary_label: 2å€¤åˆ†é¡ãƒ•ãƒ©ã‚°
            timeframe: æ™‚é–“è¶³

        Returns:
            pd.Series: ãƒ©ãƒ™ãƒ«
        """
        if self.label_cache is None:
            self.label_cache = LabelCache(ohlcv_df)
        else:
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæ›´æ–°ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚å†è¨­å®š
            self.label_cache.ohlcv_df = ohlcv_df

        return self.label_cache.get_labels(
            horizon_n=horizon_n,
            threshold_method="TRIPLE_BARRIER",
            threshold=0.0,
            timeframe=timeframe,
            price_column="close",
            pt_factor=pt_factor,
            sl_factor=sl_factor,
            use_atr=use_atr,
            atr_period=atr_period,
            binary_label=binary_label,
        )

    def get_t1_for_purged_kfold(
        self, indices: pd.DatetimeIndex, horizon_n: int, timeframe: str = "1h"
    ) -> pd.Series:
        """PurgedKFoldç”¨ã®t1ã‚’å–å¾—"""
        if self.label_cache is None:
            # ãƒ€ãƒŸãƒ¼ã®LabelCacheã‚’ä½œæˆã—ã¦ãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆ©ç”¨
            self.label_cache = LabelCache(pd.DataFrame())

        return self.label_cache.get_t1(indices, horizon_n, timeframe)

    # ---- CV è©•ä¾¡ ----

    @staticmethod
    def time_series_cv(
        X: pd.DataFrame,
        y: pd.Series,
        n_splits: int = 5,
    ) -> Dict[str, float]:
        if X.empty:
            raise ValueError("ç‰¹å¾´é‡ãŒç©ºã§ã™")
        if len(X) != len(y):
            raise ValueError("ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é•·ã•ãŒä¸€è‡´ã—ã¾ã›ã‚“")

        tscv = TimeSeriesSplit(n_splits=n_splits)

        rmses: List[float] = []
        maes: List[float] = []
        r2s: List[float] = []
        start_time = time.perf_counter()

        for train_idx, test_idx in tscv.split(X):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            # ã‚·ãƒ³ãƒ—ãƒ«ãªç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆç”¨; å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¯å‘¼ã³å‡ºã—å´ã§è¡Œã†
            from sklearn.linear_model import LinearRegression

            model = LinearRegression()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            rmse = mean_squared_error(y_test, y_pred, squared=False)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            rmses.append(rmse)
            maes.append(mae)
            r2s.append(r2)

        elapsed = time.perf_counter() - start_time

        return {
            "cv_rmse": float(np.mean(rmses)),
            "cv_rmse_std": float(np.std(rmses)),
            "cv_mae": float(np.mean(maes)),
            "cv_mae_std": float(np.std(maes)),
            "cv_r2": float(np.mean(r2s)),
            "cv_r2_std": float(np.std(r2s)),
            "train_time_sec": float(elapsed),
        }

    @staticmethod
    def purged_kfold_cv(
        X: pd.DataFrame,
        y: pd.Series,
        t1: pd.Series,
        n_splits: int = 5,
        embargo_pct: float = 0.01,
        use_meta_labeling_metrics: bool = False,
    ) -> Dict[str, float]:
        """PurgedKFoldã«ã‚ˆã‚‹CVè©•ä¾¡ï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æº–æ‹  + ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°å¯¾å¿œï¼‰

        Args:
            X: ç‰¹å¾´é‡DataFrame
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆSeries
            t1: ãƒ©ãƒ™ãƒ«ã®çµ‚äº†æ™‚åˆ»Series
            n_splits: CVåˆ†å‰²æ•°
            embargo_pct: ã‚¨ãƒ³ãƒãƒ¼ã‚´æ¯”ç‡
            use_meta_labeling_metrics: ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°è©•ä¾¡æŒ‡æ¨™ã‚’ä½¿ç”¨ã™ã‚‹ã‹

        Returns:
            Dict[str, float]: è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        if X.empty:
            raise ValueError("ç‰¹å¾´é‡ãŒç©ºã§ã™")

        cv = PurgedKFold(n_splits=n_splits, t1=t1, pct_embargo=embargo_pct)

        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMï¼‰ã§è©•ä¾¡
        import lightgbm as lgb

        f1_scores = []
        precisions = []
        recalls = []
        pipeline_scores = []

        # ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ç”¨è¿½åŠ æŒ‡æ¨™
        if use_meta_labeling_metrics:
            win_rates = []
            expected_values = []
            signal_adoption_rates = []

        start_time = time.perf_counter()

        for train_idx, val_idx in cv.split(X, y):
            if len(train_idx) == 0 or len(val_idx) == 0:
                continue

            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            # ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
            if len(y_train.unique()) > 1:
                class_weight = "balanced"
            else:
                class_weight = None

            model = lgb.LGBMClassifier(
                n_estimators=100,
                random_state=42,
                verbosity=-1,
                class_weight=class_weight,
            )

            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)

            # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            report = classification_report(
                y_val, y_pred, output_dict=True, zero_division=0
            )

            # 2å€¤åˆ†é¡ã® "1" (Positive) ã®ã‚¹ã‚³ã‚¢ã‚’å‚ç…§
            if "1" in report:
                metrics = report["1"]
                f1 = metrics["f1-score"]
                prec = metrics["precision"]
                rec = metrics["recall"]
                n_trades = y_pred.sum()

                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢: F1 * log(1 + trades)
                # ãŸã ã—ç²¾åº¦è¦ä»¶ã‚ã‚Š
                if prec < 0.55 or n_trades < 10:
                    p_score = 0.0
                else:
                    p_score = f1 * np.log1p(n_trades)

                # ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°æŒ‡æ¨™ã®è¨ˆç®—
                if use_meta_labeling_metrics:
                    # Win Rate = Precisionï¼ˆå‹ç‡ï¼‰
                    win_rate = prec

                    # Expected Valueï¼ˆæœŸå¾…å€¤ï¼‰
                    # å‹ã¡æ™‚+1ã€è² ã‘æ™‚-1ã¨ä»®å®š
                    expected_value = (prec * 1.0) + ((1 - prec) * -1.0)

                    # Signal Adoption Rateï¼ˆã‚·ã‚°ãƒŠãƒ«æ¡æŠç‡ï¼‰
                    signal_adoption_rate = (
                        n_trades / len(y_val) if len(y_val) > 0 else 0.0
                    )

                    win_rates.append(win_rate)
                    expected_values.append(expected_value)
                    signal_adoption_rates.append(signal_adoption_rate)
            else:
                # ã‚¯ãƒ©ã‚¹1ãŒå­˜åœ¨ã—ãªã„å ´åˆãªã©
                f1 = 0.0
                prec = 0.0
                rec = 0.0
                p_score = 0.0

                if use_meta_labeling_metrics:
                    win_rates.append(0.0)
                    expected_values.append(-1.0)
                    signal_adoption_rates.append(0.0)

            f1_scores.append(f1)
            precisions.append(prec)
            recalls.append(rec)
            pipeline_scores.append(p_score)

        elapsed = time.perf_counter() - start_time

        result = {
            "cv_f1": float(np.mean(f1_scores)) if f1_scores else 0.0,
            "cv_precision": float(np.mean(precisions)) if precisions else 0.0,
            "cv_recall": float(np.mean(recalls)) if recalls else 0.0,
            "cv_pipeline_score": (
                float(np.mean(pipeline_scores)) if pipeline_scores else 0.0
            ),
            "train_time_sec": float(elapsed),
        }

        # ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°æŒ‡æ¨™ã‚’è¿½åŠ 
        if use_meta_labeling_metrics:
            result.update(
                {
                    "cv_win_rate": float(np.mean(win_rates)) if win_rates else 0.0,
                    "cv_expected_value": (
                        float(np.mean(expected_values)) if expected_values else -1.0
                    ),
                    "cv_signal_adoption_rate": (
                        float(np.mean(signal_adoption_rates))
                        if signal_adoption_rates
                        else 0.0
                    ),
                }
            )

            logger.info("ğŸ“Š Meta-Labeling Evaluation Results:")
            logger.info(f"  Win Rate (Precision): {result['cv_win_rate']:.2%}")
            logger.info(f"  Expected Value: {result['cv_expected_value']:.4f}")
            logger.info(
                f"  Signal Adoption Rate: {result['cv_signal_adoption_rate']:.2%}"
            )

        return result
