#!/usr/bin/env python3
"""
AutoMLç‰¹å¾´é‡ã®æ·±åº¦åˆ†æ

ç‰¹å¾´é‡ã®å“è³ªã€ç›¸é–¢ã€é‡è¦åº¦ã€å®‰å®šæ€§ã‚’è©³ç´°ã«åˆ†æã—ã€
ã‚ˆã‚ŠåŠ¹æœçš„ãªç‰¹å¾´é‡ç”Ÿæˆã®ãŸã‚ã®æ”¹å–„ç‚¹ã‚’ç‰¹å®šã—ã¾ã™ã€‚
"""

import sys
import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import mutual_info_regression, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.services.ml.feature_engineering.enhanced_crypto_features import EnhancedCryptoFeatures

def generate_comprehensive_test_data(hours: int = 720) -> pd.DataFrame:
    """
    åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆ30æ—¥åˆ†ï¼‰
    ã‚ˆã‚Šè¤‡é›‘ã§ç¾å®Ÿçš„ãªå¸‚å ´å‹•å‘ã‚’æ¨¡æ“¬
    """
    print(f"åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {hours}æ™‚é–“åˆ†")
    
    start_time = datetime.now() - timedelta(hours=hours)
    timestamps = [start_time + timedelta(hours=i) for i in range(hours)]
    
    # è¤‡æ•°ã®å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ 
    np.random.seed(42)
    
    # 1. é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆé€±å˜ä½ï¼‰
    weekly_trend = np.sin(np.arange(hours) * 2 * np.pi / (24 * 7)) * 0.02
    
    # 2. ä¸­æœŸã‚µã‚¤ã‚¯ãƒ«ï¼ˆæ—¥å˜ä½ï¼‰
    daily_cycle = np.sin(np.arange(hours) * 2 * np.pi / 24) * 0.01
    
    # 3. çŸ­æœŸãƒã‚¤ã‚º
    short_noise = np.random.normal(0, 0.015, hours)
    
    # 4. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    volatility_regime = np.zeros(hours)
    current_vol = 0.02
    for i in range(hours):
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®æŒç¶šæ€§
        if np.random.random() < 0.05:  # 5%ã®ç¢ºç‡ã§ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰æ›´
            current_vol = np.random.choice([0.01, 0.02, 0.04], p=[0.3, 0.5, 0.2])
        volatility_regime[i] = current_vol
    
    # 5. çªç™ºçš„ãªã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ç­‰ï¼‰
    event_impacts = np.zeros(hours)
    for _ in range(5):  # 5å›ã®ã‚¤ãƒ™ãƒ³ãƒˆ
        event_time = np.random.randint(0, hours)
        event_magnitude = np.random.choice([-0.05, 0.05], p=[0.5, 0.5])
        event_duration = np.random.randint(1, 6)  # 1-5æ™‚é–“
        for j in range(event_duration):
            if event_time + j < hours:
                event_impacts[event_time + j] = event_magnitude * np.exp(-j * 0.5)
    
    # ç·åˆçš„ãªãƒªã‚¿ãƒ¼ãƒ³
    base_returns = weekly_trend + daily_cycle + event_impacts
    noise_returns = np.random.normal(0, volatility_regime)
    returns = base_returns + noise_returns
    
    # ä¾¡æ ¼ç”Ÿæˆ
    base_price = 50000
    prices = base_price * np.exp(np.cumsum(returns))
    
    # OHLCVç”Ÿæˆ
    data = []
    for i, (timestamp, close) in enumerate(zip(timestamps, prices)):
        vol = volatility_regime[i] * close
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªé«˜å€¤ãƒ»å®‰å€¤
        high_factor = np.random.exponential(0.3) * vol
        low_factor = np.random.exponential(0.3) * vol
        high = close + high_factor
        low = close - low_factor
        
        if i == 0:
            open_price = close
        else:
            gap = np.random.normal(0, vol * 0.1)
            open_price = prices[i-1] + gap
        
        # å‡ºæ¥é«˜ï¼ˆä¾¡æ ¼å¤‰å‹•ã¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã«ç›¸é–¢ï¼‰
        volume_base = 1000
        volume_volatility_factor = volatility_regime[i] * 50000
        volume_price_factor = abs(returns[i]) * 100000
        volume = max(100, np.random.normal(
            volume_base + volume_volatility_factor + volume_price_factor,
            volume_base * 0.3
        ))
        
        data.append({
            'timestamp': timestamp,
            'Open': open_price,
            'High': max(open_price, high, close),
            'Low': min(open_price, low, close),
            'Close': close,
            'Volume': volume,
            'volatility_regime': volatility_regime[i],
        })
    
    df = pd.DataFrame(data)
    df.set_index('timestamp', inplace=True)
    
    # Open Interestï¼ˆã‚ˆã‚Šè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    oi_base = 1000000
    oi_changes = []
    
    for i in range(0, hours, 8):  # 8æ™‚é–“ã”ã¨
        if i == 0:
            oi_change = 0
        else:
            # è¤‡æ•°è¦å› ã®çµ„ã¿åˆã‚ã›
            price_momentum = np.mean(returns[max(0, i-24):i])  # 24æ™‚é–“ã®ä¾¡æ ¼å‹¢ã„
            volatility_avg = np.mean(volatility_regime[max(0, i-24):i])  # 24æ™‚é–“ã®å¹³å‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            
            # OIå¤‰å‹•ã®è¦å› 
            momentum_factor = price_momentum * 0.3  # ä¾¡æ ¼å‹¢ã„ã«è¿½éš
            volatility_factor = (volatility_avg - 0.02) * 0.5  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã§å¢—åŠ 
            mean_reversion = -0.1 * (len(oi_changes) > 0 and oi_changes[-1] or 0)  # å¹³å‡å›å¸°
            noise = np.random.normal(0, 0.01)
            
            oi_change = momentum_factor + volatility_factor + mean_reversion + noise
        
        oi_changes.append(oi_change)
    
    oi_values = oi_base * np.exp(np.cumsum(oi_changes))
    oi_timestamps = [start_time + timedelta(hours=i*8) for i in range(len(oi_values))]
    oi_series = pd.Series(oi_values, index=oi_timestamps)
    df['open_interest'] = oi_series.reindex(df.index, method='ffill')
    
    # Funding Rateï¼ˆå¸‚å ´æ§‹é€ ã‚’åæ˜ ï¼‰
    fr_values = []
    for i in range(len(oi_values)):
        if i == 0:
            price_trend = 0
            oi_trend = 0
        else:
            start_idx = max(0, i*8-24)
            end_idx = i*8
            price_trend = np.mean(returns[start_idx:end_idx])
            oi_trend = oi_changes[i] if i < len(oi_changes) else 0
        
        # FRæ±ºå®šè¦å› 
        base_fr = price_trend * 0.15  # ä¾¡æ ¼ãƒˆãƒ¬ãƒ³ãƒ‰ã®å½±éŸ¿
        oi_pressure = oi_trend * 0.1  # OIå¤‰å‹•ã®å½±éŸ¿
        market_stress = (volatility_regime[min(i*8, hours-1)] - 0.02) * 0.002  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®å½±éŸ¿
        
        # æ¥µå€¤åˆ¶é™ã¨ãƒã‚¤ã‚º
        fr = np.clip(
            base_fr + oi_pressure + market_stress + np.random.normal(0, 0.0001),
            -0.01, 0.01
        )
        fr_values.append(fr)
    
    fr_series = pd.Series(fr_values, index=oi_timestamps)
    df['funding_rate'] = fr_series.reindex(df.index, method='ffill')
    
    # Fear & Greed Indexï¼ˆå¿ƒç†çš„è¦å› ã‚’åæ˜ ï¼‰
    daily_timestamps = [start_time + timedelta(days=i) for i in range(hours // 24 + 1)]
    fg_values = []
    
    for i in range(len(daily_timestamps)):
        if i == 0:
            daily_return = 0
            daily_volatility = 0.02
        else:
            start_idx = max(0, i*24-24)
            end_idx = min(hours-1, i*24)
            daily_return = np.sum(returns[start_idx:end_idx])
            daily_volatility = np.std(returns[start_idx:end_idx])
        
        # FGæ±ºå®šè¦å› 
        fear_from_loss = max(0, -daily_return * 1000)  # æå¤±ã‹ã‚‰ã®ææ€–
        fear_from_volatility = (daily_volatility - 0.02) * 500  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‹ã‚‰ã®ææ€–
        greed_from_gain = max(0, daily_return * 800)  # åˆ©ç›Šã‹ã‚‰ã®å¼·æ¬²
        
        # åŸºæº–å€¤50ã‹ã‚‰èª¿æ•´
        fg_raw = 50 - fear_from_loss - fear_from_volatility + greed_from_gain
        
        # æ…£æ€§ï¼ˆå‰æ—¥ã‹ã‚‰ã®å¤‰åŒ–ã‚’åˆ¶é™ï¼‰
        if i > 0:
            max_change = 10
            fg_raw = fg_values[-1] + np.clip(fg_raw - fg_values[-1], -max_change, max_change)
        
        # ãƒã‚¤ã‚ºã¨åˆ¶é™
        fg = np.clip(fg_raw + np.random.normal(0, 2), 0, 100)
        fg_values.append(fg)
    
    fg_series = pd.Series(fg_values, index=daily_timestamps)
    df['fear_greed_value'] = fg_series.reindex(df.index, method='ffill')
    
    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ æƒ…å ±ã‚’å‰Šé™¤ï¼ˆåˆ†æç”¨ã®ã¿ï¼‰
    df = df.drop('volatility_regime', axis=1)
    
    print(f"ç”Ÿæˆå®Œäº†: {len(df)}è¡Œ")
    print(f"ä¾¡æ ¼å¤‰å‹•: {(df['Close'].iloc[-1]/df['Close'].iloc[0]-1)*100:+.1f}%")
    print(f"æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³: {((df['Close']/df['Close'].cummax()).min()-1)*100:.1f}%")
    print(f"å¹³å‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {df['Close'].pct_change().std()*100:.2f}%")
    
    return df

def analyze_feature_stability(df: pd.DataFrame, crypto_features: EnhancedCryptoFeatures):
    """
    ç‰¹å¾´é‡ã®å®‰å®šæ€§ã‚’åˆ†æ
    """
    print("\n=== ç‰¹å¾´é‡å®‰å®šæ€§åˆ†æ ===")
    
    # è¤‡æ•°ã®æœŸé–“ã§ç‰¹å¾´é‡ã‚’è¨ˆç®—
    periods = [168, 336, 504, 720]  # 1é€±é–“ã€2é€±é–“ã€3é€±é–“ã€1ãƒ¶æœˆ
    stability_results = {}
    
    for period in periods:
        print(f"\nğŸ“Š {period}æ™‚é–“ï¼ˆ{period//24}æ—¥ï¼‰ãƒ‡ãƒ¼ã‚¿ã§ã®åˆ†æ:")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ‡ã‚Šå–ã‚Š
        period_df = df.iloc[-period:].copy()
        
        # ç‰¹å¾´é‡ç”Ÿæˆ
        enhanced_df = crypto_features.create_comprehensive_features(period_df)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
        target = enhanced_df['Close'].pct_change(4).shift(-4)
        
        # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿
        valid_mask = target.notna() & enhanced_df.notna().all(axis=1)
        if valid_mask.sum() < 50:
            print(f"  æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {valid_mask.sum()}ä»¶")
            continue
        
        # ç‰¹å¾´é‡ã®ç›¸é–¢è¨ˆç®—
        feature_cols = [col for col in enhanced_df.columns 
                       if col not in ['Open', 'High', 'Low', 'Close', 'Volume']]
        
        correlations = {}
        for col in feature_cols:
            corr = enhanced_df.loc[valid_mask, col].corr(target.loc[valid_mask])
            if not pd.isna(corr):
                correlations[col] = abs(corr)
        
        stability_results[period] = correlations
        print(f"  æœ‰åŠ¹ç‰¹å¾´é‡: {len(correlations)}å€‹")
        print(f"  å¹³å‡ç›¸é–¢: {np.mean(list(correlations.values())):.4f}")
    
    # å®‰å®šæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
    print(f"\nğŸ¯ ç‰¹å¾´é‡å®‰å®šæ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    
    # å…¨æœŸé–“ã§å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡
    common_features = set(stability_results[periods[0]].keys())
    for period in periods[1:]:
        common_features &= set(stability_results[period].keys())
    
    stability_scores = {}
    for feature in common_features:
        correlations = [stability_results[period][feature] for period in periods]
        # å®‰å®šæ€§ã‚¹ã‚³ã‚¢ = å¹³å‡ç›¸é–¢ - ç›¸é–¢ã®æ¨™æº–åå·®
        stability_score = np.mean(correlations) - np.std(correlations)
        stability_scores[feature] = {
            'stability_score': stability_score,
            'mean_correlation': np.mean(correlations),
            'correlation_std': np.std(correlations),
            'correlations': correlations,
        }
    
    # å®‰å®šæ€§é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_features = sorted(stability_scores.items(), 
                           key=lambda x: x[1]['stability_score'], reverse=True)
    
    for i, (feature, scores) in enumerate(sorted_features[:20]):
        print(f"  {i+1:2d}. {feature:<35} "
              f"å®‰å®šæ€§: {scores['stability_score']:+.4f} "
              f"(å¹³å‡: {scores['mean_correlation']:.4f}, "
              f"æ¨™æº–åå·®: {scores['correlation_std']:.4f})")
    
    return stability_scores

def analyze_feature_interactions(df: pd.DataFrame, crypto_features: EnhancedCryptoFeatures):
    """
    ç‰¹å¾´é‡é–“ã®ç›¸äº’ä½œç”¨ã‚’åˆ†æ
    """
    print("\n=== ç‰¹å¾´é‡ç›¸äº’ä½œç”¨åˆ†æ ===")
    
    # ç‰¹å¾´é‡ç”Ÿæˆ
    enhanced_df = crypto_features.create_comprehensive_features(df)
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
    target = enhanced_df['Close'].pct_change(4).shift(-4)
    
    # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿
    valid_mask = target.notna() & enhanced_df.notna().all(axis=1)
    if valid_mask.sum() < 100:
        print("æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        return
    
    # ç‰¹å¾´é‡é¸æŠ
    feature_cols = [col for col in enhanced_df.columns 
                   if col not in ['Open', 'High', 'Low', 'Close', 'Volume']]
    
    X = enhanced_df.loc[valid_mask, feature_cols]
    y = target.loc[valid_mask]
    
    # 1. ç›¸äº’æƒ…å ±é‡åˆ†æ
    print(f"\nğŸ“Š ç›¸äº’æƒ…å ±é‡åˆ†æ:")
    mi_scores = mutual_info_regression(X, y, random_state=42)
    mi_results = list(zip(feature_cols, mi_scores))
    mi_results.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ä¸Šä½10ç‰¹å¾´é‡ï¼ˆç›¸äº’æƒ…å ±é‡ï¼‰:")
    for i, (feature, score) in enumerate(mi_results[:10]):
        print(f"  {i+1:2d}. {feature:<35} MI: {score:.4f}")
    
    # 2. Fçµ±è¨ˆé‡åˆ†æ
    print(f"\nğŸ“Š Fçµ±è¨ˆé‡åˆ†æ:")
    f_scores, _ = f_regression(X, y)
    f_results = list(zip(feature_cols, f_scores))
    f_results.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ä¸Šä½10ç‰¹å¾´é‡ï¼ˆFçµ±è¨ˆé‡ï¼‰:")
    for i, (feature, score) in enumerate(f_results[:10]):
        print(f"  {i+1:2d}. {feature:<35} F: {score:.2f}")
    
    # 3. ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆé‡è¦åº¦
    print(f"\nğŸ“Š ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆé‡è¦åº¦:")
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X, y)
    
    rf_results = list(zip(feature_cols, rf.feature_importances_))
    rf_results.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ä¸Šä½10ç‰¹å¾´é‡ï¼ˆRFé‡è¦åº¦ï¼‰:")
    for i, (feature, importance) in enumerate(rf_results[:10]):
        print(f"  {i+1:2d}. {feature:<35} é‡è¦åº¦: {importance:.4f}")
    
    # 4. Lassoå›å¸°ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ
    print(f"\nğŸ“Š Lassoå›å¸°ç‰¹å¾´é‡é¸æŠ:")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    lasso = LassoCV(cv=5, random_state=42, max_iter=1000)
    lasso.fit(X_scaled, y)
    
    lasso_coefs = np.abs(lasso.coef_)
    lasso_results = list(zip(feature_cols, lasso_coefs))
    lasso_results.sort(key=lambda x: x[1], reverse=True)
    
    selected_features = [feature for feature, coef in lasso_results if coef > 0]
    print(f"é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {len(selected_features)}å€‹")
    
    print(f"ä¸Šä½10ç‰¹å¾´é‡ï¼ˆLassoä¿‚æ•°ï¼‰:")
    for i, (feature, coef) in enumerate(lasso_results[:10]):
        if coef > 0:
            print(f"  {i+1:2d}. {feature:<35} ä¿‚æ•°: {coef:.4f}")
    
    return {
        'mutual_info': mi_results,
        'f_statistic': f_results,
        'random_forest': rf_results,
        'lasso': lasso_results,
        'selected_features': selected_features,
    }

def analyze_feature_groups_performance(df: pd.DataFrame, crypto_features: EnhancedCryptoFeatures):
    """
    ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã®æ€§èƒ½åˆ†æ
    """
    print("\n=== ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—æ€§èƒ½åˆ†æ ===")
    
    # ç‰¹å¾´é‡ç”Ÿæˆ
    enhanced_df = crypto_features.create_comprehensive_features(df)
    feature_groups = crypto_features.get_feature_groups()
    
    # è¤‡æ•°ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæœŸé–“ã§åˆ†æ
    target_periods = [1, 4, 12, 24]
    group_performance = {}
    
    for period in target_periods:
        print(f"\nğŸ“ˆ {period}æ™‚é–“å¾Œäºˆæ¸¬ã§ã®æ€§èƒ½:")
        
        target = enhanced_df['Close'].pct_change(period).shift(-period)
        valid_mask = target.notna() & enhanced_df.notna().all(axis=1)
        
        if valid_mask.sum() < 50:
            print(f"  æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {valid_mask.sum()}ä»¶")
            continue
        
        period_performance = {}
        
        for group_name, features in feature_groups.items():
            if not features:
                continue
            
            # ã‚°ãƒ«ãƒ¼ãƒ—å†…ç‰¹å¾´é‡ã®ç›¸é–¢è¨ˆç®—
            group_correlations = []
            for feature in features:
                if feature in enhanced_df.columns:
                    corr = enhanced_df.loc[valid_mask, feature].corr(target.loc[valid_mask])
                    if not pd.isna(corr):
                        group_correlations.append(abs(corr))
            
            if group_correlations:
                period_performance[group_name] = {
                    'feature_count': len(group_correlations),
                    'mean_correlation': np.mean(group_correlations),
                    'max_correlation': np.max(group_correlations),
                    'std_correlation': np.std(group_correlations),
                    'top_10_percent': np.percentile(group_correlations, 90),
                }
        
        group_performance[period] = period_performance
        
        # ã‚°ãƒ«ãƒ¼ãƒ—æ€§èƒ½ã‚’ã‚½ãƒ¼ãƒˆ
        sorted_groups = sorted(period_performance.items(), 
                             key=lambda x: x[1]['mean_correlation'], reverse=True)
        
        for group_name, performance in sorted_groups:
            print(f"  {group_name:<15}: "
                  f"å¹³å‡ {performance['mean_correlation']:.4f}, "
                  f"æœ€å¤§ {performance['max_correlation']:.4f}, "
                  f"90%ile {performance['top_10_percent']:.4f} "
                  f"({performance['feature_count']}å€‹)")
    
    return group_performance

def identify_improvement_opportunities(stability_scores, interaction_results, group_performance):
    """
    æ”¹å–„æ©Ÿä¼šã®ç‰¹å®š
    """
    print("\n=== æ”¹å–„æ©Ÿä¼šã®ç‰¹å®š ===")
    
    # 1. ä¸å®‰å®šãªç‰¹å¾´é‡ã®ç‰¹å®š
    print(f"\nğŸ” ä¸å®‰å®šãªç‰¹å¾´é‡ï¼ˆæ”¹å–„ãŒå¿…è¦ï¼‰:")
    unstable_features = []
    for feature, scores in stability_scores.items():
        if scores['correlation_std'] > 0.02:  # æ¨™æº–åå·®ãŒå¤§ãã„
            unstable_features.append((feature, scores['correlation_std']))
    
    unstable_features.sort(key=lambda x: x[1], reverse=True)
    for feature, std in unstable_features[:10]:
        print(f"  - {feature:<35} æ¨™æº–åå·®: {std:.4f}")
    
    # 2. ä¸€è²«ã—ã¦é«˜æ€§èƒ½ãªç‰¹å¾´é‡
    print(f"\nâœ… ä¸€è²«ã—ã¦é«˜æ€§èƒ½ãªç‰¹å¾´é‡:")
    stable_high_performers = []
    for feature, scores in stability_scores.items():
        if scores['mean_correlation'] > 0.05 and scores['correlation_std'] < 0.01:
            stable_high_performers.append((feature, scores['stability_score']))
    
    stable_high_performers.sort(key=lambda x: x[1], reverse=True)
    for feature, score in stable_high_performers[:10]:
        print(f"  - {feature:<35} å®‰å®šæ€§ã‚¹ã‚³ã‚¢: {score:.4f}")
    
    # 3. è¤‡æ•°æ‰‹æ³•ã§é«˜è©•ä¾¡ã®ç‰¹å¾´é‡
    print(f"\nğŸ¯ è¤‡æ•°æ‰‹æ³•ã§é«˜è©•ä¾¡ã®ç‰¹å¾´é‡:")
    
    # å„æ‰‹æ³•ã®ä¸Šä½20%ã‚’å–å¾—
    mi_top = set([f for f, _ in interaction_results['mutual_info'][:len(interaction_results['mutual_info'])//5]])
    f_top = set([f for f, _ in interaction_results['f_statistic'][:len(interaction_results['f_statistic'])//5]])
    rf_top = set([f for f, _ in interaction_results['random_forest'][:len(interaction_results['random_forest'])//5]])
    lasso_selected = set(interaction_results['selected_features'])
    
    # è¤‡æ•°æ‰‹æ³•ã§é¸ã°ã‚ŒãŸç‰¹å¾´é‡
    multi_method_features = mi_top & f_top & rf_top & lasso_selected
    
    for feature in list(multi_method_features)[:10]:
        print(f"  - {feature}")
    
    # 4. æ”¹å–„ææ¡ˆ
    print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
    print(f"1. ä¸å®‰å®šç‰¹å¾´é‡ã®æ”¹è‰¯:")
    print(f"   - ç§»å‹•å¹³å‡ã‚„ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã®é©ç”¨")
    print(f"   - ã‚ˆã‚Šé•·æœŸé–“ã§ã®è¨ˆç®—")
    print(f"   - å¤–ã‚Œå€¤ã®é™¤å»")
    
    print(f"\n2. æ–°ã—ã„ç‰¹å¾´é‡ã®é–‹ç™º:")
    print(f"   - é«˜æ€§èƒ½ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›")
    print(f"   - éç·šå½¢å¤‰æ›ã®é©ç”¨")
    print(f"   - æ™‚é–“é…ã‚Œã®è€ƒæ…®")
    
    print(f"\n3. ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã®æœ€é©åŒ–:")
    print(f"   - ä½æ€§èƒ½ã‚°ãƒ«ãƒ¼ãƒ—ã®è¦‹ç›´ã—")
    print(f"   - é«˜æ€§èƒ½ã‚°ãƒ«ãƒ¼ãƒ—ã®æ‹¡å¼µ")
    print(f"   - ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ç›¸äº’ä½œç”¨")
    
    return {
        'unstable_features': unstable_features,
        'stable_high_performers': stable_high_performers,
        'multi_method_features': multi_method_features,
    }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("AutoMLç‰¹å¾´é‡ã®æ·±åº¦åˆ†æ")
    print("=" * 60)
    
    try:
        # 1. åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        df = generate_comprehensive_test_data(720)  # 30æ—¥åˆ†
        
        # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        crypto_features = EnhancedCryptoFeatures()
        
        # 3. ç‰¹å¾´é‡å®‰å®šæ€§åˆ†æ
        stability_scores = analyze_feature_stability(df, crypto_features)
        
        # 4. ç‰¹å¾´é‡ç›¸äº’ä½œç”¨åˆ†æ
        interaction_results = analyze_feature_interactions(df, crypto_features)
        
        # 5. ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—æ€§èƒ½åˆ†æ
        group_performance = analyze_feature_groups_performance(df, crypto_features)
        
        # 6. æ”¹å–„æ©Ÿä¼šã®ç‰¹å®š
        improvement_opportunities = identify_improvement_opportunities(
            stability_scores, interaction_results, group_performance
        )
        
        print("\n" + "=" * 60)
        print("ğŸ¯ åˆ†æå®Œäº†")
        print("è©³ç´°ãªæ”¹å–„ææ¡ˆã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚")
        
    except Exception as e:
        print(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
