"""
ç‰¹å¾´é‡å‰Šæ¸›å¾Œã®æ€§èƒ½è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

19å€‹ã®ç‰¹å¾´é‡ã‚’å‰Šé™¤ã—ã€79å€‹ã‹ã‚‰60å€‹ã«å‰Šæ¸›ã—ãŸå¾Œã®æ€§èƒ½ã‚’æ¯”è¼ƒè©•ä¾¡ã—ã¾ã™ã€‚
å‰Šé™¤å‰å¾Œã§åŒã˜ãƒ‡ãƒ¼ã‚¿ã€åŒã˜ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦å…¬å¹³ãªæ¯”è¼ƒã‚’è¡Œã„ã¾ã™ã€‚

å®Ÿè¡Œæ–¹æ³•:
    cd backend
    python scripts/evaluate_feature_reduction.py

å‡ºåŠ›:
    - ã‚³ãƒ³ã‚½ãƒ¼ãƒ«: è©³ç´°ãªæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ
    - CSV: backend/feature_reduction_evaluation.csv
"""

import logging
import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.config.unified_config import unified_config
from app.services.ml.feature_engineering.feature_engineering_service import (
    FeatureEngineeringService,
)
from app.services.ml.models.lightgbm import LightGBMModel
# ãƒ©ãƒ™ãƒ«ç”Ÿæˆã¯ç°¡æ˜“çš„ãªå®Ÿè£…ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸è¦
from database.connection import SessionLocal
from database.repositories.ohlcv_repository import OHLCVRepository

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class FeatureReductionEvaluator:
    """ç‰¹å¾´é‡å‰Šæ¸›ã®æ€§èƒ½è©•ä¾¡ã‚¯ãƒ©ã‚¹"""

    def __init__(self, min_samples: int = 1500):
        """
        åˆæœŸåŒ–

        Args:
            min_samples: æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
        """
        self.min_samples = min_samples
        self.feature_service = FeatureEngineeringService()
        
        # å‰Šé™¤ã•ã‚ŒãŸç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆï¼ˆ19å€‹ï¼‰
        self.removed_features = [
            # é«˜ç›¸é–¢ã«ã‚ˆã‚‹å‰Šé™¤(5å€‹)
            "macd",
            "Stochastic_K",
            "Near_Resistance",
            "MA_Long",
            "BB_Position",
            # ä½é‡è¦åº¦ã«ã‚ˆã‚‹å‰Šé™¤(14å€‹)
            "close_lag_24",
            "cumulative_returns_24",
            "Close_mean_20",
            "Local_Max",
            "Aroon_Up",
            "BB_Lower",
            "Resistance_Level",
            "BB_Middle",
            "stochastic_k",
            "rsi_14",
            "bb_lower_20",
            "bb_upper_20",
            "stochastic_d",
            "Local_Min",
        ]

    def load_data(
        self, symbol: str = "BTC/USDT:USDT", timeframe: str = "1h"
    ) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿

        Args:
            symbol: å–å¼•ãƒšã‚¢
            timeframe: æ™‚é–“è»¸

        Returns:
            OHLCVãƒ‡ãƒ¼ã‚¿ã®DataFrame
        """
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {symbol} {timeframe}")

        db = SessionLocal()
        try:
            repo = OHLCVRepository(db)
            df = repo.get_ohlcv_dataframe(
                symbol=symbol, timeframe=timeframe, limit=self.min_samples + 500
            )

            if df.empty:
                raise ValueError(f"ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {symbol} {timeframe}")

            if len(df) < self.min_samples:
                raise ValueError(
                    f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(df)}ä»¶ (å¿…è¦: {self.min_samples}ä»¶ä»¥ä¸Š)"
                )

            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
            return df

        finally:
            db.close()

    def generate_features(
        self, ohlcv_data: pd.DataFrame, use_allowlist: bool = True
    ) -> pd.DataFrame:
        """
        ç‰¹å¾´é‡ã‚’ç”Ÿæˆ

        Args:
            ohlcv_data: OHLCVãƒ‡ãƒ¼ã‚¿
            use_allowlist: allowlistã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆFalse=å…¨ç‰¹å¾´é‡ï¼‰

        Returns:
            ç‰¹å¾´é‡DataFrame
        """
        if use_allowlist:
            logger.info("ğŸ”§ ç‰¹å¾´é‡ç”Ÿæˆé–‹å§‹ï¼ˆallowlisté©ç”¨: 60å€‹ï¼‰")
        else:
            logger.info("ğŸ”§ ç‰¹å¾´é‡ç”Ÿæˆé–‹å§‹ï¼ˆå…¨ç‰¹å¾´é‡: 79å€‹ï¼‰")

        # allowlistã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã™ã‚‹å ´åˆ
        original_allowlist = None
        if not use_allowlist:
            original_allowlist = unified_config.ml.feature_engineering.feature_allowlist
            unified_config.ml.feature_engineering.feature_allowlist = None

        try:
            features = self.feature_service.calculate_advanced_features(
                ohlcv_data=ohlcv_data
            )

            logger.info(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(features.columns)}å€‹")
            return features

        finally:
            # allowlistã‚’å…ƒã«æˆ»ã™
            if not use_allowlist and original_allowlist is not None:
                unified_config.ml.feature_engineering.feature_allowlist = (
                    original_allowlist
                )

    def generate_labels(self, df: pd.DataFrame) -> pd.Series:
        """
        ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªä¾¡æ ¼å¤‰å‹•ãƒ™ãƒ¼ã‚¹ï¼‰

        Args:
            df: ç‰¹å¾´é‡DataFrame

        Returns:
            ãƒ©ãƒ™ãƒ«Series (0: DOWN, 1: RANGE, 2: UP)
        """
        logger.info("ğŸ·ï¸ ãƒ©ãƒ™ãƒ«ç”Ÿæˆé–‹å§‹")

        # æ¬¡ã®æœŸé–“ã®ä¾¡æ ¼å¤‰å‹•ç‡ã‚’è¨ˆç®—ï¼ˆ4æœ¬å…ˆã‚’è¦‹ã‚‹ï¼‰
        horizon = 4
        future_returns = df["close"].pct_change(horizon).shift(-horizon)

        # é–¾å€¤ã‚’è¨­å®šï¼ˆå¤‰å‹•ç‡ã®æ¨™æº–åå·®ã®0.5å€ï¼‰
        threshold = future_returns.std() * 0.5

        # 3ã‚¯ãƒ©ã‚¹ã«åˆ†é¡
        labels = pd.Series(index=df.index, dtype=int)
        labels[future_returns > threshold] = 2  # UP
        labels[future_returns < -threshold] = 0  # DOWN
        labels[
            (future_returns >= -threshold) & (future_returns <= threshold)
        ] = 1  # RANGE

        # NaNã‚’é™¤å»
        valid_mask = labels.notna() & future_returns.notna()
        labels = labels[valid_mask]

        logger.info(f"âœ… ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†: {len(labels)}ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {dict(labels.value_counts().sort_index())}")
        logger.info(f"é–¾å€¤: Â±{threshold:.4f} ({threshold*100:.2f}%)")

        return labels

    def prepare_data(
        self, features: pd.DataFrame, labels: pd.Series
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆåˆ†å‰²ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰

        Args:
            features: ç‰¹å¾´é‡DataFrame
            labels: ãƒ©ãƒ™ãƒ«Series

        Returns:
            (X_train, X_val, y_train, y_val)
        """
        logger.info("âš™ï¸ ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹")

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æƒãˆã‚‹
        common_index = features.index.intersection(labels.index)
        features = features.loc[common_index]
        labels = labels.loc[common_index]

        # åŸºæœ¬ã‚«ãƒ©ãƒ ã‚’é™¤å¤–
        exclude_cols = ["open", "high", "low", "close", "volume"]
        feature_cols = [col for col in features.columns if col not in exclude_cols]
        X = features[feature_cols].copy()

        # ç„¡é™å€¤ã¨NaNã‚’å‡¦ç†
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())

        # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ï¼ˆ80:20ï¼‰
        X_train, X_val, y_train, y_val = train_test_split(
            X, labels, test_size=0.2, random_state=42, stratify=labels
        )

        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
        logger.info(f"  - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train):,}ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val):,}ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"  - ç‰¹å¾´é‡æ•°: {len(feature_cols)}å€‹")

        return X_train, X_val, y_train, y_val

    def train_and_evaluate(
        self,
        X_train: pd.DataFrame,
        X_val: pd.DataFrame,
        y_train: pd.Series,
        y_val: pd.Series,
        label: str,
    ) -> Dict:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦è©•ä¾¡

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_val: æ¤œè¨¼ç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_val: æ¤œè¨¼ç”¨ãƒ©ãƒ™ãƒ«
            label: è©•ä¾¡ãƒ©ãƒ™ãƒ«ï¼ˆ"å‰Šé™¤å‰" or "å‰Šé™¤å¾Œ"ï¼‰

        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¤– {label}ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»è©•ä¾¡")
        logger.info(f"{'='*60}")

        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = LightGBMModel(random_state=42, n_estimators=100, learning_rate=0.1)

        # å­¦ç¿’æ™‚é–“è¨ˆæ¸¬
        logger.info("å­¦ç¿’ã‚’é–‹å§‹...")
        train_start = time.time()
        training_result = model._train_model_impl(X_train, X_val, y_train, y_val)
        train_time = time.time() - train_start

        # äºˆæ¸¬æ™‚é–“è¨ˆæ¸¬
        logger.info("äºˆæ¸¬ã‚’å®Ÿè¡Œ...")
        predict_start = time.time()
        y_pred_proba = model.predict_proba(X_val)
        predict_time = time.time() - predict_start

        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        result = {
            "label": label,
            "feature_count": len(X_train.columns),
            "train_samples": len(X_train),
            "val_samples": len(X_val),
            "train_time": train_time,
            "predict_time": predict_time,
            **training_result,
        }

        # çµæœã‚’è¡¨ç¤º
        logger.info(f"\nğŸ“Š {label}ã®è©•ä¾¡çµæœ:")
        logger.info(f"  - ç‰¹å¾´é‡æ•°: {result['feature_count']}å€‹")
        logger.info(f"  - Accuracy: {result.get('accuracy', 0.0):.4f}")
        logger.info(f"  - Precision: {result.get('precision', 0.0):.4f}")
        logger.info(f"  - Recall: {result.get('recall', 0.0):.4f}")
        logger.info(f"  - F1-Score: {result.get('f1_score', 0.0):.4f}")
        logger.info(f"  - AUC-ROC: {result.get('roc_auc', 0.0):.4f}")
        logger.info(f"  - å­¦ç¿’æ™‚é–“: {train_time:.2f}ç§’")
        logger.info(f"  - äºˆæ¸¬æ™‚é–“: {predict_time:.4f}ç§’")

        return result

    def compare_results(
        self, before_result: Dict, after_result: Dict
    ) -> pd.DataFrame:
        """
        çµæœã‚’æ¯”è¼ƒ

        Args:
            before_result: å‰Šé™¤å‰ã®çµæœ
            after_result: å‰Šé™¤å¾Œã®çµæœ

        Returns:
            æ¯”è¼ƒçµæœã®DataFrame
        """
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š çµæœæ¯”è¼ƒ")
        logger.info(f"{'='*60}")

        # æ¯”è¼ƒã™ã‚‹æŒ‡æ¨™
        metrics = [
            "feature_count",
            "accuracy",
            "precision",
            "recall",
            "f1_score",
            "roc_auc",
            "train_time",
            "predict_time",
        ]

        comparison_data = []
        for metric in metrics:
            before_val = before_result.get(metric, 0.0)
            after_val = after_result.get(metric, 0.0)

            # å¤‰åŒ–ç‡ã‚’è¨ˆç®—
            if before_val != 0:
                change_pct = ((after_val - before_val) / before_val) * 100
            else:
                change_pct = 0.0

            comparison_data.append(
                {
                    "metric": metric,
                    "before": before_val,
                    "after": after_val,
                    "change": after_val - before_val,
                    "change_pct": change_pct,
                }
            )

        comparison_df = pd.DataFrame(comparison_data)
        return comparison_df

    def print_report(
        self, comparison_df: pd.DataFrame, total_samples: int, class_distribution: Dict
    ) -> None:
        """
        è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›

        Args:
            comparison_df: æ¯”è¼ƒçµæœDataFrame
            total_samples: ç·ã‚µãƒ³ãƒ—ãƒ«æ•°
            class_distribution: ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        """
        print("\n" + "=" * 80)
        print("ç‰¹å¾´é‡å‰Šæ¸›ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)

        print("\nã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‘")
        print(f"- ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples:,}ä»¶")
        train_samples = int(
            comparison_df[comparison_df["metric"] == "feature_count"]["before"].iloc[0]
        )
        val_samples = total_samples - train_samples
        print(f"- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {train_samples:,}ä»¶")
        print(f"- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {val_samples:,}ä»¶")

        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’è¡¨ç¤º
        class_str = ", ".join(
            [f"ã‚¯ãƒ©ã‚¹{k}={v}" for k, v in sorted(class_distribution.items())]
        )
        print(f"- ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {class_str}")

        # å‰Šé™¤å‰ã®çµæœ
        print("\nã€å‰Šé™¤å‰ã€‘ç‰¹å¾´é‡æ•°: 79å€‹")
        for _, row in comparison_df.iterrows():
            if row["metric"] == "feature_count":
                continue
            if row["metric"] in ["train_time", "predict_time"]:
                print(f"- {row['metric']}: {row['before']:.2f}ç§’")
            else:
                print(f"- {row['metric']}: {row['before']:.4f}")

        # å‰Šé™¤å¾Œã®çµæœ
        print("\nã€å‰Šé™¤å¾Œã€‘ç‰¹å¾´é‡æ•°: 60å€‹")
        for _, row in comparison_df.iterrows():
            if row["metric"] == "feature_count":
                continue

            change_sign = "+" if row["change_pct"] >= 0 else ""
            if row["metric"] in ["train_time", "predict_time"]:
                print(
                    f"- {row['metric']}: {row['after']:.2f}ç§’ "
                    f"({change_sign}{row['change_pct']:.1f}%)"
                )
            else:
                print(
                    f"- {row['metric']}: {row['after']:.4f} "
                    f"({change_sign}{row['change_pct']:.2f}%)"
                )

        # å‰Šé™¤ã•ã‚ŒãŸç‰¹å¾´é‡
        print(f"\nã€å‰Šé™¤ã•ã‚ŒãŸç‰¹å¾´é‡ã€‘19å€‹:")
        for i, feature in enumerate(self.removed_features, 1):
            print(f"  {i:2d}. {feature}")

        # çµè«–
        print("\nã€çµè«–ã€‘")

        # æ€§èƒ½å¤‰åŒ–ã‚’åˆ¤å®š
        avg_performance_change = comparison_df[
            comparison_df["metric"].isin(
                ["accuracy", "precision", "recall", "f1_score", "roc_auc"]
            )
        ]["change_pct"].mean()

        if abs(avg_performance_change) < 1.0:
            performance_status = "ç¶­æŒ"
            recommendation = "å‰Šé™¤ã‚’æ¨å¥¨"
        elif avg_performance_change > 0:
            performance_status = "æ”¹å–„"
            recommendation = "å‰Šé™¤ã‚’å¼·ãæ¨å¥¨"
        else:
            performance_status = "ä½ä¸‹"
            if abs(avg_performance_change) < 5.0:
                recommendation = "å‰Šé™¤ã‚’æ¨å¥¨ï¼ˆæ€§èƒ½ä½ä¸‹ã¯è¨±å®¹ç¯„å›²å†…ï¼‰"
            else:
                recommendation = "å‰Šé™¤ã‚’éæ¨å¥¨"

        print(f"- äºˆæ¸¬æ€§èƒ½: {performance_status} (å¹³å‡å¤‰åŒ–ç‡: {avg_performance_change:+.2f}%)")

        # å­¦ç¿’é€Ÿåº¦æ”¹å–„
        train_time_change = comparison_df[comparison_df["metric"] == "train_time"][
            "change_pct"
        ].iloc[0]
        print(f"- å­¦ç¿’é€Ÿåº¦: {abs(train_time_change):.1f}%æ”¹å–„")

        # æ¨å¥¨
        print(f"- æ¨å¥¨: {recommendation}")

        print("\nè©³ç´°çµæœ: backend/feature_reduction_evaluation.csv")
        print("=" * 80)

    def save_results(
        self, comparison_df: pd.DataFrame, output_path: str = "backend/feature_reduction_evaluation.csv"
    ) -> None:
        """
        çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

        Args:
            comparison_df: æ¯”è¼ƒçµæœDataFrame
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        comparison_df.to_csv(output_path, index=False, encoding="utf-8")
        logger.info(f"âœ… çµæœã‚’CSVã«ä¿å­˜: {output_path}")

    def run(
        self, symbol: str = "BTC/USDT:USDT", timeframe: str = "1h"
    ) -> pd.DataFrame:
        """
        è©•ä¾¡ã‚’å®Ÿè¡Œ

        Args:
            symbol: å–å¼•ãƒšã‚¢
            timeframe: æ™‚é–“è»¸

        Returns:
            æ¯”è¼ƒçµæœDataFrame
        """
        try:
            logger.info("\nğŸš€ ç‰¹å¾´é‡å‰Šæ¸›ã®æ€§èƒ½è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™\n")

            # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            ohlcv_data = self.load_data(symbol, timeframe)

            # 2. å‰Šé™¤å‰ã®ç‰¹å¾´é‡ç”Ÿæˆï¼ˆå…¨ç‰¹å¾´é‡ï¼‰
            features_before = self.generate_features(ohlcv_data, use_allowlist=False)

            # 3. ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
            labels = self.generate_labels(features_before)

            # 4. å‰Šé™¤å‰ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨è©•ä¾¡
            X_train_before, X_val_before, y_train, y_val = self.prepare_data(
                features_before, labels
            )
            before_result = self.train_and_evaluate(
                X_train_before, X_val_before, y_train, y_val, "å‰Šé™¤å‰ï¼ˆ79å€‹ï¼‰"
            )

            # 5. å‰Šé™¤å¾Œã®ç‰¹å¾´é‡ç”Ÿæˆï¼ˆallowlisté©ç”¨ï¼‰
            features_after = self.generate_features(ohlcv_data, use_allowlist=True)

            # 6. å‰Šé™¤å¾Œã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨è©•ä¾¡ï¼ˆåŒã˜ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ï¼‰
            X_train_after, X_val_after, y_train_after, y_val_after = self.prepare_data(
                features_after, labels
            )
            after_result = self.train_and_evaluate(
                X_train_after, X_val_after, y_train_after, y_val_after, "å‰Šé™¤å¾Œï¼ˆ60å€‹ï¼‰"
            )

            # 7. çµæœæ¯”è¼ƒ
            comparison_df = self.compare_results(before_result, after_result)

            # 8. ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
            total_samples = len(X_train_before) + len(X_val_before)
            class_distribution = dict(y_train.value_counts().sort_index())
            self.print_report(comparison_df, total_samples, class_distribution)

            # 9. CSVä¿å­˜
            self.save_results(comparison_df)

            logger.info("\nâœ… è©•ä¾¡ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            return comparison_df

        except Exception as e:
            logger.error(f"\nâŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("=" * 80)
    logger.info("ç‰¹å¾´é‡å‰Šæ¸›æ€§èƒ½è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    logger.info("=" * 80)

    evaluator = FeatureReductionEvaluator(min_samples=1500)

    try:
        results = evaluator.run()
        print("\nâœ… åˆ†æãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
        print("è©³ç´°çµæœ: backend/feature_reduction_evaluation.csv")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()