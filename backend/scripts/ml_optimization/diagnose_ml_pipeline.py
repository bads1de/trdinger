"""
ML Pipeline è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç¾åœ¨ã®MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å•é¡Œç‚¹ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªè¨ºæ–­ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import logging
import sys
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from app.services.ml.common.common_data import CommonData
from app.services.ml.label_generation.label_generation_service import (
    LabelGenerationService,
)
from app.services.ml.feature_engineering.feature_engineering_service import (
    FeatureEngineeringService,
    DEFAULT_FEATURE_ALLOWLIST,
)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class MLPipelineDiagnostics:
    """MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨ºæ–­ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.label_service = LabelGenerationService()
        self.feature_service = FeatureEngineeringService()
        self.common_data = CommonData()

    def diagnose_all(self, symbol: str = "BTC/USDT:USDT", timeframe: str = "1h"):
        """å…¨è¨ºæ–­ã‚’å®Ÿè¡Œ"""
        logger.info("=" * 80)
        logger.info("ML Pipeline åŒ…æ‹¬è¨ºæ–­é–‹å§‹")
        logger.info("=" * 80)

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        logger.info(f"\n1. ãƒ‡ãƒ¼ã‚¿å–å¾—: {symbol} {timeframe}")
        ohlcv_data, fr_data, oi_data = self._fetch_data(symbol, timeframe)

        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆã¨è¨ºæ–­
        logger.info("\n2. ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨ºæ–­")
        labels_df = self._diagnose_labels(ohlcv_data, fr_data, oi_data)

        # ç‰¹å¾´é‡è¨ºæ–­
        logger.info("\n3. ç‰¹å¾´é‡è¨ºæ–­")
        features_df = self._diagnose_features(ohlcv_data, fr_data, oi_data)

        # æ™‚ç³»åˆ—è¨ºæ–­
        logger.info("\n4. æ™‚ç³»åˆ—è¨ºæ–­")
        self._diagnose_temporal_patterns(ohlcv_data, labels_df)

        # æ¨å¥¨äº‹é …
        logger.info("\n5. æ¨å¥¨äº‹é …")
        self._generate_recommendations(labels_df, features_df, ohlcv_data)

        logger.info("\n" + "=" * 80)
        logger.info("è¨ºæ–­å®Œäº†")
        logger.info("=" * 80)

    def _fetch_data(self, symbol: str, timeframe: str):
        """ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        data = self.common_data.prepare_data(
            symbol=symbol, timeframe=timeframe, limit=50000
        )

        logger.info(f"  OHLCV: {len(data.ohlcv)} rows")
        logger.info(f"  æœŸé–“: {data.ohlcv.index[0]} ï½ {data.ohlcv.index[-1]}")
        logger.info(f"  FR: {len(data.fr) if data.fr is not None else 0} rows")
        logger.info(f"  OI: {len(data.oi) if data.oi is not None else 0} rows")

        return data.ohlcv, data.fr, data.oi

    def _diagnose_labels(self, ohlcv_data, fr_data, oi_data):
        """ãƒ©ãƒ™ãƒ«ç”Ÿæˆã‚’è¨ºæ–­"""

        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
        labels_result = self.label_service.generate_labels(ohlcv_data, fr_data, oi_data)

        labels_df = labels_result["labels"]
        t_events = labels_result["t_events"]

        logger.info(f"\n  === CUSUMã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡º ===")
        logger.info(f"  æ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆæ•°: {len(t_events)}")
        logger.info(f"  å…¨ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(ohlcv_data)}")
        logger.info(f"  ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºç‡: {len(t_events) / len(ohlcv_data) * 100:.2f}%")

        # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
        logger.info(f"\n  === ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ ===")
        label_counts = labels_df["label"].value_counts()
        total_labels = len(labels_df.dropna())

        for label_val, count in label_counts.items():
            percentage = (count / total_labels) * 100
            logger.info(f"  {label_val}: {count} ({percentage:.2f}%)")

        # ãƒ©ãƒ™ãƒ«å“è³ªæŒ‡æ¨™
        logger.info(f"\n  === ãƒ©ãƒ™ãƒ«å“è³ªæŒ‡æ¨™ ===")

        # å®Ÿéš›ã®ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—ï¼ˆæ¬¡ã®12ãƒãƒ¼ã®ä¾¡æ ¼å¤‰åŒ–ï¼‰
        future_returns = ohlcv_data["close"].pct_change(12).shift(-12)

        # ãƒ©ãƒ™ãƒ«=UPã®å®Ÿéš›ã®ãƒªã‚¿ãƒ¼ãƒ³
        up_mask = labels_df["label"] == "UP"
        if up_mask.sum() > 0:
            up_returns = future_returns[up_mask].dropna()
            logger.info(f"  UP ãƒ©ãƒ™ãƒ«ã®å®Ÿéš›ã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³: {up_returns.mean():.4f}")
            logger.info(
                f"  UP ãƒ©ãƒ™ãƒ«ã§å®Ÿéš›ã«ä¸Šæ˜‡ã—ãŸæ¯”ç‡: {(up_returns > 0).sum() / len(up_returns) * 100:.2f}%"
            )

        # ãƒ©ãƒ™ãƒ«=DOWNã®å®Ÿéš›ã®ãƒªã‚¿ãƒ¼ãƒ³
        down_mask = labels_df["label"] == "DOWN"
        if down_mask.sum() > 0:
            down_returns = future_returns[down_mask].dropna()
            logger.info(f"  DOWN ãƒ©ãƒ™ãƒ«ã®å®Ÿéš›ã®å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³: {down_returns.mean():.4f}")
            logger.info(
                f"  DOWN ãƒ©ãƒ™ãƒ«ã§å®Ÿéš›ã«ä¸‹é™ã—ãŸæ¯”ç‡: {(down_returns < 0).sum() / len(down_returns) * 100:.2f}%"
            )

        return labels_df

    def _diagnose_features(self, ohlcv_data, fr_data, oi_data):
        """ç‰¹å¾´é‡ã‚’è¨ºæ–­"""

        # ç‰¹å¾´é‡è¨ˆç®—
        features_df = self.feature_service.calculate_advanced_features(
            ohlcv_data, fr_data, oi_data
        )

        logger.info(f"\n  === ç‰¹å¾´é‡åŸºæœ¬æƒ…å ± ===")
        logger.info(f"  ç”Ÿæˆç‰¹å¾´é‡æ•°: {len(features_df.columns)}")
        logger.info(f"  Allowlistç‰¹å¾´é‡æ•°: {len(DEFAULT_FEATURE_ALLOWLIST)}")

        # Allowlistå†…ã®ç‰¹å¾´é‡ã®å­˜åœ¨ç¢ºèª
        missing_features = [
            f for f in DEFAULT_FEATURE_ALLOWLIST if f not in features_df.columns
        ]
        if missing_features:
            logger.warning(
                f"  âš ï¸ Allowlistã«å«ã¾ã‚Œã‚‹ãŒç”Ÿæˆã•ã‚Œã¦ã„ãªã„ç‰¹å¾´é‡: {len(missing_features)}"
            )
            logger.warning(f"    {missing_features[:5]}")  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º

        # æ¬ æå€¤è¨ºæ–­
        logger.info(f"\n  === æ¬ æå€¤è¨ºæ–­ ===")
        null_counts = features_df[DEFAULT_FEATURE_ALLOWLIST].isnull().sum()
        high_null_features = null_counts[
            null_counts > len(features_df) * 0.1
        ]  # 10%ä»¥ä¸Šæ¬ æ

        if len(high_null_features) > 0:
            logger.warning(f"  âš ï¸ æ¬ æå€¤ãŒ10%ä»¥ä¸Šã®ç‰¹å¾´é‡: {len(high_null_features)}")
            for feat, null_count in high_null_features.items():
                logger.warning(
                    f"    {feat}: {null_count / len(features_df) * 100:.2f}%"
                )
        else:
            logger.info(f"  âœ… ã™ã¹ã¦ã®ç‰¹å¾´é‡ã®æ¬ æå€¤ã¯10%æœªæº€")

        # åˆ†æ•£è¨ºæ–­
        logger.info(f"\n  === åˆ†æ•£è¨ºæ–­ ===")
        variances = features_df[DEFAULT_FEATURE_ALLOWLIST].var()
        zero_var_features = variances[variances < 1e-10]

        if len(zero_var_features) > 0:
            logger.warning(f"  âš ï¸ åˆ†æ•£ãŒã»ã¼ã‚¼ãƒ­ã®ç‰¹å¾´é‡: {len(zero_var_features)}")
            logger.warning(f"    {list(zero_var_features.index[:5])}")
        else:
            logger.info(f"  âœ… ã™ã¹ã¦ã®ç‰¹å¾´é‡ã«ååˆ†ãªåˆ†æ•£ã‚ã‚Š")

        return features_df

    def _diagnose_temporal_patterns(self, ohlcv_data, labels_df):
        """æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨ºæ–­"""

        logger.info(f"\n  === å¹´åˆ¥ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ ===")

        # ãƒ‡ãƒ¼ã‚¿ã‚’å¹´ã”ã¨ã«åˆ†å‰²
        ohlcv_data["year"] = ohlcv_data.index.year
        labels_with_year = labels_df.copy()
        labels_with_year["year"] = labels_df.index.year

        for year in sorted(labels_with_year["year"].unique()):
            year_labels = labels_with_year[labels_with_year["year"] == year]["label"]
            if len(year_labels) > 0:
                up_pct = (year_labels == "UP").sum() / len(year_labels) * 100
                down_pct = (year_labels == "DOWN").sum() / len(year_labels) * 100
                range_pct = (year_labels == "RANGE").sum() / len(year_labels) * 100

                logger.info(
                    f"  {year}: UP={up_pct:.1f}%, DOWN={down_pct:.1f}%, RANGE={range_pct:.1f}%  (n={len(year_labels)})"
                )

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è¨ºæ–­
        logger.info(f"\n  === å¹´åˆ¥ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ ===")
        for year in sorted(ohlcv_data["year"].unique()):
            year_data = ohlcv_data[ohlcv_data["year"] == year]
            returns = year_data["close"].pct_change()
            volatility = returns.std() * np.sqrt(365 * 24)  # å¹´ç‡æ›ç®—
            logger.info(f"  {year}: {volatility:.4f}")

    def _generate_recommendations(self, labels_df, features_df, ohlcv_data):
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""

        recommendations = []

        # 1. ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºç‡ãƒã‚§ãƒƒã‚¯
        event_rate = len(labels_df.dropna()) / len(ohlcv_data)
        if event_rate > 0.2:
            recommendations.append(
                f"âš ï¸ CUSUMã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºç‡ãŒé«˜ã™ãã¾ã™ ({event_rate*100:.1f}%)ã€‚"
                f"cusum_vol_multiplier ã‚’ 2.0 ï½ 3.0 ã«å¼•ãä¸Šã’ã‚‹ã“ã¨ã‚’æ¨å¥¨ã€‚"
            )
        elif event_rate < 0.05:
            recommendations.append(
                f"âš ï¸ CUSUMã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºç‡ãŒä½ã™ãã¾ã™ ({event_rate*100:.1f}%)ã€‚"
                f"å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            )

        # 2. ãƒ©ãƒ™ãƒ«ä¸å‡è¡¡ãƒã‚§ãƒƒã‚¯
        label_counts = labels_df["label"].value_counts()
        total = label_counts.sum()

        for label_val, count in label_counts.items():
            pct = count / total
            if pct < 0.1:
                recommendations.append(
                    f"âš ï¸ '{label_val}' ãƒ©ãƒ™ãƒ«ãŒæ¥µç«¯ã«å°‘ãªã„ ({pct*100:.1f}%)ã€‚"
                    f"ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«ã‚ˆã‚Šå­¦ç¿’ãŒå›°é›£ãªå¯èƒ½æ€§ã€‚"
                )

        # 3. ãƒ‡ãƒ¼ã‚¿æœŸé–“ãƒã‚§ãƒƒã‚¯
        data_start = ohlcv_data.index[0]
        data_end = ohlcv_data.index[-1]
        data_span_years = (data_end - data_start).days / 365.25

        if data_span_years > 3:
            recommendations.append(
                f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿æœŸé–“ãŒé•·ã™ãã¾ã™ ({data_span_years:.1f}å¹´)ã€‚"
                f"å¸‚å ´ç’°å¢ƒã®å¤‰åŒ–ã‚’è€ƒæ…®ã—ã€ç›´è¿‘1ï½2å¹´ã«é™å®šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã€‚"
            )

        # æ¨å¥¨äº‹é …ã‚’è¡¨ç¤º
        logger.info("\n  === æ¨å¥¨æ”¹å–„ç­– ===")
        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                logger.info(f"  {i}. {rec}")
        else:
            logger.info("  âœ… æ˜ã‚‰ã‹ãªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

        logger.info("\n  === æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ===")
        logger.info("  1. CUSUMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ï¼ˆvol_multiplierï¼‰")
        logger.info("  2. ãƒ‡ãƒ¼ã‚¿æœŸé–“ã®é™å®šï¼ˆç›´è¿‘1ï½2å¹´ï¼‰")
        logger.info("  3. Triple Barrierãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ï¼ˆpt/slæ¯”ï¼‰")


if __name__ == "__main__":
    diagnostics = MLPipelineDiagnostics()
    diagnostics.diagnose_all()



