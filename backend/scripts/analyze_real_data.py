#!/usr/bin/env python3
"""
å®Ÿéš›ã®OHLCVã€FGã€OIã€FRãƒ‡ãƒ¼ã‚¿ã®åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ãƒ‡ãƒ¼ã‚¿ã®æœŸé–“ä¸ä¸€è‡´ã€æ¬ æå€¤ã€ç‰¹å¾´é‡ã®æœ‰åŠ¹æ€§ã‚’åˆ†æã—ã¾ã™ã€‚
"""

import sys
import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from database.connection import get_db
from database.repositories.ohlcv_repository import OHLCVRepository
from database.repositories.open_interest_repository import OpenInterestRepository
from database.repositories.funding_rate_repository import FundingRateRepository
from database.repositories.fear_greed_repository import FearGreedIndexRepository
from app.services.backtest.backtest_data_service import BacktestDataService


def analyze_data_coverage():
    """ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’åˆ†æ"""
    print("=== ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ ===")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    db = next(get_db())

    try:
        # ãƒªãƒã‚¸ãƒˆãƒªåˆæœŸåŒ–
        ohlcv_repo = OHLCVRepository(db)
        oi_repo = OpenInterestRepository(db)
        fr_repo = FundingRateRepository(db)
        fg_repo = FearGreedIndexRepository(db)

        # åˆ†ææœŸé–“ï¼ˆéå»30æ—¥ï¼‰
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)
        symbol = "BTC/USDT"

        print(
            f"åˆ†ææœŸé–“: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}"
        )
        print(f"å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«: {symbol}")

        # å„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ä»¶æ•°ã‚’ç¢ºèª
        ohlcv_data = ohlcv_repo.get_ohlcv_data(
            symbol=symbol, timeframe="1h", start_time=start_date, end_time=end_date
        )
        oi_data = oi_repo.get_open_interest_data(
            symbol=symbol, start_time=start_date, end_time=end_date
        )
        fr_data = fr_repo.get_funding_rate_data(
            symbol=symbol, start_time=start_date, end_time=end_date
        )
        fg_data = fg_repo.get_fear_greed_data(start_time=start_date, end_time=end_date)

        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°:")
        print(f"  OHLCV (1h): {len(ohlcv_data)}ä»¶")
        print(f"  Open Interest: {len(oi_data)}ä»¶")
        print(f"  Funding Rate: {len(fr_data)}ä»¶")
        print(f"  Fear & Greed: {len(fg_data)}ä»¶")

        # æœŸé–“åˆ†æ
        if ohlcv_data:
            ohlcv_start = min(d.timestamp for d in ohlcv_data)
            ohlcv_end = max(d.timestamp for d in ohlcv_data)
            print(f"\nğŸ“ˆ OHLCVæœŸé–“: {ohlcv_start} - {ohlcv_end}")

        if oi_data:
            oi_start = min(d.timestamp for d in oi_data)
            oi_end = max(d.timestamp for d in oi_data)
            print(f"ğŸ“Š OIæœŸé–“: {oi_start} - {oi_end}")

        if fr_data:
            fr_start = min(d.timestamp for d in fr_data)
            fr_end = max(d.timestamp for d in fr_data)
            print(f"ğŸ’° FRæœŸé–“: {fr_start} - {fr_end}")

        if fg_data:
            fg_start = min(d.data_timestamp for d in fg_data)
            fg_end = max(d.data_timestamp for d in fg_data)
            print(f"ğŸ˜¨ FGæœŸé–“: {fg_start} - {fg_end}")

        return {
            "ohlcv": ohlcv_data,
            "oi": oi_data,
            "fr": fr_data,
            "fg": fg_data,
        }

    finally:
        db.close()


def analyze_data_integration():
    """ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†ã®åˆ†æ"""
    print("\n=== ãƒ‡ãƒ¼ã‚¿çµ±åˆåˆ†æ ===")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    db = next(get_db())

    try:
        # BacktestDataServiceã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        service = BacktestDataService(db)

        # åˆ†ææœŸé–“ï¼ˆéå»7æ—¥ï¼‰
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)
        symbol = "BTC/USDT"
        timeframe = "1h"

        print(f"çµ±åˆãƒ‡ãƒ¼ã‚¿å–å¾—: {symbol} {timeframe}")
        print(
            f"æœŸé–“: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}"
        )

        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        df = service.get_data_for_backtest(
            symbol=symbol, timeframe=timeframe, start_date=start_date, end_date=end_date
        )

        print(f"\nğŸ“Š çµ±åˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
        print(f"ğŸ“Š ã‚«ãƒ©ãƒ : {list(df.columns)}")

        # æ¬ æå€¤åˆ†æ
        print(f"\nğŸ” æ¬ æå€¤åˆ†æ:")
        missing_analysis = df.isnull().sum()
        for col, missing_count in missing_analysis.items():
            missing_pct = (missing_count / len(df)) * 100
            print(f"  {col}: {missing_count}ä»¶ ({missing_pct:.1f}%)")

        # ãƒ‡ãƒ¼ã‚¿å‹åˆ†æ
        print(f"\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿å‹:")
        for col, dtype in df.dtypes.items():
            print(f"  {col}: {dtype}")

        # çµ±è¨ˆæƒ…å ±
        print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        stats = df[numeric_cols].describe()
        print(stats)

        return df

    finally:
        db.close()


def analyze_feature_effectiveness(df: pd.DataFrame):
    """ç‰¹å¾´é‡ã®æœ‰åŠ¹æ€§ã‚’åˆ†æ"""
    print("\n=== ç‰¹å¾´é‡æœ‰åŠ¹æ€§åˆ†æ ===")

    if df.empty:
        print("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’ä½œæˆï¼ˆæ¬¡ã®æ™‚é–“ã®ä¾¡æ ¼å¤‰å‹•ç‡ï¼‰
    if "Close" in df.columns:
        df["target"] = df["Close"].pct_change().shift(-1)
    else:
        print("Closeã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # æ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ã‚’é¸æŠ
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [col for col in numeric_cols if col != "target"]

    print(f"åˆ†æå¯¾è±¡ç‰¹å¾´é‡: {len(numeric_cols)}å€‹")

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã®ç›¸é–¢åˆ†æ
    correlations = []
    for col in numeric_cols:
        if df[col].notna().sum() > 10:  # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒ10å€‹ä»¥ä¸Š
            corr = df[col].corr(df["target"])
            if not pd.isna(corr):
                correlations.append(
                    {
                        "feature": col,
                        "correlation": abs(corr),
                        "correlation_raw": corr,
                        "valid_count": df[col].notna().sum(),
                        "missing_pct": (df[col].isna().sum() / len(df)) * 100,
                    }
                )

    # ç›¸é–¢ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    correlations = sorted(correlations, key=lambda x: x["correlation"], reverse=True)

    print(f"\nğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã®ç›¸é–¢ï¼ˆä¸Šä½20ä½ï¼‰:")
    for i, item in enumerate(correlations[:20]):
        print(
            f"  {i+1:2d}. {item['feature']:<30} "
            f"ç›¸é–¢: {item['correlation_raw']:+.4f} "
            f"(æœ‰åŠ¹: {item['valid_count']:4d}ä»¶, "
            f"æ¬ æ: {item['missing_pct']:5.1f}%)"
        )

    # ä½ç›¸é–¢ç‰¹å¾´é‡
    print(f"\nâŒ ä½ç›¸é–¢ç‰¹å¾´é‡ï¼ˆç›¸é–¢<0.01ï¼‰:")
    low_corr = [item for item in correlations if item["correlation"] < 0.01]
    for item in low_corr[:10]:
        print(f"  - {item['feature']:<30} ç›¸é–¢: {item['correlation_raw']:+.4f}")

    # æ¬ æå€¤ã®å¤šã„ç‰¹å¾´é‡
    print(f"\nâš ï¸  æ¬ æå€¤ã®å¤šã„ç‰¹å¾´é‡ï¼ˆ>50%ï¼‰:")
    high_missing = [item for item in correlations if item["missing_pct"] > 50]
    for item in high_missing:
        print(f"  - {item['feature']:<30} æ¬ æ: {item['missing_pct']:5.1f}%")

    return correlations


def analyze_data_frequency():
    """ãƒ‡ãƒ¼ã‚¿é »åº¦ã®åˆ†æ"""
    print("\n=== ãƒ‡ãƒ¼ã‚¿é »åº¦åˆ†æ ===")

    db = next(get_db())

    try:
        # å„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®æ›´æ–°é »åº¦ã‚’åˆ†æ
        ohlcv_repo = OHLCVRepository(db)
        oi_repo = OpenInterestRepository(db)
        fr_repo = FundingRateRepository(db)
        fg_repo = FearGreedIndexRepository(db)

        symbol = "BTC/USDT"
        end_date = datetime.now()
        start_date = end_date - timedelta(days=3)  # 3æ—¥é–“ã§åˆ†æ

        # OHLCVï¼ˆ1æ™‚é–“ï¼‰
        ohlcv_data = ohlcv_repo.get_ohlcv_data(
            symbol=symbol, timeframe="1h", start_time=start_date, end_time=end_date
        )
        if ohlcv_data:
            ohlcv_times = [d.timestamp for d in ohlcv_data]
            ohlcv_intervals = [
                (ohlcv_times[i] - ohlcv_times[i - 1]).total_seconds() / 3600
                for i in range(1, len(ohlcv_times))
            ]
            print(
                f"ğŸ“ˆ OHLCVé–“éš”: å¹³å‡ {np.mean(ohlcv_intervals):.2f}æ™‚é–“, "
                f"æ¨™æº–åå·® {np.std(ohlcv_intervals):.2f}æ™‚é–“"
            )

        # Open Interest
        oi_data = oi_repo.get_open_interest_data(
            symbol=symbol, start_time=start_date, end_time=end_date
        )
        if oi_data and len(oi_data) > 1:
            oi_times = [d.timestamp for d in oi_data]
            oi_intervals = [
                (oi_times[i] - oi_times[i - 1]).total_seconds() / 3600
                for i in range(1, len(oi_times))
            ]
            print(
                f"ğŸ“Š OIé–“éš”: å¹³å‡ {np.mean(oi_intervals):.2f}æ™‚é–“, "
                f"æ¨™æº–åå·® {np.std(oi_intervals):.2f}æ™‚é–“"
            )

        # Funding Rate
        fr_data = fr_repo.get_funding_rate_data(
            symbol=symbol, start_time=start_date, end_time=end_date
        )
        if fr_data and len(fr_data) > 1:
            fr_times = [d.timestamp for d in fr_data]
            fr_intervals = [
                (fr_times[i] - fr_times[i - 1]).total_seconds() / 3600
                for i in range(1, len(fr_times))
            ]
            print(
                f"ğŸ’° FRé–“éš”: å¹³å‡ {np.mean(fr_intervals):.2f}æ™‚é–“, "
                f"æ¨™æº–åå·® {np.std(fr_intervals):.2f}æ™‚é–“"
            )

        # Fear & Greed
        fg_data = fg_repo.get_fear_greed_data(start_time=start_date, end_time=end_date)
        if fg_data and len(fg_data) > 1:
            fg_times = [d.data_timestamp for d in fg_data]
            fg_intervals = [
                (fg_times[i] - fg_times[i - 1]).total_seconds() / 3600
                for i in range(1, len(fg_times))
            ]
            print(
                f"ğŸ˜¨ FGé–“éš”: å¹³å‡ {np.mean(fg_intervals):.2f}æ™‚é–“, "
                f"æ¨™æº–åå·® {np.std(fg_intervals):.2f}æ™‚é–“"
            )

    finally:
        db.close()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹")
    print("=" * 50)

    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        data_sources = analyze_data_coverage()

        # ãƒ‡ãƒ¼ã‚¿çµ±åˆåˆ†æ
        integrated_df = analyze_data_integration()

        # ç‰¹å¾´é‡æœ‰åŠ¹æ€§åˆ†æ
        if integrated_df is not None and not integrated_df.empty:
            correlations = analyze_feature_effectiveness(integrated_df)

        # ãƒ‡ãƒ¼ã‚¿é »åº¦åˆ†æ
        analyze_data_frequency()

        print("\n" + "=" * 50)
        print("ğŸ“Š åˆ†æå®Œäº†")

        # æ¨å¥¨äº‹é …
        print("\nğŸ¯ æ¨å¥¨äº‹é …:")
        print("1. æ¬ æå€¤ã®å¤šã„ç‰¹å¾´é‡ã¯é™¤å¤–ã¾ãŸã¯æ”¹å–„ãŒå¿…è¦")
        print("2. ä½ç›¸é–¢ç‰¹å¾´é‡ã¯ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§æ”¹å–„")
        print("3. ãƒ‡ãƒ¼ã‚¿é »åº¦ã®é•ã„ã‚’è€ƒæ…®ã—ãŸè£œé–“æ–¹æ³•ã®æœ€é©åŒ–")
        print("4. ã‚ˆã‚ŠåŠ¹æœçš„ãªç‰¹å¾´é‡ã®é–‹ç™º")

    except Exception as e:
        print(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
