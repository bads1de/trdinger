#!/usr/bin/env python3
"""
æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ

æ·±åº¦åˆ†æã®çµæœã«åŸºã¥ã„ã¦æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ã®åŠ¹æœã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import sys
import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.services.ml.feature_engineering.enhanced_crypto_features import EnhancedCryptoFeatures
from app.services.ml.feature_engineering.optimized_crypto_features import OptimizedCryptoFeatures
from app.services.ml.feature_engineering.enhanced_feature_engineering_service import EnhancedFeatureEngineeringService

def generate_realistic_market_data(hours: int = 720) -> pd.DataFrame:
    """
    ãƒªã‚¢ãƒ«ãªå¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆ30æ—¥åˆ†ï¼‰
    """
    print(f"ãƒªã‚¢ãƒ«ãªå¸‚å ´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {hours}æ™‚é–“åˆ†")
    
    start_time = datetime.now() - timedelta(hours=hours)
    timestamps = [start_time + timedelta(hours=i) for i in range(hours)]
    
    # è¤‡é›‘ãªå¸‚å ´å‹•å‘ã‚’æ¨¡æ“¬
    np.random.seed(42)
    
    # è¤‡æ•°ã®ã‚µã‚¤ã‚¯ãƒ«
    weekly_cycle = np.sin(np.arange(hours) * 2 * np.pi / (24 * 7)) * 0.02
    daily_cycle = np.sin(np.arange(hours) * 2 * np.pi / 24) * 0.01
    
    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    volatility_regime = np.zeros(hours)
    current_vol = 0.02
    for i in range(hours):
        if np.random.random() < 0.03:  # 3%ã®ç¢ºç‡ã§ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰æ›´
            current_vol = np.random.choice([0.01, 0.02, 0.04], p=[0.3, 0.5, 0.2])
        volatility_regime[i] = current_vol
    
    # ä¾¡æ ¼ç”Ÿæˆ
    base_returns = weekly_cycle + daily_cycle
    noise_returns = np.random.normal(0, volatility_regime)
    returns = base_returns + noise_returns
    
    base_price = 50000
    prices = base_price * np.exp(np.cumsum(returns))
    
    # OHLCVç”Ÿæˆ
    data = []
    for i, (timestamp, close) in enumerate(zip(timestamps, prices)):
        vol = volatility_regime[i] * close
        high = close + np.random.exponential(vol * 0.3)
        low = close - np.random.exponential(vol * 0.3)
        
        if i == 0:
            open_price = close
        else:
            open_price = prices[i-1] + np.random.normal(0, vol * 0.1)
        
        volume = max(100, np.random.normal(1000 + abs(returns[i]) * 50000, 500))
        
        data.append({
            'timestamp': timestamp,
            'Open': open_price,
            'High': max(open_price, high, close),
            'Low': min(open_price, low, close),
            'Close': close,
            'Volume': volume,
        })
    
    df = pd.DataFrame(data)
    df.set_index('timestamp', inplace=True)
    
    # Open Interestï¼ˆ8æ™‚é–“ã”ã¨ï¼‰
    oi_base = 1000000
    oi_changes = []
    for i in range(0, hours, 8):
        if i == 0:
            oi_change = 0
        else:
            price_momentum = np.mean(returns[max(0, i-24):i])
            oi_change = price_momentum * 0.3 + np.random.normal(0, 0.01)
        oi_changes.append(oi_change)
    
    oi_values = oi_base * np.exp(np.cumsum(oi_changes))
    oi_timestamps = [start_time + timedelta(hours=i*8) for i in range(len(oi_values))]
    oi_series = pd.Series(oi_values, index=oi_timestamps)
    df['open_interest'] = oi_series.reindex(df.index, method='ffill')
    
    # Funding Rateï¼ˆ8æ™‚é–“ã”ã¨ï¼‰
    fr_values = []
    for i in range(len(oi_values)):
        if i == 0:
            price_trend = 0
        else:
            start_idx = max(0, i*8-24)
            end_idx = i*8
            price_trend = np.mean(returns[start_idx:end_idx])
        
        fr = np.clip(price_trend * 0.15 + np.random.normal(0, 0.0001), -0.01, 0.01)
        fr_values.append(fr)
    
    fr_series = pd.Series(fr_values, index=oi_timestamps)
    df['funding_rate'] = fr_series.reindex(df.index, method='ffill')
    
    # Fear & Greedï¼ˆ1æ—¥ã”ã¨ï¼‰
    daily_timestamps = [start_time + timedelta(days=i) for i in range(hours // 24 + 1)]
    fg_values = []
    for i in range(len(daily_timestamps)):
        if i == 0:
            daily_return = 0
            daily_volatility = 0.02
        else:
            start_idx = max(0, i*24-24)
            end_idx = min(hours-1, i*24)
            daily_return = np.sum(returns[start_idx:end_idx])
            daily_volatility = np.std(returns[start_idx:end_idx])
        
        fg_raw = 50 - daily_return * 1000 + (daily_volatility - 0.02) * 500
        fg = np.clip(fg_raw + np.random.normal(0, 2), 0, 100)
        fg_values.append(fg)
    
    fg_series = pd.Series(fg_values, index=daily_timestamps)
    df['fear_greed_value'] = fg_series.reindex(df.index, method='ffill')
    
    print(f"ç”Ÿæˆå®Œäº†: {len(df)}è¡Œ")
    return df

def compare_feature_engines():
    """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ³ã®æ¯”è¼ƒ"""
    print("\n=== ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ³æ¯”è¼ƒ ===")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    df = generate_realistic_market_data(720)  # 30æ—¥åˆ†
    
    print(f"åŸºæœ¬ãƒ‡ãƒ¼ã‚¿: {df.shape}")
    
    # 1. å¾“æ¥ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    print(f"\nğŸ“Š å¾“æ¥ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°:")
    enhanced_features = EnhancedCryptoFeatures()
    
    start_time = datetime.now()
    enhanced_df = enhanced_features.create_comprehensive_features(df)
    enhanced_time = (datetime.now() - start_time).total_seconds()
    
    print(f"  å‡¦ç†æ™‚é–“: {enhanced_time:.2f}ç§’")
    print(f"  ç‰¹å¾´é‡æ•°: {enhanced_df.shape[1]}å€‹")
    print(f"  è¿½åŠ ç‰¹å¾´é‡: {enhanced_df.shape[1] - df.shape[1]}å€‹")
    
    # 2. æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    print(f"\nğŸš€ æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°:")
    optimized_features = OptimizedCryptoFeatures()
    
    start_time = datetime.now()
    optimized_df = optimized_features.create_optimized_features(df)
    optimized_time = (datetime.now() - start_time).total_seconds()
    
    print(f"  å‡¦ç†æ™‚é–“: {optimized_time:.2f}ç§’")
    print(f"  ç‰¹å¾´é‡æ•°: {optimized_df.shape[1]}å€‹")
    print(f"  è¿½åŠ ç‰¹å¾´é‡: {optimized_df.shape[1] - df.shape[1]}å€‹")
    
    # 3. çµ±åˆç‰ˆï¼ˆä¸¡æ–¹ã‚’ä½¿ç”¨ï¼‰
    print(f"\nğŸ”¥ çµ±åˆç‰ˆï¼ˆä¸¡æ–¹ã‚’ä½¿ç”¨ï¼‰:")
    
    start_time = datetime.now()
    integrated_df = enhanced_features.create_comprehensive_features(df)
    integrated_df = optimized_features.create_optimized_features(integrated_df)
    integrated_time = (datetime.now() - start_time).total_seconds()
    
    print(f"  å‡¦ç†æ™‚é–“: {integrated_time:.2f}ç§’")
    print(f"  ç‰¹å¾´é‡æ•°: {integrated_df.shape[1]}å€‹")
    print(f"  è¿½åŠ ç‰¹å¾´é‡: {integrated_df.shape[1] - df.shape[1]}å€‹")
    
    return enhanced_df, optimized_df, integrated_df

def analyze_feature_quality(df: pd.DataFrame, name: str):
    """ç‰¹å¾´é‡å“è³ªã®åˆ†æ"""
    print(f"\n=== {name} å“è³ªåˆ†æ ===")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
    target = df['Close'].pct_change(4).shift(-4)
    
    # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿
    valid_mask = target.notna() & df.notna().all(axis=1)
    valid_count = valid_mask.sum()
    
    print(f"æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {valid_count}ä»¶ / {len(df)}ä»¶ ({valid_count/len(df)*100:.1f}%)")
    
    if valid_count < 50:
        print("æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        return {}
    
    # ç‰¹å¾´é‡ã®ç›¸é–¢åˆ†æ
    feature_cols = [col for col in df.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']]
    
    correlations = []
    for col in feature_cols:
        corr = df.loc[valid_mask, col].corr(target.loc[valid_mask])
        if not pd.isna(corr):
            correlations.append({
                'feature': col,
                'correlation': abs(corr),
                'correlation_raw': corr,
            })
    
    # ç›¸é–¢é †ã«ã‚½ãƒ¼ãƒˆ
    correlations.sort(key=lambda x: x['correlation'], reverse=True)
    
    # çµ±è¨ˆæƒ…å ±
    corr_values = [item['correlation'] for item in correlations]
    
    print(f"ç‰¹å¾´é‡çµ±è¨ˆ:")
    print(f"  ç·ç‰¹å¾´é‡æ•°: {len(correlations)}å€‹")
    print(f"  å¹³å‡ç›¸é–¢: {np.mean(corr_values):.4f}")
    print(f"  æœ€å¤§ç›¸é–¢: {np.max(corr_values):.4f}")
    print(f"  é«˜ç›¸é–¢ç‰¹å¾´é‡(>0.05): {sum(1 for c in corr_values if c > 0.05)}å€‹")
    print(f"  ä¸­ç›¸é–¢ç‰¹å¾´é‡(0.02-0.05): {sum(1 for c in corr_values if 0.02 <= c <= 0.05)}å€‹")
    print(f"  ä½ç›¸é–¢ç‰¹å¾´é‡(<0.02): {sum(1 for c in corr_values if c < 0.02)}å€‹")
    
    print(f"\nä¸Šä½10ç‰¹å¾´é‡:")
    for i, item in enumerate(correlations[:10]):
        print(f"  {i+1:2d}. {item['feature']:<40} ç›¸é–¢: {item['correlation_raw']:+.4f}")
    
    return {
        'total_features': len(correlations),
        'mean_correlation': np.mean(corr_values),
        'max_correlation': np.max(corr_values),
        'high_corr_count': sum(1 for c in corr_values if c > 0.05),
        'correlations': correlations,
    }

def test_feature_stability(df: pd.DataFrame, name: str):
    """ç‰¹å¾´é‡ã®å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ"""
    print(f"\n=== {name} å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ ===")
    
    # ç•°ãªã‚‹æœŸé–“ã§ã®ãƒ†ã‚¹ãƒˆ
    periods = [168, 336, 504, 720]  # 1é€±é–“ã€2é€±é–“ã€3é€±é–“ã€1ãƒ¶æœˆ
    stability_results = {}
    
    for period in periods:
        if period > len(df):
            continue
        
        period_df = df.iloc[-period:].copy()
        target = period_df['Close'].pct_change(4).shift(-4)
        
        valid_mask = target.notna() & period_df.notna().all(axis=1)
        if valid_mask.sum() < 30:
            continue
        
        feature_cols = [col for col in period_df.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']]
        
        correlations = {}
        for col in feature_cols:
            corr = period_df.loc[valid_mask, col].corr(target.loc[valid_mask])
            if not pd.isna(corr):
                correlations[col] = abs(corr)
        
        stability_results[period] = correlations
    
    if len(stability_results) < 2:
        print("å®‰å®šæ€§ãƒ†ã‚¹ãƒˆã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return {}
    
    # å®‰å®šæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
    common_features = set(stability_results[periods[0]].keys())
    for period in periods[1:]:
        if period in stability_results:
            common_features &= set(stability_results[period].keys())
    
    stability_scores = {}
    for feature in common_features:
        correlations = [stability_results[period][feature] for period in periods if period in stability_results]
        if len(correlations) >= 2:
            stability_score = np.mean(correlations) - np.std(correlations)
            stability_scores[feature] = {
                'stability_score': stability_score,
                'mean_correlation': np.mean(correlations),
                'correlation_std': np.std(correlations),
            }
    
    # å®‰å®šæ€§é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_features = sorted(stability_scores.items(), key=lambda x: x[1]['stability_score'], reverse=True)
    
    print(f"å®‰å®šæ€§ã®é«˜ã„ç‰¹å¾´é‡ï¼ˆä¸Šä½10ä½ï¼‰:")
    for i, (feature, scores) in enumerate(sorted_features[:10]):
        print(f"  {i+1:2d}. {feature:<40} "
              f"å®‰å®šæ€§: {scores['stability_score']:+.4f} "
              f"(å¹³å‡: {scores['mean_correlation']:.4f})")
    
    return stability_scores

def test_integrated_service():
    """çµ±åˆã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== çµ±åˆã‚µãƒ¼ãƒ“ã‚¹ãƒ†ã‚¹ãƒˆ ===")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    df = generate_realistic_market_data(240)  # 10æ—¥åˆ†
    target = df['Close'].pct_change(4).shift(-4)
    
    # çµ±åˆã‚µãƒ¼ãƒ“ã‚¹
    service = EnhancedFeatureEngineeringService()
    
    start_time = datetime.now()
    
    try:
        enhanced_df = service.calculate_enhanced_features(
            ohlcv_data=df,
            target=target,
            lookback_periods={
                'short': 4,
                'medium': 24,
                'long': 168,
                'extra_long': 336,
            }
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        print(f"âœ… çµ±åˆã‚µãƒ¼ãƒ“ã‚¹æˆåŠŸ:")
        print(f"  å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"  å…¥åŠ›ãƒ‡ãƒ¼ã‚¿: {df.shape}")
        print(f"  å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿: {enhanced_df.shape}")
        print(f"  è¿½åŠ ç‰¹å¾´é‡: {enhanced_df.shape[1] - df.shape[1]}å€‹")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ª
        missing_count = enhanced_df.isnull().sum().sum()
        inf_count = np.isinf(enhanced_df.select_dtypes(include=[np.number])).sum().sum()
        print(f"  ãƒ‡ãƒ¼ã‚¿å“è³ª: æ¬ æå€¤ {missing_count}å€‹, ç„¡é™å€¤ {inf_count}å€‹")
        
        return enhanced_df
        
    except Exception as e:
        print(f"âŒ çµ±åˆã‚µãƒ¼ãƒ“ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        # 1. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ³æ¯”è¼ƒ
        enhanced_df, optimized_df, integrated_df = compare_feature_engines()
        
        # 2. å“è³ªåˆ†æ
        enhanced_quality = analyze_feature_quality(enhanced_df, "å¾“æ¥ç‰ˆ")
        optimized_quality = analyze_feature_quality(optimized_df, "æœ€é©åŒ–ç‰ˆ")
        integrated_quality = analyze_feature_quality(integrated_df, "çµ±åˆç‰ˆ")
        
        # 3. å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ
        enhanced_stability = test_feature_stability(enhanced_df, "å¾“æ¥ç‰ˆ")
        optimized_stability = test_feature_stability(optimized_df, "æœ€é©åŒ–ç‰ˆ")
        
        # 4. çµ±åˆã‚µãƒ¼ãƒ“ã‚¹ãƒ†ã‚¹ãƒˆ
        service_result = test_integrated_service()
        
        print("\n" + "=" * 60)
        print("ğŸ¯ ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
        
        print(f"\nğŸ“Š å“è³ªæ¯”è¼ƒ:")
        print(f"  å¾“æ¥ç‰ˆ: å¹³å‡ç›¸é–¢ {enhanced_quality.get('mean_correlation', 0):.4f}, "
              f"é«˜ç›¸é–¢ç‰¹å¾´é‡ {enhanced_quality.get('high_corr_count', 0)}å€‹")
        print(f"  æœ€é©åŒ–ç‰ˆ: å¹³å‡ç›¸é–¢ {optimized_quality.get('mean_correlation', 0):.4f}, "
              f"é«˜ç›¸é–¢ç‰¹å¾´é‡ {optimized_quality.get('high_corr_count', 0)}å€‹")
        print(f"  çµ±åˆç‰ˆ: å¹³å‡ç›¸é–¢ {integrated_quality.get('mean_correlation', 0):.4f}, "
              f"é«˜ç›¸é–¢ç‰¹å¾´é‡ {integrated_quality.get('high_corr_count', 0)}å€‹")
        
        print(f"\nğŸ¯ æ”¹å–„åŠ¹æœ:")
        if enhanced_quality and optimized_quality:
            quality_improvement = (optimized_quality['mean_correlation'] - enhanced_quality['mean_correlation']) / enhanced_quality['mean_correlation'] * 100
            print(f"  å¹³å‡ç›¸é–¢æ”¹å–„: {quality_improvement:+.1f}%")
        
        if service_result is not None:
            print(f"  çµ±åˆã‚µãƒ¼ãƒ“ã‚¹: æ­£å¸¸å‹•ä½œ âœ…")
        else:
            print(f"  çµ±åˆã‚µãƒ¼ãƒ“ã‚¹: ã‚¨ãƒ©ãƒ¼ âŒ")
        
        print(f"\nâœ… æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼")
        
    except Exception as e:
        print(f"ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
