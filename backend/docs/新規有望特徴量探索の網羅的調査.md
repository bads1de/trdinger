# **次世代特徴量エンジニアリング：2024-2025年における構造的・生成的アプローチに関する包括的研究報告書**

## **1\. 序論：特徴量エンジニアリングのルネサンス**

機械学習のワークフローにおいて、特徴量エンジニアリング（Feature Engineering）は長らく「ブラックアート（熟練の職人芸）」と見なされてきた。モデルの性能を左右する決定的な要因でありながら、そのプロセスはデータサイエンティストの直感、ドメイン知識、そして膨大な試行錯誤に依存していたからである。しかし、2024年から2025年にかけての技術的進歩は、この領域に劇的なパラダイムシフトをもたらしている。従来の「手動による仮説検証」や「総当たり的な自動生成」を超え、大規模言語モデル（LLM）の推論能力を活用した**生成的特徴量エンジニアリング（Generative Feature Engineering）や、データ間の潜在的な関係性を明示的にモデル化するグラフベースの表現学習**、さらには物理的挙動や市場の微細構造（Microstructure）を捉える高度なドメイン特化型特徴量が台頭している。

本報告書は、最新の査読付き論文（arXiv, NeurIPS, ICML 2024-2025）、Kaggleなどのデータ分析コンペティションにおける上位解法、および先端的な産業応用事例を網羅的に調査・分析したものである。特に、従来の定型的な統計量（平均、分散など）では捉えきれない「意味的・構造的・物理的」な情報を抽出するための新規かつ有望な特徴量に焦点を当てる。これらの特徴量は、単なる予測精度の向上にとどまらず、モデルの解釈可能性やロバスト性（頑健性）を飛躍的に高める可能性を秘めている。

本稿では、LLMを進化論的オプティマイザとして再定義する**LLM-FE**、タブラーデータをグラフ構造として再解釈する**auGraph**、金融市場の歪みを数理的に捉える**Goto Conversion**や**Triplet Imbalance**、そして人間とボットを峻別する**行動バイオメトリクス**など、最先端の技術動向を詳述する。

## **2\. 大規模言語モデルによる生成的特徴量エンジニアリング (GenFE)**

特徴量エンジニアリングの自動化（AutoFE）はこれまで、四則演算や対数変換などの基本的な演算子を組み合わせる「探索的アプローチ」が主流であった。しかし、この手法は意味的に無意味な特徴量を大量に生成し、過学習や計算コストの増大を招くという課題があった。これに対し、2024年以降のトレンドは、LLMが持つ世界知識（World Knowledge）とコード生成能力を活用し、意味のある特徴量を「推論・生成」するアプローチへと移行している。

### **2.1 LLM-FE：進化論的オプティマイザとしてのLLM**

2025年に発表された**LLM-FE**（Large Language Model for Feature Engineering）フレームワークは、LLMを単なる知識ベースとしてではなく、進化的アルゴリズムにおける「知的変異オペレータ」として位置付けている点において革新的である 1。従来の手法（OpenFEやAutoFeatなど）が固定された演算子の組み合わせ空間を探索するのに対し、LLM-FEはPythonコードという表現力の高い空間内で、ドメイン知識に基づいた仮説生成と検証のループを回すことで、人間には想起困難な特徴量を発見する。

#### **2.1.1 進化的プロンプトエンジニアリングのメカニズム**

LLM-FEの中核をなすのは、反復的な最適化プロセスである。このプロセスは、FunSearchなどのアルゴリズム発見手法に触発されており、以下のステップで構成される 2。

1. **初期化とコンテキスト注入:** システムはまず、データセットのスキーマ（列名、データ型、各列の説明）と、データの分布を代表する少数のサンプル（Few-shot examples）をLLMに提示する。ここで重要なのは、単なる数値データだけでなく、各列が何を意味するか（例：「血糖値」「取引金額」）というメタデータを与えることである。これにより、LLMは数値の背後にある物理的・社会的意味を理解する 2。  
2. **仮説生成 (Hypothesis Generation):** LLMに対して、「ターゲット変数との相互情報量を最大化するような、新しい特徴量変換を行うPython関数」を生成するよう指示する。この際、プロンプトには「過去のイテレーションで高い検証スコアを記録した特徴量変換コード（In-Context Examples）」が含まれる。これにより、LLMは「何がうまくいったか」を学習し、成功したパターンの変種や、それを組み合わせた新しいアイデアを生成する 2。  
3. **データ駆動型評価 (Data-Driven Evaluation):** 生成されたコードは即座に実行され、トレーニングデータに新しい特徴量が追加される。その後、LightGBMやXGBoostなどの高速な勾配ブースティングモデルを用いて検証セットでのスコア（AUC, RMSE等）が算出される。このスコアは、その特徴量の「適応度（Fitness）」として扱われる 1。  
4. **進化と変異 (Evolution & Mutation):** 適応度の高い特徴量生成コードは「エリート個体」としてメモリバッファ（Experience Memory）に保存される。次回のイテレーションでは、これらのエリートコードがプロンプトに再注入され、LLMはそれらを改良（Mutate）したり、異なるコードのアイデアを交叉（Crossover）させたりして、より洗練された特徴量を探索する 2。

#### **2.1.2 構造化プロンプトによる推論の誘導**

LLM-FEが成功する鍵は、プロンプトの厳密な構造化にある。自由記述ではなく、JSON形式や特定のPython関数シグネチャ（例: def modify\_features(df):...）を強制することで、出力の解析エラーを防ぎ、実行可能なコードを確実に取得する。さらに、コード生成の前に「思考プロセス（Thought）」を出力させるChain-of-Thought（CoT）プロンプティングを適用することで、数理的な妥当性とドメイン知識の整合性を担保している 5。

例えば、糖尿病予測タスクにおいて、LLMは以下のような推論を行う：

Thought: インスリン抵抗性は糖尿病の重要なリスク因子である。医学的指標であるHOMA-IRは、空腹時血糖値とインスリン値の積に比例する。したがって、これら2つの変数の相互作用項を作成することで、モデルの予測能を向上させることができるはずだ。  
Code: df\['insulin\_resistance\_proxy'\] \= df\['glucose'\] \* df\['insulin'\] / 405

このように、LLM-FEは「なぜその特徴量が有効か」という解釈性を伴った特徴量を生成する点で、従来のブラックボックス的なAutoFEとは一線を画している。

### **2.2 OCTree：決定木推論によるフィードバックループ**

LLMの最大の弱点である「数値データの分布理解の欠如」を克服するために提案されたのが、**OCTree**（Optimizing Column feature generator with decision Tree reasoning）である 6。OCTreeは、LLMと従来の決定木モデル（CART）を協調させることで、データの実分布に即した特徴量生成を実現している。

#### **2.2.1 決定木からの「知識蒸留」とLLMへの伝達**

OCTreeのプロセスは、まず生の数値データを用いて浅い決定木（深さ3〜4程度）を学習させることから始まる。この決定木は、現在の特徴量空間におけるデータの分割ルール（Decision Boundaries）を表現している。OCTreeは、この決定木の構造を自然言語のルール記述に変換する 8。

例えば、決定木が Age \> 45 かつ Balance \< 1000 の領域で債務不履行率が高いというルールを見つけた場合、OCTreeはこれをテキスト化し、「現在のモデルは、年齢が45歳以上で残高が1000未満の層をリスクが高いと判断している」という現状認識としてLLMにフィードバックする。

#### **2.2.2 反復的なルール洗練 (Iterative Refinement)**

LLMはこのフィードバックを受け取り、「現在の決定木が捉えきれていない境界」や「より効率的にデータを分離できる新しい軸」を推論する。例えば、「年齢と残高の単純なAND条件ではなく、年齢に対する残高の比率（資産形成能力）の方が、リスクをより滑らかに表現できるのではないか」といった仮説を立て、Balance / Age という新しい特徴量を提案する。

生成された特徴量は再び決定木の学習に使用され、その結果（精度向上や新しい分割ルール）が再びLLMにフィードバックされる。この「決定木による現状分析」と「LLMによる仮説生成」のサイクルを繰り返すことで、OCTreeはデータの統計的性質と意味的性質の両方を考慮した特徴量空間を構築する 10。実験結果によれば、OCTreeはXGBoostやMLPの性能を一貫して向上させ、特にコンテキスト情報が希薄なデータセットにおいても、決定木の分割情報のみから有効な特徴量を発見できることが示されている 6。

### **2.3 CAAFE：意味論的メタデータの活用**

**CAAFE**（Context-Aware Automated Feature Engineering）は、データセットに含まれる「テキスト情報（列名、説明文）」を徹底的に活用するアプローチである 11。多くのタブラーデータには、列名以外にも「データ辞書」や「収集背景」といったメタデータが付随しているが、従来の手法はこれらを無視していた。

CAAFEは、このメタデータをプロンプトとしてLLMに入力し、「意味的事前分布（Semantic Priors）」に基づいた特徴量生成を行う。例えば、ある列が「緯度」と「経度」であることがわかれば、LLMは即座に地理的な距離計算（Haversine式）や、特定のランドマーク（病院、学校、中心市街地など）への距離、あるいはエリアのクラスタリングといった空間的特徴量を提案する。これは、数値の羅列からは決して導き出せない特徴量である。Kaggleのコンペティションにおいても、CAAFEが生成した特徴量は、ドメイン専門家が作成したものと同等、あるいはそれ以上の性能を発揮することが確認されている 11。

## **3\. タブラーデータのグラフ構造化と表現学習**

従来の機械学習モデル（GBDTやNN）は、データを行（インスタンス）ごとの独立したベクトルとして扱う。しかし、現実世界のデータには、行間（エンティティ間）や列間（属性間）に複雑な依存関係が存在する。2024-2025年の研究では、タブラーデータを**グラフ構造**に変換し、Graph Neural Networks（GNN）を用いてこれらの潜在的な関係性を学習する手法が注目されている 13。

### **3.1 auGraph：タスク認識型グラフ構築と属性プロモーション**

タブラーデータのグラフ化における最大の課題は、「どのようなトポロジー（接続構造）を構築するか」である。既存の手法は、外部キーや主キーに基づく自明な関係のみを利用していたが、これでは隠れた相関を捉えることはできない。VLDB 2025ワークショップで提案された**auGraph**フレームワークは、\*\*「属性プロモーション（Attribute Promotion）」\*\*という概念導入により、この問題を解決している 16。

#### **3.1.1 属性プロモーションのメカニズム**

属性プロモーションとは、特定の列（属性）の値そのものを、グラフ上の独立した「ノード」として昇格させる操作である。  
例えば、顧客データの City 列に "New York", "London", "Tokyo" という値が含まれているとする。auGraphは、これらの都市名をそれぞれノードとしてグラフに追加する。そして、"New York" に住むすべての顧客（行ノード）と、この "New York" 属性ノードをエッジで接続する。  
これにより、直接的な関係を持たない顧客同士であっても、"New York" という共通の属性ノードを介して間接的に接続されることになる。GNNのメッセージパッシング（Message Passing）機構において、情報は「顧客A → New Yorkノード → 顧客B」と伝播するため、同じ都市に住む顧客群の統計的特徴や隠れた傾向が、各顧客の埋め込みベクトル（Embedding）に集約される 16。

#### **3.1.2 スコアリング関数による最適構造の探索**

すべての属性を無差別にノード化すると、グラフが過密になり（Super-node問題）、計算コストが増大するだけでなく、意味のない情報伝播（Over-smoothing）が発生する。auGraphは、下流タスク（予測）への貢献度を定量化する**スコアリング関数**を用いて、プロモーションすべき属性を厳選する 16。

特に、\*\*カーディナリティ（固有値の数）\*\*とタスク関連性のバランスが重要である。

* **低カーディナリティ（例: 性別）:** 多くのノードが接続されすぎるため、情報が希釈されやすく、差別化要因としては弱い。  
* **高カーディナリティ（例: ID）:** 接続がスパースすぎて、情報の共有が起こらない。  
* **中カーディナリティ（例: 職業、地域）:** 適度なグループサイズを形成し、有意義なメッセージパッシングを促進するため、スコアが高くなる傾向がある。

auGraphは、この選択プロセスを自動化することで、データのスキーマ（構造）を保存しつつ、予測タスクに特化した最適なグラフ構造を動的に生成する。実験結果では、この手法により生成されたグラフを用いたGNNは、従来のフラットなタブラー学習モデルや、単純なヒューリスティックに基づくグラフ構築手法を一貫して上回る性能を示している 18。

### **3.2 GNNによる高次相互作用の抽出**

GNNをタブラーデータに適用することで得られる最大の特徴量は、\*\*高次相互作用（High-order Interactions）\*\*の表現である 14。  
GBDTなどの決定木ベースのモデルは、特徴量の組み合わせ（交互作用）を捉えるのが得意だが、それはあくまで「明示的な変数」の組み合わせに限られる。一方、GNNはグラフ構造上での多段ホップ（Multi-hop）の近傍情報を集約することで、以下のような複雑な関係性を数値化できる。

* **コミュニティ特徴量:** 「自分と同じ商品を頻繁に購入している他のユーザーたちが、最近購入し始めた商品カテゴリ」のような、協調フィルタリング的な情報を、個々のユーザーの特徴量として取り込むことができる。  
* **コンテキスト依存の埋め込み:** 同じ Age=30 という値であっても、それがどのような属性ノード（職業や地域）と接続しているかによって、最終的な埋め込みベクトルが変化する。これにより、文脈に応じた動的な表現学習が可能となる。

## **4\. 金融市場の微細構造と定量的特徴量 (Financial Microstructure Features)**

金融市場の予測、特に高頻度取引（HFT）や短期トレードの領域では、一般的なOHLC（始値、高値、安値、終値）データだけでは優位性（Alpha）を得ることが困難になっている。2024年のKaggleコンペティション「Optiver \- Trading at the Close」の上位解法や、最新のクオンツ研究からは、市場の\*\*微細構造（Microstructure）\*\*における不均衡や圧力を数理的に捉える特徴量が重要視されていることがわかる。

### **4.1 オーダーブック・インバランスと市場の緊急度**

板情報（Order Book）の深層にある需給バランスの崩れは、価格変動の先行指標となる。

#### **4.1.1 Market Urgency（市場の緊急度）**

価格が動く直前には、流動性の枯渇と注文の偏りが同時に発生することが多い。これを捉えるのが Market Urgency である 20。

$$\\text{Market Urgency} \= \\text{Price Spread} \\times \\text{Liquidity Imbalance}$$

ここで、各項は以下のように定義される：

$$\\text{Price Spread} \= \\text{Ask Price} \- \\text{Bid Price}$$

$$\\text{Liquidity Imbalance} \= \\frac{\\text{Bid Size} \- \\text{Ask Size}}{\\text{Bid Size} \+ \\text{Ask Size}}$$

スプレッドが拡大している（＝流動性が低い、またはボラティリティが高い）状態で、かつ買い注文と売り注文の量に大きな偏りがある場合、価格はインバランスの方向へ急激に動く可能性が高い。この特徴量は、単なるスプレッドやサイズ単体よりも、価格変動の「切迫度」を強力に表現する。

#### **4.1.2 Triplet Imbalance（三つ組不均衡）**

単一の価格ペアではなく、3つの異なる価格指標（例：最良気配値、中間気配値、遠方の気配値など）の関係性から、市場の歪みを検出する特徴量が Triplet Imbalance である 20。

$$\\text{Imbalance}(A, B, C) \= \\frac{\\max(A, B, C) \- \\text{mid}(A, B, C)}{\\text{mid}(A, B, C) \- \\min(A, B, C)}$$

この指標は、3つの価格が等間隔に並んでいるか、あるいは一方向に偏っているかを正規化されたスケールで測定する。例えば、最良売り気配（Ask）と最良買い気配（Bid）に対し、直近の約定価格（Last）がどちらに張り付いているか、あるいは参照価格（Reference Price）との乖離がどの程度かといった「相対的な位置関係」をロバストに捉えることができる。Optiverコンペティションの優勝チームは、数百の価格・サイズ変数の組み合わせに対してこのTriplet Imbalanceを計算し、Numbaを用いて並列処理することで強力なエッジを得た。

### **4.2 統計的裁定のための確率変換：Goto Conversion**

予測市場や金融市場には、「本命・大穴バイアス（Favorite-Longshot Bias）」と呼ばれる現象が存在する。これは、人気の高い（確率が高い）事象は過小評価され、人気の低い（確率が低い、リスクが高い）事象は過大評価される傾向を指す。このバイアスを補正し、真の確率分布や公正価格を推定するための手法として、**Goto Conversion** が2024年のKaggleコミュニティで注目を集めた 20。

#### **4.2.1 Zero-Sum Variant のメカニズム**

株式市場において、複数の銘柄の将来リターンを予測する際、Goto Conversionのゼロサム・バリアントは以下のロジックで予測値を調整する。

1. **標準誤差の推定:** 取引量（Volume）が少ない銘柄ほど、価格形成の不確実性（ノイズ）が大きいと仮定し、標準誤差を $\\sigma \\propto \\frac{1}{\\sqrt{\\text{Volume}}}$ （あるいは簡易的に $\\sqrt{\\text{Volume}}$ の逆数に関連する項）として見積もる。  
2. **一律調整ステップの計算:** 市場全体の整合性を保つため（例：インデックスに対する相対リターンの総和がゼロになるように）、全銘柄の予測値をその標準誤差の単位で一律にシフトさせる。  
   $$\\text{Step} \= \\frac{\\sum \\text{Predicted Prices}}{\\sum \\text{Standard Error}}$$  
3. **価格の補正:**  
   $$\\text{Adjusted Price}\_i \= \\text{Predicted Price}\_i \- \\text{Standard Error}\_i \\times \\text{Step}$$

この変換により、流動性が低くボラティリティの高い銘柄（大穴）の予測値は大きく補正され、流動性の高い銘柄（本命）は小さく補正される。結果として、市場のノイズを除去し、より堅牢なポートフォリオ構築に資する特徴量が生成される。

### **4.3 テクニカルパターンのアルゴリズム化：Fakeout Detection**

トレーディングにおいて、多くの参加者が騙される「ダマシ（Fakeout）」を検知することは、逆張りの絶好の機会となる。Pine Scriptなどのトレーディング言語で実装されているロジックをPythonの特徴量として再実装することで、機械学習モデルに「相場の罠」を学習させることができる 22。

#### **4.3.1 Volume Divergence（出来高ダイバージェンス）**

価格がレジスタンスラインを更新（ブレイクアウト）したにもかかわらず、出来高がそれに追随していない現象を捉える。

* **特徴量定義:** 直近 $N$ 期間の高値を更新した時点 $t$ において、  
  $$\\text{VolDiff ratio} \= \\frac{\\text{Volume}\_t}{\\text{SMA}(\\text{Volume}, N)}$$  
  もし価格が新高値をつけたのに $\\text{VolDiff ratio} \< 1.0$ （平均以下）であれば、そのブレイクアウトは「偽」である確率が高い。

#### **4.3.2 Weak Retest Decay（弱いリテストの減衰）**

ブレイクアウト後、価格がレジスタンスラインまで戻る（リテスト）挙動において、その「反発の弱さ」を定量化する。

* **Wick Rejection Ratio:** リテスト時のローソク足において、ラインに触れた部分の「ヒゲ（Wick）」の長さが実体（Body）に対してどれだけ長いか。長いヒゲは強い拒絶（反発）を意味するが、ヒゲが短く実体でラインを割り込む場合は「弱いリテスト（Fakeoutの確認）」となる。  
  $$\\text{Wick Ratio} \= \\frac{|\\text{High} \- \\max(\\text{Open}, \\text{Close})|}{|\\text{High} \- \\text{Low}|}$$  
  この値が閾値以下である場合、ブレイクアウト失敗（Fakeout）の特徴として扱う。

## **5\. 行動バイオメトリクスと物理的特徴量 (Behavioral Biometrics)**

不正検知（Fraud Detection）や本人認証の分野では、アクセス元のIPアドレスやデバイス情報といった静的な属性は容易に偽装されるようになっている。これに対抗するため、ユーザーの身体的・認知的な特性が反映される「操作の物理的挙動（Behavioral Biometrics）」を特徴量として利用する動きが、2025年に向けて加速している。これらは、AIボットがどれほど高度化しても模倣が困難な「人間らしさ」の痕跡である 25。

### **5.1 マウス・ダイナミクス：運動法則の適用**

マウスカーソルの軌跡データ $(x\_t, y\_t, t)$ からは、運動生理学に基づいた高度な特徴量が抽出できる 28。

| 特徴量カテゴリ | 具体的な特徴量定義 | 人間 vs ボットの差異 |
| :---- | :---- | :---- |
| **曲率とエントロピー** | **Curvature Change Rate:** 軌跡の曲率の変化率の統計量（平均、分散）。 **Trajectory Entropy:** 移動方向の分布のエントロピー。 | 人間の動きは滑らかだが不完全で、曲率は連続的に変化する。ボットは直線的か、あるいは数式（ベジェ曲線等）に基づく完璧すぎる曲線を描くため、エントロピーが極端に低いか、逆にランダムノイズによる不自然な高エントロピーを示す。 |
| **微細運動 (Jitter)** | **Tremor Magnitude:** 平滑化軌跡と実軌跡の差分。 **Micro-movements:** 停止状態におけるピクセル単位の微動。 | 人間には生理的な手の震え（Physiological Tremor）や、クリック時の微細なズレが必ず生じる。ボットにはこの物理的な「ノイズ」が欠如している。 |
| **効率性** | **Straightness Ratio (SR):** 実移動距離と始点・終点間ユークリッド距離の比。 $SR \= \\frac{\\sum d\_i}{\\text{Euclidean Dist}}$ | 人間は常に $SR \> 1$ となる（完全に真っ直ぐには動かせない）。ボットは $SR \\approx 1$ になりがちである。 |
| **運動法則** | **Fitts's Law Adherence:** ターゲットまでの距離と大きさに対する移動時間の適合度。 | 人間のポインティング動作はフィッツの法則に従う（遠く小さな目標ほど時間がかかる）。ボットはこの法則を無視した一定速度や瞬時移動を行うことが多い。 |

### **5.2 キーストローク・ダイナミクス**

キーボード入力のリズムもまた、個人の神経筋プロセスを反映する 31。

* **Flight Time (飛行時間):** キー $K\_1$ を離してから次のキー $K\_2$ を押すまでの時間間隔。  
* **Dwell Time (滞留時間):** キーを押し下げてから離すまでの時間。  
* **N-graph Latency:** 特定の文字の並び（Digraphs, Trigraphs）における入力速度。例えば、よく使う "th", "er", "ing" などの並びは、他のランダムな並びに比べて極端に速く入力される（熟練効果）。ボットはこの文脈依存の速度変化を模倣するのが難しい。  
* **修正行動の特徴:** Backspaceキーの使用頻度や、修正後の入力リズムの変化。人間はミスをすると一時的に入力が遅くなる（認知的負荷）が、ボットは一定のリズムを維持するか、そもそもミスをしない。

## **6\. 時系列基盤モデルによる埋め込み表現 (Time Series Foundation Models)**

自然言語処理におけるBERTやGPTの成功を受け、時系列データにおいても、大規模な事前学習済みモデル（Foundation Models）を用いて特徴抽出を行うアプローチが2025年のトレンドとなっている。

### **6.1 Chronos, MOMENT, CHARMによるゼロショット特徴抽出**

**Chronos**（Amazon）や**MOMENT**、**CHARM**といったモデルは、数百万〜数十億の多様な時系列データセットで事前学習されている 33。これらのモデルは、時系列データをトークン化（Tokenization）し、Transformerアーキテクチャを通してその文脈を学習する。

従来の特徴量エンジニアリングでは、移動平均やラグ、フーリエ変換などを手動で設計する必要があったが、これらの基盤モデルを用いることで、以下のプロセスで高度な特徴量（Embeddings）を抽出できる：

1. **トークン化と入力:** 対象の時系列データを正規化・量子化し、モデルに入力する。  
2. **エンコーダによる表現学習:** モデル内部のTransformerエンコーダが、局所的なパターン（トレンド、季節性）と大域的な依存関係を処理し、各タイムステップまたはセグメントごとの高次元ベクトル（Hidden States）を出力する。  
3. **特徴量としての利用:** この出力ベクトル（例: 768次元の埋め込み）を、そのままXGBoostやLightGBMなどの下流モデルの入力特徴量として使用する。

このアプローチの利点は、\*\*Zero-Shot（ゼロショット）\*\*で機能する点である。追加の学習を行わなくても、事前学習モデルは「一般的な時系列の挙動」を理解しているため、未知のデータセットからでも、ノイズに強く、将来の変動を示唆する抽象的な特徴を抽出できる。これにより、データ数が少ないタスクや、複雑な非線形性を持つデータにおいても、劇的な精度向上が期待できる。

## **7\. 高速自動特徴量生成フレームワーク：OpenFE**

LLMのような生成的アプローチとは対照的に、計算効率と決定論的な探索能力を極限まで高めたツールとして**OpenFE**が、実務的なデータサイエンスの現場で評価されている 37。

### **7.1 23の演算子による探索空間の定義**

OpenFEは、ランダムな探索ではなく、経験的に有効性が確認された\*\*23種類の演算子（Operators）\*\*を用いて候補特徴量を生成する。これらの演算子は、数値データだけでなく、カテゴリカルデータに対しても適用される。

| カテゴリ | 演算子の例 | 役割 |
| :---- | :---- | :---- |
| **Unary (単項)** | Log, Sqrt, Sigmoid, Residual, Abs | 分布の正規化、非線形変換、外れ値の影響緩和。 |
| **Binary (二項)** | \+, \-, \*, / | 基本的な相互作用。特に「比率」や「差分」はドメイン知識（利益率、成長率など）と直結しやすい。 |
| **Aggregation (集約)** | GroupByThenMean, GroupByThenMax, GroupByThenStd, GroupByThenRank | 「あるカテゴリ（例：ユーザーID）内での統計量」を計算。ターゲットエンコーディングの拡張版として強力。 |
| **Combination** | Combine (Cross Product) | 複数のカテゴリカル変数を結合し、より細かい粒度のグループを作成（例：Country x Device）。 |

### **7.2 効率的な剪定アルゴリズム (Pruning Strategy)**

数千から数万に及ぶ候補特徴量の中から、真に有効な数十個を選別するために、OpenFEは以下の2段階の剪定戦略を採用している。

1. **Successive Halving（逐次半減法）:** データセットの小さなサブセットを用いて多数の特徴量を高速に評価し、下位半分を捨てて残りをより多くのデータで評価するプロセスを繰り返す。これにより、計算リソースを有望な特徴量に集中させる。  
2. **Feature Boosting:** 残った候補特徴量を既存のモデルに追加し、勾配ブースティング（GBDT）のFeature Importance（重要度スコア）を計算する。ここで、既存の特徴量と比較して有意なゲインをもたらすものだけを最終的に採用する。

このアプローチにより、OpenFEは数時間の探索で、Kaggleの上位チームが数週間かけて手動で作成するのと同等レベルの特徴量セットを自動生成することが可能となっている。

## **8\. 結論：特徴量エンジニアリングの未来**

2024年から2025年にかけての特徴量エンジニアリングは、**「意味の理解（LLM）」と「構造の活用（Graph）」**、そして\*\*「物理法則の適用（Microstructure/Biometrics）」\*\*という3つのベクトルで急速に進化している。

* **LLM-FE**や**CAAFE**は、データセットのコンテキストを読み解き、人間のような推論プロセスを経て特徴量を生成することを可能にした。  
* **auGraph**や**GNN**は、フラットな表形式データの裏にあるリレーショナルな構造を顕在化させ、個々のデータポイントを孤立させずに分析する道を開いた。  
* **Goto Conversion**や**行動バイオメトリクス**は、特定ドメインにおける物理的・数理的な法則性を特徴量として組み込むことで、モデルの精度とロバスト性を極限まで高めている。

データサイエンティストやエンジニアは、これらの新しいツールと概念を積極的に取り入れることで、単なる「パラメータチューニング」の競争から脱却し、データの本質的な価値を引き出すより高次元なモデリングへと移行することが求められる。特徴量エンジニアリングはもはや前処理の一部ではなく、それ自体が高度な知能を要する独立した科学領域へと変貌を遂げたのである。

#### **引用文献**

1. LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/pdf/2503.14434](https://arxiv.org/pdf/2503.14434)  
2. LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/html/2503.14434v1](https://arxiv.org/html/2503.14434v1)  
3. nikhilsab/LLMFE: This is the official repo for the paper "LLM-FE" \- GitHub, 12月 22, 2025にアクセス、 [https://github.com/nikhilsab/LLMFE](https://github.com/nikhilsab/LLMFE)  
4. (PDF) Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive Ablation Study for Ensemble Classifiers \- ResearchGate, 12月 22, 2025にアクセス、 [https://www.researchgate.net/publication/385529229\_Enriching\_Tabular\_Data\_with\_Contextual\_LLM\_Embeddings\_A\_Comprehensive\_Ablation\_Study\_for\_Ensemble\_Classifiers](https://www.researchgate.net/publication/385529229_Enriching_Tabular_Data_with_Contextual_LLM_Embeddings_A_Comprehensive_Ablation_Study_for_Ensemble_Classifiers)  
5. Advanced Prompt Engineering for Data Science Projects, 12月 22, 2025にアクセス、 [https://towardsdatascience.com/advanced-prompt-engineering-for-data-science-projects/](https://towardsdatascience.com/advanced-prompt-engineering-for-data-science-projects/)  
6. Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning \- NIPS papers, 12月 22, 2025にアクセス、 [https://papers.nips.cc/paper\_files/paper/2024/file/a7ebe2e8d8cfd2fcec6cd77f9e6fd34d-Paper-Conference.pdf](https://papers.nips.cc/paper_files/paper/2024/file/a7ebe2e8d8cfd2fcec6cd77f9e6fd34d-Paper-Conference.pdf)  
7. \[Quick Review\] Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning \- Liner, 12月 22, 2025にアクセス、 [https://liner.com/review/optimized-feature-generation-for-tabular-data-via-llms-with-decision](https://liner.com/review/optimized-feature-generation-for-tabular-data-via-llms-with-decision)  
8. \[2406.08527\] Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/abs/2406.08527](https://arxiv.org/abs/2406.08527)  
9. Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning, 12月 22, 2025にアクセス、 [https://openreview.net/forum?id=APSBwuMopO¬eId=SqEcRAcGMw](https://openreview.net/forum?id=APSBwuMopO&noteId=SqEcRAcGMw)  
10. Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning, 12月 22, 2025にアクセス、 [https://arxiv.org/html/2406.08527v2](https://arxiv.org/html/2406.08527v2)  
11. Large Language Models Engineer Too Many Simple Features for Tabular Data \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/html/2410.17787v2](https://arxiv.org/html/2410.17787v2)  
12. How Usable is Automated Feature Engineering for Tabular Data? \- OpenReview, 12月 22, 2025にアクセス、 [https://openreview.net/pdf?id=xlUVOMtL3W](https://openreview.net/pdf?id=xlUVOMtL3W)  
13. \[2401.02143\] Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/abs/2401.02143](https://arxiv.org/abs/2401.02143)  
14. Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy & Directions, 12月 22, 2025にアクセス、 [https://arxiv.org/html/2401.02143v1](https://arxiv.org/html/2401.02143v1)  
15. Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions, 12月 22, 2025にアクセス、 [https://www.researchgate.net/publication/393289856\_Graph\_Neural\_Networks\_for\_Tabular\_Data\_Learning\_A\_Survey\_with\_Taxonomy\_and\_Directions](https://www.researchgate.net/publication/393289856_Graph_Neural_Networks_for_Tabular_Data_Learning_A_Survey_with_Taxonomy_and_Directions)  
16. From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs \- VLDB Endowment, 12月 22, 2025にアクセス、 [https://www.vldb.org/2025/Workshops/VLDB-Workshops-2025/TaDA/TaDA25\_5.pdf](https://www.vldb.org/2025/Workshops/VLDB-Workshops-2025/TaDA/TaDA25_5.pdf)  
17. From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/html/2506.02243v1](https://arxiv.org/html/2506.02243v1)  
18. From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/pdf/2506.02243](https://arxiv.org/pdf/2506.02243)  
19. \[2506.02243\] From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/abs/2506.02243](https://arxiv.org/abs/2506.02243)  
20. liyiyan128/optiver-trading-at-the-close: Optiver Quantitative ... \- GitHub, 12月 22, 2025にアクセス、 [https://github.com/liyiyan128/optiver-trading-at-the-close](https://github.com/liyiyan128/optiver-trading-at-the-close)  
21. Research on Financial Stock Market Prediction Based on the Hidden Quantum Markov Model \- MDPI, 12月 22, 2025にアクセス、 [https://www.mdpi.com/2227-7390/13/15/2505](https://www.mdpi.com/2227-7390/13/15/2505)  
22. How to identify and trade fakeouts – A complete trader's guide \- Equiti, 12月 22, 2025にアクセス、 [https://www.equiti.com/sc-en/news/trading-ideas/how-to-identify-and-trade-fakeouts-a-complete-traders-guide/](https://www.equiti.com/sc-en/news/trading-ideas/how-to-identify-and-trade-fakeouts-a-complete-traders-guide/)  
23. Breakout or Fakeout? How to Spot the Difference and Trade. \- TradingView, 12月 22, 2025にアクセス、 [https://www.tradingview.com/chart/ETHUSDT.P/MrLaMqF9-Breakout-or-Fakeout-How-to-Spot-the-Difference-and-Trade/](https://www.tradingview.com/chart/ETHUSDT.P/MrLaMqF9-Breakout-or-Fakeout-How-to-Spot-the-Difference-and-Trade/)  
24. 5 False Breakout Strategies for Traders \- LuxAlgo, 12月 22, 2025にアクセス、 [https://www.luxalgo.com/blog/5-false-breakout-strategies-for-traders/](https://www.luxalgo.com/blog/5-false-breakout-strategies-for-traders/)  
25. Fraud Analytics in 2025: Why Traditional Systems Fail \- CLEAR, 12月 22, 2025にアクセス、 [https://identity.clearme.com/post/fraud-analytics-2025-why-traditional-systems-fail](https://identity.clearme.com/post/fraud-analytics-2025-why-traditional-systems-fail)  
26. How can Behavioral Biometrics prevent fraud? \- Sardine, 12月 22, 2025にアクセス、 [https://www.sardine.ai/blog/how-can-behavioral-biometrics-prevent-fraud](https://www.sardine.ai/blog/how-can-behavioral-biometrics-prevent-fraud)  
27. Is Behavioral Biometrics the Future of Fraud Protection? \- PCBB, 12月 22, 2025にアクセス、 [https://www.pcbb.com/bid/2025-04-02-is-behavioral-biometrics-the-future-of-fraud-protection](https://www.pcbb.com/bid/2025-04-02-is-behavioral-biometrics-the-future-of-fraud-protection)  
28. Pointergeist's Human Cursor: Mouse Movement Generator for Human-Like Trajectories \- GitHub, 12月 22, 2025にアクセス、 [https://github.com/Pointergeist/Pointergeist-Human-cursor](https://github.com/Pointergeist/Pointergeist-Human-cursor)  
29. Adaptability of current keystroke and mouse behavioral biometric systems \- ResearchOnline@JCU, 12月 22, 2025にアクセス、 [https://researchonline.jcu.edu.au/89343/7/89343.pdf](https://researchonline.jcu.edu.au/89343/7/89343.pdf)  
30. Mouse Dynamics Behavioral Biometrics: A Survey \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/html/2208.09061v2](https://arxiv.org/html/2208.09061v2)  
31. Keystroke dynamics \- Wikipedia, 12月 22, 2025にアクセス、 [https://en.wikipedia.org/wiki/Keystroke\_dynamics](https://en.wikipedia.org/wiki/Keystroke_dynamics)  
32. What's Your Type? Keystroke Dynamics as Behavioral Biometrics \- ARATEK, 12月 22, 2025にアクセス、 [https://www.aratek.co/news/keystroke-dynamics-as-behavioral-biometrics](https://www.aratek.co/news/keystroke-dynamics-as-behavioral-biometrics)  
33. Meet CHARM: C3 AI's Foundation Embedding Model for Time Series, 12月 22, 2025にアクセス、 [https://c3.ai/blog/meet-charm-c3-ais-foundation-embedding-model-for-time-series/](https://c3.ai/blog/meet-charm-c3-ais-foundation-embedding-model-for-time-series/)  
34. \[2505.14543\] Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions \- arXiv, 12月 22, 2025にアクセス、 [https://arxiv.org/abs/2505.14543](https://arxiv.org/abs/2505.14543)  
35. Chronos: Time Series Foundation Model \- Emergent Mind, 12月 22, 2025にアクセス、 [https://www.emergentmind.com/topics/chronos-time-series-foundation-model](https://www.emergentmind.com/topics/chronos-time-series-foundation-model)  
36. Fast and accurate zero-shot forecasting with Chronos-Bolt and AutoGluon \- AWS, 12月 22, 2025にアクセス、 [https://aws.amazon.com/blogs/machine-learning/fast-and-accurate-zero-shot-forecasting-with-chronos-bolt-and-autogluon/](https://aws.amazon.com/blogs/machine-learning/fast-and-accurate-zero-shot-forecasting-with-chronos-bolt-and-autogluon/)  
37. OpenFE: automated feature generation with expert-level performance \- GitHub, 12月 22, 2025にアクセス、 [https://github.com/IIIS-Li-Group/OpenFE](https://github.com/IIIS-Li-Group/OpenFE)  
38. OpenFE: Automated Feature Generation with Expert-level Performance, 12月 22, 2025にアクセス、 [https://proceedings.mlr.press/v202/zhang23ay.html](https://proceedings.mlr.press/v202/zhang23ay.html)