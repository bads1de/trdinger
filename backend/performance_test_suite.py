#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§ã‚’è©³ç´°ã«æ¤œè¨¼ã—ã¾ã™ã€‚
- å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ€§èƒ½
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–
- å®Ÿè¡Œæ™‚é–“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼
"""

import sys
import os
import logging
import pandas as pd
import numpy as np
import time
import psutil
import gc
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, field
import threading
import multiprocessing

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    test_name: str
    data_size: int
    execution_time: float
    memory_usage_mb: float
    peak_memory_mb: float
    cpu_usage_percent: float
    throughput_records_per_sec: float
    success: bool
    error_message: str = ""
    additional_metrics: Dict[str, Any] = field(default_factory=dict)

class PerformanceTestSuite:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""
    
    def __init__(self):
        self.results: List[PerformanceMetrics] = []
        self.process = psutil.Process()
        
    def create_large_dataset(self, rows: int, complexity: str = "medium") -> pd.DataFrame:
        """å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        logger.info(f"ğŸ“Š {rows:,}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–‹å§‹ (è¤‡é›‘åº¦: {complexity})")
        
        np.random.seed(42)
        dates = pd.date_range('2020-01-01', periods=rows, freq='h')
        
        # åŸºæœ¬ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        base_price = 50000
        trend = np.linspace(0, 10000, rows)
        
        if complexity == "simple":
            volatility = np.random.normal(0, 500, rows)
        elif complexity == "medium":
            volatility = np.random.normal(0, 1000, rows) + np.sin(np.arange(rows) * 0.01) * 500
        else:  # complex
            volatility = (np.random.normal(0, 1500, rows) + 
                         np.sin(np.arange(rows) * 0.01) * 500 +
                         np.cos(np.arange(rows) * 0.005) * 300)
        
        close_prices = base_price + trend + volatility
        
        data = {
            'Open': close_prices + np.random.normal(0, 100, rows),
            'High': close_prices + np.abs(np.random.normal(200, 150, rows)),
            'Low': close_prices - np.abs(np.random.normal(200, 150, rows)),
            'Close': close_prices,
            'Volume': np.random.lognormal(10, 0.5, rows),
        }
        
        df = pd.DataFrame(data, index=dates)
        
        # ä¾¡æ ¼æ•´åˆæ€§ã‚’ç¢ºä¿
        df['High'] = df[['Open', 'Close', 'High']].max(axis=1)
        df['Low'] = df[['Open', 'Close', 'Low']].min(axis=1)
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {df.shape}, ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {df.memory_usage(deep=True).sum() / 1024**2:.2f}MB")
        return df
    
    def monitor_system_resources(self, duration: float) -> Dict[str, float]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’ç›£è¦–"""
        start_time = time.time()
        cpu_samples = []
        memory_samples = []
        
        while time.time() - start_time < duration:
            cpu_samples.append(psutil.cpu_percent(interval=0.1))
            memory_samples.append(self.process.memory_info().rss / 1024**2)
            time.sleep(0.1)
        
        return {
            'avg_cpu_percent': np.mean(cpu_samples),
            'max_cpu_percent': np.max(cpu_samples),
            'avg_memory_mb': np.mean(memory_samples),
            'peak_memory_mb': np.max(memory_samples)
        }
    
    def test_large_data_processing(self):
        """å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸš€ å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        data_sizes = [1000, 5000, 10000, 20000]
        
        for size in data_sizes:
            logger.info(f"ğŸ“Š {size:,}è¡Œãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            gc.collect()
            initial_memory = self.process.memory_info().rss / 1024**2
            
            start_time = time.time()
            
            try:
                # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                large_data = self.create_large_dataset(size, "medium")
                
                # MLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
                from app.services.ml.single_model.single_model_trainer import SingleModelTrainer
                
                trainer = SingleModelTrainer(model_type="lightgbm")
                
                # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚’é–‹å§‹
                monitor_thread = threading.Thread(
                    target=lambda: self.monitor_system_resources(60),
                    daemon=True
                )
                monitor_thread.start()
                
                # å­¦ç¿’å®Ÿè¡Œ
                result = trainer.train_model(
                    training_data=large_data,
                    save_model=False,
                    threshold_up=0.02,
                    threshold_down=-0.02
                )
                
                execution_time = time.time() - start_time
                final_memory = self.process.memory_info().rss / 1024**2
                memory_usage = final_memory - initial_memory
                
                # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
                throughput = size / execution_time
                
                self.results.append(PerformanceMetrics(
                    test_name=f"å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†_{size:,}è¡Œ",
                    data_size=size,
                    execution_time=execution_time,
                    memory_usage_mb=memory_usage,
                    peak_memory_mb=final_memory,
                    cpu_usage_percent=psutil.cpu_percent(),
                    throughput_records_per_sec=throughput,
                    success=True,
                    additional_metrics={
                        'accuracy': result.get('accuracy', 0),
                        'feature_count': result.get('feature_count', 0),
                        'model_size_mb': 0  # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã—ã¦ã„ãªã„ãŸã‚0
                    }
                ))
                
                logger.info(f"âœ… {size:,}è¡Œå‡¦ç†å®Œäº†: {execution_time:.2f}ç§’, {throughput:.1f}è¡Œ/ç§’")
                
            except Exception as e:
                execution_time = time.time() - start_time
                
                self.results.append(PerformanceMetrics(
                    test_name=f"å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†_{size:,}è¡Œ",
                    data_size=size,
                    execution_time=execution_time,
                    memory_usage_mb=0,
                    peak_memory_mb=0,
                    cpu_usage_percent=0,
                    throughput_records_per_sec=0,
                    success=False,
                    error_message=str(e)
                ))
                
                logger.error(f"âŒ {size:,}è¡Œå‡¦ç†å¤±æ•—: {e}")
    
    def test_memory_optimization(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        # ç•°ãªã‚‹è¤‡é›‘åº¦ã§ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¯”è¼ƒ
        complexities = ["simple", "medium", "complex"]
        data_size = 5000
        
        for complexity in complexities:
            logger.info(f"ğŸ§® è¤‡é›‘åº¦ '{complexity}' ã§ã®ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆ")
            
            gc.collect()
            initial_memory = self.process.memory_info().rss / 1024**2
            
            start_time = time.time()
            
            try:
                # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                test_data = self.create_large_dataset(data_size, complexity)
                
                # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
                from app.services.ml.feature_engineering.feature_engineering_service import FeatureEngineeringService
                
                fe_service = FeatureEngineeringService()
                features = fe_service.calculate_advanced_features(test_data)
                
                execution_time = time.time() - start_time
                final_memory = self.process.memory_info().rss / 1024**2
                memory_usage = final_memory - initial_memory
                
                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè¨ˆç®—
                data_memory = test_data.memory_usage(deep=True).sum() / 1024**2
                features_memory = features.memory_usage(deep=True).sum() / 1024**2
                
                self.results.append(PerformanceMetrics(
                    test_name=f"ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–_{complexity}",
                    data_size=data_size,
                    execution_time=execution_time,
                    memory_usage_mb=memory_usage,
                    peak_memory_mb=final_memory,
                    cpu_usage_percent=psutil.cpu_percent(),
                    throughput_records_per_sec=data_size / execution_time,
                    success=True,
                    additional_metrics={
                        'data_memory_mb': data_memory,
                        'features_memory_mb': features_memory,
                        'memory_efficiency': features_memory / data_memory,
                        'feature_count': len(features.columns)
                    }
                ))
                
                logger.info(f"âœ… {complexity}è¤‡é›‘åº¦å®Œäº†: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ {memory_usage:.2f}MB")
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                del test_data, features
                gc.collect()
                
            except Exception as e:
                execution_time = time.time() - start_time
                
                self.results.append(PerformanceMetrics(
                    test_name=f"ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–_{complexity}",
                    data_size=data_size,
                    execution_time=execution_time,
                    memory_usage_mb=0,
                    peak_memory_mb=0,
                    cpu_usage_percent=0,
                    throughput_records_per_sec=0,
                    success=False,
                    error_message=str(e)
                ))
                
                logger.error(f"âŒ {complexity}è¤‡é›‘åº¦å¤±æ•—: {e}")
    
    def test_concurrent_processing(self):
        """ä¸¦è¡Œå‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info("âš¡ ä¸¦è¡Œå‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        data_size = 2000
        test_data = self.create_large_dataset(data_size, "medium")
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†
        logger.info("ğŸ“Š ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ãƒ†ã‚¹ãƒˆ")
        start_time = time.time()
        
        try:
            from app.services.ml.single_model.single_model_trainer import SingleModelTrainer
            
            trainer = SingleModelTrainer(model_type="lightgbm")
            result1 = trainer.train_model(
                training_data=test_data,
                save_model=False,
                threshold_up=0.02,
                threshold_down=-0.02
            )
            
            sequential_time = time.time() - start_time
            
            self.results.append(PerformanceMetrics(
                test_name="ä¸¦è¡Œå‡¦ç†_ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«",
                data_size=data_size,
                execution_time=sequential_time,
                memory_usage_mb=self.process.memory_info().rss / 1024**2,
                peak_memory_mb=self.process.memory_info().rss / 1024**2,
                cpu_usage_percent=psutil.cpu_percent(),
                throughput_records_per_sec=data_size / sequential_time,
                success=True,
                additional_metrics={
                    'processing_type': 'sequential',
                    'accuracy': result1.get('accuracy', 0)
                }
            ))
            
            logger.info(f"âœ… ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†å®Œäº†: {sequential_time:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†å¤±æ•—: {e}")
    
    def test_scalability_limits(self):
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£é™ç•Œãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ“ˆ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£é™ç•Œãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        # æ®µéšçš„ã«ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’å¢—åŠ 
        max_size = 50000
        step_size = 10000
        
        for size in range(step_size, max_size + 1, step_size):
            logger.info(f"ğŸ” {size:,}è¡Œã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ")
            
            gc.collect()
            start_time = time.time()
            initial_memory = self.process.memory_info().rss / 1024**2
            
            try:
                # ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒã‚§ãƒƒã‚¯
                available_memory = psutil.virtual_memory().available / 1024**2
                if available_memory < 1000:  # 1GBæœªæº€ã®å ´åˆã¯åœæ­¢
                    logger.warning(f"âš ï¸ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚{size:,}è¡Œãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
                    break
                
                # ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆè»½é‡ç‰ˆï¼‰
                test_data = self.create_large_dataset(size, "simple")
                
                # è»½é‡ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
                from app.services.ml.feature_engineering.feature_engineering_service import FeatureEngineeringService
                
                fe_service = FeatureEngineeringService()
                features = fe_service.calculate_basic_features(test_data)
                
                execution_time = time.time() - start_time
                final_memory = self.process.memory_info().rss / 1024**2
                memory_usage = final_memory - initial_memory
                
                self.results.append(PerformanceMetrics(
                    test_name=f"ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£_{size:,}è¡Œ",
                    data_size=size,
                    execution_time=execution_time,
                    memory_usage_mb=memory_usage,
                    peak_memory_mb=final_memory,
                    cpu_usage_percent=psutil.cpu_percent(),
                    throughput_records_per_sec=size / execution_time,
                    success=True,
                    additional_metrics={
                        'feature_count': len(features.columns),
                        'memory_per_record_kb': (memory_usage * 1024) / size,
                        'available_memory_mb': available_memory
                    }
                ))
                
                logger.info(f"âœ… {size:,}è¡Œå®Œäº†: {execution_time:.2f}ç§’, ãƒ¡ãƒ¢ãƒª {memory_usage:.2f}MB")
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                del test_data, features
                gc.collect()
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒæ€¥æ¿€ã«å¢—åŠ ã—ãŸå ´åˆã¯åœæ­¢
                if memory_usage > 2000:  # 2GBä»¥ä¸Š
                    logger.warning(f"âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ{memory_usage:.2f}MBã«é”ã—ãŸãŸã‚åœæ­¢")
                    break
                
            except Exception as e:
                execution_time = time.time() - start_time
                
                self.results.append(PerformanceMetrics(
                    test_name=f"ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£_{size:,}è¡Œ",
                    data_size=size,
                    execution_time=execution_time,
                    memory_usage_mb=0,
                    peak_memory_mb=0,
                    cpu_usage_percent=0,
                    throughput_records_per_sec=0,
                    success=False,
                    error_message=str(e)
                ))
                
                logger.error(f"âŒ {size:,}è¡Œå¤±æ•—: {e}")
                break  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯åœæ­¢

if __name__ == "__main__":
    logger.info("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")
    
    test_suite = PerformanceTestSuite()
    
    # å„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    test_suite.test_large_data_processing()
    test_suite.test_memory_optimization()
    test_suite.test_concurrent_processing()
    test_suite.test_scalability_limits()
    
    # çµæœã‚µãƒãƒªãƒ¼
    total_tests = len(test_suite.results)
    successful_tests = sum(1 for r in test_suite.results if r.success)
    
    print("\n" + "="*80)
    print("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ")
    print("="*80)
    print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
    print(f"âœ… æˆåŠŸ: {successful_tests}")
    print(f"âŒ å¤±æ•—: {total_tests - successful_tests}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {(successful_tests/total_tests*100):.1f}%")
    
    print("\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©³ç´°:")
    for result in test_suite.results:
        status = "âœ…" if result.success else "âŒ"
        print(f"{status} {result.test_name}")
        if result.success:
            print(f"   å®Ÿè¡Œæ™‚é–“: {result.execution_time:.2f}ç§’")
            print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result.throughput_records_per_sec:.1f}è¡Œ/ç§’")
            print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {result.memory_usage_mb:.2f}MB")
    
    print("="*80)
    
    logger.info("ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†")
