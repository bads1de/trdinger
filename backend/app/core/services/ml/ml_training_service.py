"""
MLå­¦ç¿’ã‚µãƒ¼ãƒ“ã‚¹

MLãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ©Ÿèƒ½ã‚’å°‚é–€çš„ã«æ‰±ã†ã‚µãƒ¼ãƒ“ã‚¹ã€‚
BaseMLTrainerã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ¼ãƒ‰ã®é‡è¤‡ã‚’è§£æ¶ˆã—ã€è²¬ä»»ã‚’æ˜ç¢ºåŒ–ã—ã¾ã™ã€‚
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–æ©Ÿèƒ½ã‚‚æä¾›ã—ã¾ã™ã€‚
"""

import logging
import pandas as pd
from typing import Dict, Any, Optional, Callable


from .config import ml_config
from ...utils.unified_error_handler import safe_ml_operation
from .lightgbm_trainer import LightGBMTrainer
from .model_manager import model_manager
from ..optimization.optuna_optimizer import OptunaOptimizer, ParameterSpace

logger = logging.getLogger(__name__)


class OptimizationSettings:
    """æœ€é©åŒ–è¨­å®šã‚¯ãƒ©ã‚¹ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰"""

    def __init__(
        self,
        enabled: bool = False,
        n_calls: int = 50,
        parameter_space: Optional[Dict[str, Dict[str, Any]]] = None,
    ):
        self.enabled = enabled
        self.n_calls = n_calls
        self.parameter_space = parameter_space or {}


class MLTrainingService:
    """
    MLå­¦ç¿’ã‚µãƒ¼ãƒ“ã‚¹

    BaseMLTrainerã‚’ä½¿ç”¨ã—ã¦MLãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã€ä¿å­˜ã‚’å°‚é–€çš„ã«è¡Œã†ã‚µãƒ¼ãƒ“ã‚¹ã€‚
    ã‚³ãƒ¼ãƒ‰ã®é‡è¤‡ã‚’è§£æ¶ˆã—ã€ä¿å®ˆæ€§ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚
    """

    def __init__(
        self,
        trainer_type: str = "lightgbm",
        automl_config: Optional[Dict[str, Any]] = None,
    ):
        """
        åˆæœŸåŒ–

        Args:
            trainer_type: ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ã‚¿ã‚¤ãƒ—ï¼ˆç¾åœ¨ã¯"lightgbm"ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼‰
            automl_config: AutoMLè¨­å®šï¼ˆè¾æ›¸å½¢å¼ï¼‰
        """
        self.config = ml_config
        self.automl_config = automl_config

        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’é¸æŠï¼ˆAutoMLè¨­å®šã‚’æ¸¡ã™ï¼‰
        if trainer_type.lower() == "lightgbm":
            self.trainer = LightGBMTrainer(automl_config=automl_config)
        else:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—: {trainer_type}")

        self.trainer_type = trainer_type

    def train_model(
        self,
        training_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        save_model: bool = True,
        model_name: Optional[str] = None,
        optimization_settings: Optional[OptimizationSettings] = None,
        automl_config: Optional[Dict[str, Any]] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆæœ€é©åŒ–æ©Ÿèƒ½ä»˜ãï¼‰

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_model: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            optimization_settings: æœ€é©åŒ–è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            automl_config: AutoMLç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸

        Raises:
            MLDataError: ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ãªå ´åˆ
            MLModelError: å­¦ç¿’ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        # AutoMLè¨­å®šã®å‡¦ç†
        effective_automl_config = automl_config or self.automl_config
        if effective_automl_config:
            # AutoMLè¨­å®šãŒæä¾›ã•ã‚ŒãŸå ´åˆã€æ–°ã—ã„ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            if self.trainer_type.lower() == "lightgbm":
                trainer = LightGBMTrainer(automl_config=effective_automl_config)
            else:
                trainer = self.trainer
            logger.info(
                "ğŸ¤– AutoMLç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™"
            )
        else:
            trainer = self.trainer
            logger.info(
                "ğŸ“Š åŸºæœ¬ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™"
            )

        # æœ€é©åŒ–ãŒæœ‰åŠ¹ãªå ´åˆã¯æœ€é©åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
        if optimization_settings and optimization_settings.enabled:
            return self._train_with_optimization(
                training_data=training_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                save_model=save_model,
                model_name=model_name,
                optimization_settings=optimization_settings,
                trainer=trainer,  # é©åˆ‡ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’æ¸¡ã™
                **training_params,
            )
        else:
            # é€šå¸¸ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            return trainer.train_model(
                training_data=training_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                save_model=save_model,
                model_name=model_name,
                **training_params,
            )

    def evaluate_model(
        self,
        test_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> Dict[str, Any]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

        Args:
            test_data: ãƒ†ã‚¹ãƒˆç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        # BaseMLTrainerã«å§”è­²
        return self.trainer.evaluate_model(
            test_data=test_data,
            funding_rate_data=funding_rate_data,
            open_interest_data=open_interest_data,
        )

    def get_training_status(self) -> Dict[str, Any]:
        """
        å­¦ç¿’çŠ¶æ…‹ã‚’å–å¾—

        Returns:
            å­¦ç¿’çŠ¶æ…‹ã®è¾æ›¸
        """
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        if hasattr(self.trainer, "get_model_info"):
            model_info = self.trainer.get_model_info()
            model_info["trainer_type"] = self.trainer_type
            return model_info
        else:
            return {
                "is_trained": self.trainer.is_trained,
                "feature_columns": self.trainer.feature_columns,
                "feature_count": (
                    len(self.trainer.feature_columns)
                    if self.trainer.feature_columns
                    else 0
                ),
                "model_type": (
                    type(self.trainer.model).__name__ if self.trainer.model else None
                ),
                "trainer_type": self.trainer_type,
            }

    def predict(self, features_df: pd.DataFrame) -> Dict[str, Any]:
        """
        äºˆæ¸¬ã‚’å®Ÿè¡Œ

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        predictions = self.trainer.predict(features_df)
        return {
            "predictions": predictions,
            "model_type": self.trainer_type,
            "feature_count": (
                len(self.trainer.feature_columns) if self.trainer.feature_columns else 0
            ),
        }

    def get_feature_importance(self) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—

        Returns:
            ç‰¹å¾´é‡é‡è¦åº¦ã®è¾æ›¸
        """
        if hasattr(self.trainer, "get_feature_importance"):
            return self.trainer.get_feature_importance()
        else:
            return {}

    @safe_ml_operation(
        default_return=False, context="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )
    def load_model(self, model_path: str) -> bool:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿

        Args:
            model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            èª­ã¿è¾¼ã¿æˆåŠŸãƒ•ãƒ©ã‚°
        """
        return self.trainer.load_model(model_path)

    def get_latest_model_path(self) -> Optional[str]:
        """æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return model_manager.get_latest_model("*")

    def list_available_models(self) -> list:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§ã‚’å–å¾—"""
        return model_manager.list_models("*")

    def _train_with_optimization(
        self,
        training_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        save_model: bool = True,
        model_name: Optional[str] = None,
        optimization_settings: OptimizationSettings = None,
        trainer: Optional[Any] = None,  # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’å—ã‘å–ã‚‹
        **training_params,
    ) -> Dict[str, Any]:
        """
        æœ€é©åŒ–ã‚’ä½¿ç”¨ã—ã¦MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_model: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            optimization_settings: æœ€é©åŒ–è¨­å®š
            trainer: ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€æŒ‡å®šã•ã‚Œãªã„å ´åˆã¯self.trainerã‚’ä½¿ç”¨ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸ï¼ˆæœ€é©åŒ–æƒ…å ±ã‚’å«ã‚€ï¼‰
        """
        try:
            # ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’æ±ºå®š
            effective_trainer = trainer if trainer is not None else self.trainer

            logger.info("ğŸš€ Optunaæœ€é©åŒ–ã‚’é–‹å§‹")
            logger.info(f"ğŸ¯ ç›®æ¨™è©¦è¡Œå›æ•°: {optimization_settings.n_calls}")
            logger.info(f"ğŸ¤– ä½¿ç”¨ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼: {type(effective_trainer).__name__}")

            # Optunaã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
            optimizer = OptunaOptimizer()
            logger.info("âœ… OptunaOptimizer ã‚’ä½œæˆã—ã¾ã—ãŸ")

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æº–å‚™
            if not optimization_settings.parameter_space:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ä½¿ç”¨
                parameter_space = optimizer.get_default_parameter_space()
                logger.info("ğŸ“Š ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ä½¿ç”¨")
            else:
                parameter_space = self._prepare_parameter_space(
                    optimization_settings.parameter_space
                )
            logger.info(
                f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æº–å‚™: {len(parameter_space)}å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"
            )

            # ç›®çš„é–¢æ•°ã‚’ä½œæˆ
            objective_function = self._create_objective_function(
                training_data=training_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                optimization_settings=optimization_settings,
                trainer=effective_trainer,  # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’æ¸¡ã™
                **training_params,
            )
            logger.info("ğŸ¯ ç›®çš„é–¢æ•°ã‚’ä½œæˆã—ã¾ã—ãŸ")

            # æœ€é©åŒ–ã‚’å®Ÿè¡Œ
            logger.info("ğŸ”„ æœ€é©åŒ–å‡¦ç†ã‚’é–‹å§‹...")
            optimization_result = optimizer.optimize(
                objective_function=objective_function,
                parameter_space=parameter_space,
                n_calls=optimization_settings.n_calls,
            )

            logger.info("ğŸ‰ æœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            logger.info(f"ğŸ† ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢: {optimization_result.best_score:.4f}")
            logger.info(f"âš™ï¸  æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {optimization_result.best_params}")
            logger.info(f"ğŸ“ˆ ç·è©•ä¾¡å›æ•°: {optimization_result.total_evaluations}")
            logger.info(f"â±ï¸  æœ€é©åŒ–æ™‚é–“: {optimization_result.optimization_time:.2f}ç§’")

            # æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
            final_training_params = {
                **training_params,
                **optimization_result.best_params,
            }
            final_result = effective_trainer.train_model(  # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½¿ç”¨
                training_data=training_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                save_model=save_model,
                model_name=model_name,
                **final_training_params,
            )

            # æœ€é©åŒ–æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            final_result["optimization_result"] = {
                "method": "optuna",
                "best_params": optimization_result.best_params,
                "best_score": optimization_result.best_score,
                "total_evaluations": optimization_result.total_evaluations,
                "optimization_time": optimization_result.optimization_time,
            }

            return final_result

        except Exception as e:
            logger.error(f"æœ€é©åŒ–å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            raise

    def _prepare_parameter_space(
        self, parameter_space_config: Dict[str, Dict[str, Any]]
    ) -> Dict[str, ParameterSpace]:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“è¨­å®šã‚’ParameterSpaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›

        Args:
            parameter_space_config: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“è¨­å®š

        Returns:
            ParameterSpaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¾æ›¸
        """
        parameter_space = {}

        for param_name, param_config in parameter_space_config.items():
            param_type = param_config["type"]
            low = param_config.get("low")
            high = param_config.get("high")

            # integerå‹ã®å ´åˆã¯ã€lowã¨highã‚’æ•´æ•°ã«å¤‰æ›
            if param_type == "integer" and low is not None and high is not None:
                low = int(low)
                high = int(high)

            parameter_space[param_name] = ParameterSpace(
                type=param_type,
                low=low,
                high=high,
                categories=param_config.get("categories"),
            )

        return parameter_space

    def _create_objective_function(
        self,
        training_data: pd.DataFrame,
        optimization_settings: "OptimizationSettings",
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        trainer: Optional[Any] = None,  # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’å—ã‘å–ã‚‹
        **base_training_params,
    ) -> Callable[[Dict[str, Any]], float]:
        """
        æœ€é©åŒ–ã®ãŸã‚ã®ç›®çš„é–¢æ•°ã‚’ä½œæˆ

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            **base_training_params: ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ç›®çš„é–¢æ•°ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã‚¹ã‚³ã‚¢ã‚’è¿”ã™é–¢æ•°ï¼‰
        """
        # ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’æ±ºå®š
        effective_trainer = trainer if trainer is not None else self.trainer

        # è©¦è¡Œå›æ•°ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        evaluation_count = 0

        def objective_function(params: Dict[str, Any]) -> float:
            """
            ç›®çš„é–¢æ•°ï¼šä¸ãˆã‚‰ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒŸãƒ‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¿”ã™

            Args:
                params: æœ€é©åŒ–å¯¾è±¡ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

            Returns:
                è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆF1ã‚¹ã‚³ã‚¢ï¼‰
            """
            nonlocal evaluation_count
            evaluation_count += 1

            try:
                logger.info(
                    f"ğŸ” æœ€é©åŒ–è©¦è¡Œ {evaluation_count}/{optimization_settings.n_calls}"
                )
                logger.info(f"ğŸ“‹ è©¦è¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params}")

                # ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
                training_params = {**base_training_params, **params}

                # ä¸€æ™‚çš„ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆï¼ˆå…ƒã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
                # AutoMLè¨­å®šãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’å¼•ãç¶™ã
                if hasattr(effective_trainer, "automl_config"):
                    temp_trainer = LightGBMTrainer(
                        automl_config=effective_trainer.automl_config
                    )
                else:
                    temp_trainer = LightGBMTrainer()

                # ãƒŸãƒ‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œï¼ˆä¿å­˜ã¯ã—ãªã„ï¼‰
                result = temp_trainer.train_model(
                    training_data=training_data,
                    funding_rate_data=funding_rate_data,
                    open_interest_data=open_interest_data,
                    save_model=False,  # æœ€é©åŒ–ä¸­ã¯ä¿å­˜ã—ãªã„
                    model_name=None,
                    **training_params,
                )

                # F1ã‚¹ã‚³ã‚¢ã‚’è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨
                f1_score = result.get("f1_score", 0.0)

                # ãƒã‚¯ãƒ­å¹³å‡F1ã‚¹ã‚³ã‚¢ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆ
                if "classification_report" in result:
                    macro_f1 = (
                        result["classification_report"]
                        .get("macro avg", {})
                        .get("f1-score", f1_score)
                    )
                    f1_score = macro_f1

                logger.info(f"ğŸ“Š è©¦è¡Œçµæœ: F1ã‚¹ã‚³ã‚¢={f1_score:.4f}")
                logger.info("-" * 50)
                return f1_score

            except Exception as e:
                logger.warning(f"ç›®çš„é–¢æ•°è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ä½ã„ã‚¹ã‚³ã‚¢ã‚’è¿”ã™
                return 0.0

        return objective_function


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯LightGBMã€AutoMLè¨­å®šãªã—ï¼‰
ml_training_service = MLTrainingService(trainer_type="lightgbm", automl_config=None)
