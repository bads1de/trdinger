"""
æˆ¦ç•¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒŠãƒ¼

Optuna ã‚’ä½¿ç”¨ã—ã¦æˆ¦ç•¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚
"""

import logging
from typing import Any, Dict, Optional

from app.services.ml.optimization.optuna_optimizer import OptunaOptimizer

from ..config.ga_runtime import GAConfig
from ..core.individual_evaluator import IndividualEvaluator
from ..models.strategy_gene import StrategyGene
from .strategy_parameter_space import StrategyParameterSpace

logger = logging.getLogger(__name__)


class StrategyParameterTuner:
    """
    Optuna ã«ã‚ˆã‚‹æˆ¦ç•¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

    GAã§ç™ºè¦‹ã•ã‚ŒãŸæˆ¦ç•¥æ§‹é€ ã«å¯¾ã—ã¦ã€Optunaã‚’ä½¿ç”¨ã—ã¦
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿æœŸé–“ã€TPSLè¨­å®šãªã©ï¼‰ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚
    """

    def __init__(
        self,
        evaluator: IndividualEvaluator,
        config: GAConfig,
        n_trials: int = 30,
        use_wfa: bool = True,
        include_indicators: bool = True,
        include_tpsl: bool = True,
        include_thresholds: bool = False,
    ):
        """
        åˆæœŸåŒ–

        Args:
            evaluator: è©•ä¾¡å™¨ï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼‰
            config: GAè¨­å®š
            n_trials: Optunaè©¦è¡Œå›æ•°
            use_wfa: WFAè©•ä¾¡ã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
            include_indicators: ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹ã‹
            include_tpsl: TPSLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹ã‹
            include_thresholds: æ¡ä»¶é–¾å€¤ã‚’æœ€é©åŒ–ã™ã‚‹ã‹
        """
        self.evaluator = evaluator
        self.config = config
        self.n_trials = n_trials
        self.use_wfa = use_wfa
        self.include_indicators = include_indicators
        self.include_tpsl = include_tpsl
        self.include_thresholds = include_thresholds

        self.parameter_space_builder = StrategyParameterSpace()
        self.optimizer = OptunaOptimizer()

    def tune(self, gene: StrategyGene) -> StrategyGene:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ

        Args:
            gene: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®æˆ¦ç•¥éºä¼å­

        Returns:
            æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤æ–°ã—ã„ StrategyGene
        """
        logger.info("ğŸ”§ æˆ¦ç•¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹")

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æ§‹ç¯‰
        parameter_space = self.parameter_space_builder.build_parameter_space(
            gene,
            include_indicators=self.include_indicators,
            include_tpsl=self.include_tpsl,
            include_thresholds=self.include_thresholds,
        )

        if not parameter_space:
            logger.warning("æœ€é©åŒ–å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return gene

        logger.info(f"æœ€é©åŒ–å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(parameter_space)}")

        # ç›®çš„é–¢æ•°ã‚’å®šç¾©
        def objective_function(params: Dict[str, Any]) -> float:
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©ç”¨ã—ãŸéºä¼å­ã‚’ä½œæˆ
            tuned_gene = self.parameter_space_builder.apply_params_to_gene(gene, params)

            # ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã‚’è©•ä¾¡
            fitness = self._evaluate_gene(tuned_gene)

            return fitness

        try:
            # Optunaæœ€é©åŒ–ã‚’å®Ÿè¡Œ
            result = self.optimizer.optimize(
                objective_function=objective_function,
                parameter_space=parameter_space,
                n_calls=self.n_trials,
            )

            logger.info(
                f"âœ… ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢={result.best_score:.4f}, "
                f"è©¦è¡Œå›æ•°={result.total_evaluations}"
            )

            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©ç”¨
            tuned_gene = self.parameter_space_builder.apply_params_to_gene(
                gene, result.best_params
            )

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–æƒ…å ±ã‚’è¿½åŠ 
            tuned_gene.metadata["optuna_tuned"] = True
            tuned_gene.metadata["optuna_best_score"] = result.best_score
            tuned_gene.metadata["optuna_trials"] = result.total_evaluations
            tuned_gene.metadata["optuna_time"] = result.optimization_time

            return tuned_gene

        except Exception as e:
            logger.error(f"ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®éºä¼å­ã‚’è¿”ã™
            return gene

        finally:
            # ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.optimizer.cleanup()

    def _evaluate_gene(self, gene: StrategyGene) -> float:
        """
        éºä¼å­ã®ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã‚’è©•ä¾¡

        Args:
            gene: è©•ä¾¡å¯¾è±¡ã®éºä¼å­

        Returns:
            ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹å€¤ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
        """
        try:
            # WFAè©•ä¾¡ãŒæœ‰åŠ¹ãªå ´åˆ
            if self.use_wfa and self.config.enable_walk_forward:
                # WFAè¨­å®šã‚’ä¸€æ™‚çš„ã«æœ‰åŠ¹åŒ–ã—ãŸconfigã‚’ä½¿ç”¨
                wfa_config = self._create_wfa_config()
                fitness_tuple = self.evaluator.evaluate_individual(gene, wfa_config)
            else:
                fitness_tuple = self.evaluator.evaluate_individual(gene, self.config)

            # ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã‚¿ãƒ—ãƒ«ã‹ã‚‰ä¸»è¦ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
            if isinstance(fitness_tuple, tuple) and len(fitness_tuple) > 0:
                return float(fitness_tuple[0])
            else:
                return 0.0

        except Exception as e:
            logger.warning(f"éºä¼å­è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _create_wfa_config(self) -> GAConfig:
        """WFAç”¨ã®è¨­å®šã‚’ä½œæˆ"""
        # å…ƒã®configã‚’ã‚³ãƒ”ãƒ¼ã—ã¦WFAã‚’æœ‰åŠ¹åŒ–
        import copy

        wfa_config = copy.deepcopy(self.config)
        wfa_config.enable_walk_forward = True

        # WFAãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°ã‚’æ¸›ã‚‰ã—ã¦é«˜é€ŸåŒ–ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ï¼‰
        if wfa_config.wfa_n_folds > 3:
            wfa_config.wfa_n_folds = 3

        return wfa_config

    def tune_multiple(
        self, genes: list[StrategyGene], top_n: Optional[int] = None
    ) -> list[StrategyGene]:
        """
        è¤‡æ•°ã®éºä¼å­ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

        Args:
            genes: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®éºä¼å­ãƒªã‚¹ãƒˆ
            top_n: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ä¸Šä½Nå€‹ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰

        Returns:
            ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸéºä¼å­ã®ãƒªã‚¹ãƒˆ
        """
        if top_n is not None:
            genes = genes[:top_n]

        tuned_genes = []
        for idx, gene in enumerate(genes):
            logger.info(f"éºä¼å­ {idx + 1}/{len(genes)} ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
            tuned_gene = self.tune(gene)
            tuned_genes.append(tuned_gene)

        return tuned_genes


