import logging
from typing import Any, Callable, Dict, Optional, List
import pandas as pd

from ...utils.error_handler import safe_operation
from .optuna_optimizer import OptunaOptimizer, ParameterSpace
from ..ml.ensemble.ensemble_trainer import EnsembleTrainer

logger = logging.getLogger(__name__)


class OptimizationSettings:
    """æœ€é©åŒ–è¨­å®šã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        enabled: bool = False,
        n_calls: int = 50,
        parameter_space: Optional[Dict[str, Dict[str, Any]]] = None,
    ):
        self.enabled = enabled
        self.n_calls = n_calls
        self.parameter_space = parameter_space or {}


class OptimizationService:
    """
    æœ€é©åŒ–ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆçµ±ä¸€ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼å¯¾å¿œï¼‰

    MLãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’ç®¡ç†ã—ã¾ã™ã€‚
    SingleModelTrainerã¯å»ƒæ­¢ã—ã€å…¨ã¦EnsembleTrainerã§çµ±ä¸€ã€‚
    """

    def __init__(self):
        self.optimizer = OptunaOptimizer()

    @safe_operation(context="ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–", is_api_call=False)
    def optimize_parameters(
        self,
        trainer: Any,
        training_data: pd.DataFrame,
        optimization_settings: OptimizationSettings,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        model_name: Optional[str] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè¡Œ
        """
        logger.info("ğŸš€ æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹")

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æº–å‚™
        parameter_space = self._prepare_parameter_space(trainer, optimization_settings)

        # ç›®çš„é–¢æ•°ã‚’ä½œæˆ
        objective_function = self._create_objective_function(
            trainer=trainer,
            training_data=training_data,
            optimization_settings=optimization_settings,
            funding_rate_data=funding_rate_data,
            open_interest_data=open_interest_data,
            **training_params,
        )

        # æœ€é©åŒ–ã‚’å®Ÿè¡Œ
        result = self.optimizer.optimize(
            objective_function=objective_function,
            parameter_space=parameter_space,
            n_calls=optimization_settings.n_calls,
        )

        return {
            "method": "optuna",
            "best_params": result.best_params,
            "best_score": result.best_score,
            "total_evaluations": result.total_evaluations,
            "optimization_time": result.optimization_time,
        }

    def _prepare_parameter_space(
        self, trainer: Any, optimization_settings: OptimizationSettings
    ) -> Dict[str, ParameterSpace]:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æº–å‚™"""
        if not optimization_settings.parameter_space:
            # EnsembleTrainerã®å ´åˆï¼ˆå˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚‚å«ã‚€ï¼‰
            if hasattr(trainer, "ensemble_config"):
                ensemble_method = trainer.ensemble_config.get("method", "stacking")
                enabled_models = trainer.ensemble_config.get(
                    "models", ["lightgbm", "xgboost"]
                )
                return self.optimizer.get_ensemble_parameter_space(
                    ensemble_method, enabled_models
                )
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“
                return self.optimizer.get_default_parameter_space()
        else:
            return self._convert_parameter_space_config(
                optimization_settings.parameter_space
            )

    def _convert_parameter_space_config(
        self, parameter_space_config: Dict[str, Dict[str, Any]]
    ) -> Dict[str, ParameterSpace]:
        """è¨­å®šè¾æ›¸ã‚’ParameterSpaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        parameter_space = {}
        for param_name, param_config in parameter_space_config.items():
            param_type = param_config["type"]
            low = param_config.get("low")
            high = param_config.get("high")

            if param_type == "integer" and low is not None and high is not None:
                low = int(low)
                high = int(high)

            parameter_space[param_name] = ParameterSpace(
                type=param_type,
                low=low,
                high=high,
                categories=param_config.get("categories"),
            )
        return parameter_space

    def _create_objective_function(
        self,
        trainer: Any,
        training_data: pd.DataFrame,
        optimization_settings: OptimizationSettings,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        **base_training_params,
    ) -> Callable[[Dict[str, Any]], float]:
        """ç›®çš„é–¢æ•°ã‚’ä½œæˆ"""
        evaluation_count = 0

        def objective_function(params: Dict[str, Any]) -> float:
            nonlocal evaluation_count
            evaluation_count += 1

            try:
                logger.info(
                    f"ğŸ” è©¦è¡Œ {evaluation_count}/{optimization_settings.n_calls}: {params}"
                )

                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸
                training_params = {**base_training_params, **params}

                # ä¸€æ™‚çš„ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆ
                temp_trainer = self._create_temp_trainer(trainer, params)

                # å­¦ç¿’å®Ÿè¡Œï¼ˆä¿å­˜ãªã—ï¼‰
                result = temp_trainer.train_model(
                    training_data=training_data,
                    funding_rate_data=funding_rate_data,
                    open_interest_data=open_interest_data,
                    save_model=False,
                    model_name=None,
                    **training_params,
                )

                # ã‚¹ã‚³ã‚¢å–å¾—
                f1_score = result.get("f1_score", 0.0)
                if "classification_report" in result:
                    f1_score = (
                        result["classification_report"]
                        .get("macro avg", {})
                        .get("f1-score", f1_score)
                    )

                return f1_score

            except Exception as e:
                logger.warning(f"ç›®çš„é–¢æ•°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                return 0.0

        return objective_function

    def _create_temp_trainer(
        self, original_trainer: Any, params: Dict[str, Any]
    ) -> Any:
        """ä¸€æ™‚çš„ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆï¼ˆå…¨ã¦EnsembleTrainerã§çµ±ä¸€ï¼‰"""
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒEnsembleTrainerã§ã‚ã‚‹ã“ã¨ã‚’å‰æ
        if hasattr(original_trainer, "ensemble_config"):
            temp_config = original_trainer.ensemble_config.copy()

            # æœ€é©åŒ–ç”¨ã«CV foldsã‚’æ¸›ã‚‰ã™ï¼ˆé€Ÿåº¦å‘ä¸Šï¼‰
            if "stacking_params" in temp_config:
                stacking_params = temp_config["stacking_params"].copy()
                stacking_params["cv_folds"] = 3
                temp_config["stacking_params"] = stacking_params

            return EnsembleTrainer(ensemble_config=temp_config)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§EnsembleTrainerä½œæˆ
            return EnsembleTrainer(ensemble_config={"method": "stacking"})

    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.optimizer.cleanup()
