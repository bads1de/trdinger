"""
MLå­¦ç¿’ã‚µãƒ¼ãƒ“ã‚¹

MLãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»ä¿å­˜ã‚’å–ã‚Šæ‰±ã†ã‚µãƒ¼ãƒ“ã‚¹å±¤ã§ã™ã€‚
å†…éƒ¨å®Ÿè£…ã®è©³ç´°ã‚„ç‰¹å®šã®æœ€é©åŒ–æ‰‹æ³•ã®èª¬æ˜ã¯Docstringã«å«ã‚ãšã€
ã‚µãƒ¼ãƒ“ã‚¹ã®å½¹å‰²ï¼ˆå­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®èª¿æ•´ã¨çµæœã®æä¾›ï¼‰ã«é™å®šã—ã¦è¨˜è¿°ã—ã¾ã™ã€‚
"""

import logging
from typing import Any, Callable, Dict, List, Optional

import numpy as np
import pandas as pd

from ...utils.data_processing import data_processor as data_preprocessor
from ...utils.error_handler import safe_ml_operation
from ..optimization.optuna_optimizer import OptunaOptimizer, ParameterSpace
from .base_ml_trainer import BaseMLTrainer
from .common.base_resource_manager import BaseResourceManager, CleanupLevel
from .config import ml_config
from .ensemble.ensemble_trainer import EnsembleTrainer
from .single_model.single_model_trainer import SingleModelTrainer

logger = logging.getLogger(__name__)


class OptimizationSettings:
    """æœ€é©åŒ–è¨­å®šã‚¯ãƒ©ã‚¹ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰"""

    def __init__(
        self,
        enabled: bool = False,
        n_calls: int = 50,
        parameter_space: Optional[Dict[str, Dict[str, Any]]] = None,
    ):
        self.enabled = enabled
        self.n_calls = n_calls
        self.parameter_space = parameter_space or {}


class MLTrainingService(BaseResourceManager):
    """
    MLå­¦ç¿’ã‚µãƒ¼ãƒ“ã‚¹

    BaseMLTrainerã‚’ä½¿ç”¨ã—ã¦MLãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€è©•ä¾¡ã€ä¿å­˜ã‚’å°‚é–€çš„ã«è¡Œã†ã‚µãƒ¼ãƒ“ã‚¹ã€‚
    ã‚³ãƒ¼ãƒ‰ã®é‡è¤‡ã‚’è§£æ¶ˆã—ã€ä¿å®ˆæ€§ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚
    """

    def __init__(
        self,
        trainer_type: str = "ensemble",
        ensemble_config: Optional[Dict[str, Any]] = None,
        single_model_config: Optional[Dict[str, Any]] = None,
    ):
        """
        åˆæœŸåŒ–

        Args:
            trainer_type: ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ã‚¿ã‚¤ãƒ—ï¼ˆ'ensemble' ã¾ãŸã¯ 'single'ï¼‰
            ensemble_config: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šï¼ˆè¾æ›¸å½¢å¼ï¼‰
            single_model_config: å˜ä¸€ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆè¾æ›¸å½¢å¼ï¼‰
        """
        # BaseResourceManagerã®åˆæœŸåŒ–
        super().__init__()

        self.config = ml_config
        self.ensemble_config = ensemble_config
        self.single_model_config = single_model_config

        # çµ±åˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šã‚’ä½œæˆ
        trainer_config = self._create_trainer_config(
            trainer_type, ensemble_config, single_model_config
        )

        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’é¸æŠã—ã¦åˆæœŸåŒ–
        if trainer_type.lower() == "single":
            model_type = trainer_config.get("model_type", "lightgbm")
            # æ˜ç¤ºçš„ã« SingleModelTrainer ã‚’ä½¿ç”¨ï¼ˆãƒ†ã‚¹ãƒˆæœŸå¾…ã¨ä¸€è‡´ï¼‰
            self.trainer = SingleModelTrainer(model_type=model_type)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯çµ±åˆ BaseMLTrainer
            self.trainer = BaseMLTrainer(trainer_config=trainer_config)

        self.trainer_type = trainer_type

        if trainer_type == "single" and single_model_config:
            logger.info(f"å˜ä¸€ãƒ¢ãƒ‡ãƒ«è¨­å®š: {single_model_config}")

    def _create_trainer_config(
        self,
        trainer_type: str,
        ensemble_config: Optional[Dict[str, Any]],
        single_model_config: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        çµ±åˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šã‚’ä½œæˆ

        Args:
            trainer_type: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—
            ensemble_config: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š
            single_model_config: å˜ä¸€ãƒ¢ãƒ‡ãƒ«è¨­å®š

        Returns:
            ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šè¾æ›¸
        """
        if trainer_type.lower() == "ensemble":
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼‰
            default_ensemble_config = {
                "method": "stacking",
                "stacking_params": {
                    "base_models": ["lightgbm", "xgboost"],
                    "meta_model": "lightgbm",
                    "cv_folds": 5,
                    "use_probas": True,
                    "random_state": 42,
                },
            }

            # è¨­å®šã‚’ãƒãƒ¼ã‚¸
            final_ensemble_config = default_ensemble_config.copy()
            if ensemble_config:
                final_ensemble_config.update(ensemble_config)

            return {
                "type": "ensemble",
                "model_type": final_ensemble_config.get("method", "stacking"),
                "ensemble_config": final_ensemble_config,
            }

        elif trainer_type.lower() == "single":
            # å˜ä¸€ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            model_type = "lightgbm"
            if single_model_config and "model_type" in single_model_config:
                model_type = single_model_config["model_type"]

            return {
                "type": "single",
                "model_type": model_type,
                "model_params": single_model_config,
            }

        else:
            raise ValueError(
                f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—: {trainer_type}ã€‚"
                f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¿ã‚¤ãƒ—: 'ensemble', 'single'"
            )

    @staticmethod
    def get_available_single_models() -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªå˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return SingleModelTrainer.get_available_models()

    @staticmethod
    def determine_trainer_type(ensemble_config: Optional[Dict[str, Any]]) -> str:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã«åŸºã¥ã„ã¦ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š

        Args:
            ensemble_config: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š

        Returns:
            ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆ'ensemble' ã¾ãŸã¯ 'single'ï¼‰
        """
        if ensemble_config and ensemble_config.get("enabled", True) is False:
            return "single"
        return "ensemble"

    def train_model(
        self,
        training_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        save_model: bool = True,
        model_name: Optional[str] = None,
        optimization_settings: Optional[OptimizationSettings] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆæœ€é©åŒ–æ©Ÿèƒ½ä»˜ãï¼‰

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_model: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            optimization_settings: æœ€é©åŒ–è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                - use_time_series_split: æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ml_config.training.USE_TIME_SERIES_SPLITï¼‰
                - use_cross_validation: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Falseï¼‰
                - cv_splits: CVåˆ†å‰²æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ml_config.training.CROSS_VALIDATION_FOLDSï¼‰
                - max_train_size: TimeSeriesSplitã®æœ€å¤§å­¦ç¿’ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ml_config.training.MAX_TRAIN_SIZEï¼‰

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸

        Raises:
            MLDataError: ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ãªå ´åˆ
            MLModelError: å­¦ç¿’ã«å¤±æ•—ã—ãŸå ´åˆ
            ValueError: ç„¡åŠ¹ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã®å ´åˆ
        """
        # TimeSeriesSplité–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ml_configã‹ã‚‰è¨­å®šï¼ˆæœªæŒ‡å®šã®å ´åˆï¼‰
        training_params = self._prepare_training_params(training_params)

        trainer = self.trainer

        # æœ€é©åŒ–ãŒæœ‰åŠ¹ãªå ´åˆã¯æœ€é©åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
        if optimization_settings and optimization_settings.enabled:
            return self._train_with_optimization(
                training_data=training_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                save_model=save_model,
                model_name=model_name,
                optimization_settings=optimization_settings,
                trainer=trainer,  # é©åˆ‡ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’æ¸¡ã™
                **training_params,
            )
        else:
            # é€šå¸¸ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            return trainer.train_model(
                training_data=training_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                save_model=save_model,
                model_name=model_name,
                **training_params,
            )

    def _prepare_training_params(
        self, training_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        training_paramsã«TimeSeriesSplité–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š

        Args:
            training_params: å…ƒã®training_params

        Returns:
            TimeSeriesSplité–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚€training_params

        Raises:
            ValueError: ç„¡åŠ¹ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã®å ´åˆ
        """
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        params = training_params.copy()

        # TimeSeriesSplitåˆ©ç”¨ãƒ•ãƒ©ã‚°ï¼ˆml_configã‹ã‚‰å–å¾—ã€training_paramsã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
        if "use_time_series_split" not in params:
            params["use_time_series_split"] = self.config.training.USE_TIME_SERIES_SPLIT

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ©ç”¨æ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        use_cross_validation = params.get("use_cross_validation", False)
        if use_cross_validation:
            # CVåˆ†å‰²æ•°ï¼ˆml_configã‹ã‚‰å–å¾—ã€training_paramsã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
            if "cv_splits" not in params:
                params["cv_splits"] = self.config.training.CROSS_VALIDATION_FOLDS

            # TimeSeriesSplitã®æœ€å¤§å­¦ç¿’ã‚µã‚¤ã‚ºï¼ˆml_configã‹ã‚‰å–å¾—ã€training_paramsã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
            if "max_train_size" not in params:
                params["max_train_size"] = self.config.training.MAX_TRAIN_SIZE

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            cv_splits = params.get("cv_splits")
            if cv_splits is not None and cv_splits < 2:
                raise ValueError(f"cv_splitsã¯2ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: {cv_splits}")

            max_train_size = params.get("max_train_size")
            if max_train_size is not None and max_train_size <= 0:
                raise ValueError(
                    f"max_train_sizeã¯æ­£ã®æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: {max_train_size}"
                )

            logger.info(
                f"TimeSeriesSplitè¨­å®š: "
                f"use_time_series_split={params['use_time_series_split']}, "
                f"cv_splits={cv_splits}, max_train_size={max_train_size}"
            )

        return params

    def evaluate_model(
        self,
        test_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> Dict[str, Any]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

        Args:
            test_data: ãƒ†ã‚¹ãƒˆç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        # BaseMLTrainerã«å§”è­²
        return self.trainer.evaluate_model(
            test_data=test_data,
            funding_rate_data=funding_rate_data,
            open_interest_data=open_interest_data,
        )

    def get_training_status(self) -> Dict[str, Any]:
        """
        å­¦ç¿’çŠ¶æ…‹ã‚’å–å¾—

        Returns:
            å­¦ç¿’çŠ¶æ…‹ã®è¾æ›¸
        """
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        if hasattr(self.trainer, "get_model_info"):
            model_info = self.trainer.get_model_info()
            model_info["trainer_type"] = self.trainer_type
            return model_info
        else:
            return {
                "is_trained": self.trainer.is_trained,
                "feature_columns": self.trainer.feature_columns,
                "feature_count": (
                    len(self.trainer.feature_columns)
                    if self.trainer.feature_columns
                    else 0
                ),
                "model_type": (
                    type(self.trainer.model).__name__ if self.trainer.model else None
                ),
                "trainer_type": self.trainer_type,
            }

    def predict(self, features_df: pd.DataFrame) -> Dict[str, Any]:
        """
        äºˆæ¸¬ã‚’å®Ÿè¡Œ

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        predictions = self.trainer.predict(features_df)
        return {
            "predictions": predictions,
            "model_type": self.trainer_type,
            "feature_count": (
                len(self.trainer.feature_columns) if self.trainer.feature_columns else 0
            ),
        }

    def get_feature_importance(self) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—

        Returns:
            ç‰¹å¾´é‡é‡è¦åº¦ã®è¾æ›¸
        """
        if hasattr(self.trainer, "get_feature_importance"):
            return self.trainer.get_feature_importance()
        else:
            return {}

    @safe_ml_operation(
        default_return=False, context="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )
    def load_model(self, model_path: str) -> bool:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿

        Args:
            model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            èª­ã¿è¾¼ã¿æˆåŠŸãƒ•ãƒ©ã‚°
        """
        return self.trainer.load_model(model_path)

    def generate_signals(self, features: pd.DataFrame) -> Dict[str, float]:
        """
        äºˆæ¸¬ä¿¡å·ã‚’ç”Ÿæˆï¼ˆMLSignalGeneratorã®predictãƒ¡ã‚½ãƒƒãƒ‰ã‹ã‚‰ç§»æ¤ï¼‰

        Args:
            features: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬ç¢ºç‡ã®è¾æ›¸ {"up": float, "down": float, "range": float}
        """
        try:

            if not self.trainer.is_trained or self.trainer.model is None:
                # ãƒ¢ãƒ‡ãƒ«æœªå­¦ç¿’æ™‚ã¯è­¦å‘Šãƒ¬ãƒ™ãƒ«ã§ãƒ­ã‚°å‡ºåŠ›
                logger.warning("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã—ã¾ã™ã€‚")
                default_predictions = self.config.prediction.get_default_predictions()
                return default_predictions

            if self.trainer.feature_columns is None:
                # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€åˆ©ç”¨å¯èƒ½ãªå…¨ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
                logger.warning(
                    "ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªå…¨ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
                )
                # çµ±è¨ˆçš„æ‰‹æ³•ã§æ¬ æå€¤ã‚’è£œå®Œ
                features_selected = data_preprocessor.interpolate_columns(
                    features, columns=list(features.columns), strategy="median"
                )
            else:
                # ç‰¹å¾´é‡ã‚’é¸æŠãƒ»æ•´å½¢
                available_columns = [
                    col
                    for col in self.trainer.feature_columns
                    if col in features.columns
                ]
                missing_columns = [
                    col
                    for col in self.trainer.feature_columns
                    if col not in features.columns
                ]

                if len(missing_columns) > 0:
                    logger.warning(f"æ¬ æã—ã¦ã„ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ : {missing_columns}")

                if not available_columns:
                    logger.warning(
                        "æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã—ã¾ã™ã€‚"
                    )
                    return self.config.prediction.get_default_predictions()
                else:
                    # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ç”¨ã—ã€çµ±è¨ˆçš„æ‰‹æ³•ã§æ¬ æå€¤ã‚’è£œå®Œ
                    features_subset = features[available_columns]
                    features_selected = data_preprocessor.interpolate_columns(
                        features_subset,
                        columns=list(features_subset.columns),
                        strategy="median",
                    )

                    # ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ã‚’ä¸€åº¦ã«ã¾ã¨ã‚ã¦è¿½åŠ ï¼ˆDataFrameæ–­ç‰‡åŒ–ã‚’é˜²ãï¼‰
                    if missing_columns:
                        # ä¸è¶³ç‰¹å¾´é‡ã®DataFrameã‚’ä½œæˆ
                        missing_features_df = pd.DataFrame(
                            0.0,
                            index=features_selected.index,
                            columns=pd.Index(missing_columns),
                        )
                        # pd.concatã§ä¸€åº¦ã«çµåˆï¼ˆæ–­ç‰‡åŒ–ã‚’é˜²ãï¼‰
                        features_selected = pd.concat(
                            [features_selected, missing_features_df], axis=1
                        )

                    # å­¦ç¿’æ™‚ã¨åŒã˜é †åºã§ä¸¦ã³æ›¿ãˆ
                    features_selected = features_selected[self.trainer.feature_columns]

            # æ¨™æº–åŒ–
            if self.trainer.scaler is not None:
                features_scaled = self.trainer.scaler.transform(features_selected)
            else:
                logger.warning(
                    "ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ¨™æº–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                )
                features_scaled = features_selected.values

            # äºˆæ¸¬ï¼ˆLightGBMãƒ¢ãƒ‡ãƒ«ã®å ´åˆï¼‰
            # best_iterationå±æ€§ã®å­˜åœ¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ä½¿ç”¨
            if hasattr(self.trainer.model, "best_iteration") and hasattr(
                self.trainer.model, "predict"
            ):
                try:
                    best_iter = getattr(self.trainer.model, "best_iteration", None)
                    if best_iter is not None:
                        predictions = np.array(
                            self.trainer.model.predict(
                                features_scaled, num_iteration=best_iter
                            )
                        )
                    else:
                        predictions = np.array(
                            self.trainer.model.predict(features_scaled)
                        )
                except TypeError:
                    # num_iterationãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆ
                    predictions = np.array(self.trainer.model.predict(features_scaled))
            else:
                predictions = np.array(self.trainer.model.predict(features_scaled))

            # æœ€æ–°ã®äºˆæ¸¬çµæœã‚’å–å¾—
            if predictions.ndim == 2:
                latest_pred = predictions[-1]  # æœ€å¾Œã®è¡Œ
            else:
                latest_pred = predictions

            # äºˆæ¸¬çµæœã‚’3ã‚¯ãƒ©ã‚¹ï¼ˆdown, range, upï¼‰ã®ç¢ºç‡ã«å¤‰æ›
            if latest_pred.shape[0] == 3:
                predictions = {
                    "down": float(latest_pred[0]),
                    "range": float(latest_pred[1]),
                    "up": float(latest_pred[2]),
                }
                return predictions
            else:
                # 3ã‚¯ãƒ©ã‚¹ä»¥å¤–ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                logger.error(
                    f"äºˆæœŸã—ãªã„äºˆæ¸¬çµæœã®å½¢å¼: {latest_pred.shape}. 3ã‚¯ãƒ©ã‚¹åˆ†é¡ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚"
                )
                default_predictions = self.config.prediction.get_default_predictions()
                return default_predictions

        except Exception as e:
            logger.warning(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            default_predictions = self.config.prediction.get_default_predictions()
            return default_predictions

    def _train_with_optimization(
        self,
        training_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        save_model: bool = True,
        model_name: Optional[str] = None,
        optimization_settings: Optional[OptimizationSettings] = None,
        trainer: Optional[Any] = None,  # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’å—ã‘å–ã‚‹
        **training_params,
    ) -> Dict[str, Any]:
        """
        æœ€é©åŒ–ã‚’ä½¿ç”¨ã—ã¦MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_model: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            optimization_settings: æœ€é©åŒ–è¨­å®š
            trainer: ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€æŒ‡å®šã•ã‚Œãªã„å ´åˆã¯self.trainerã‚’ä½¿ç”¨ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸ï¼ˆæœ€é©åŒ–æƒ…å ±ã‚’å«ã‚€ï¼‰
        """
        optimizer = None
        try:
            # optimization_settingsãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if optimization_settings is None:
                raise ValueError("optimization_settingsãŒNoneã§ã™")

            # ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’æ±ºå®š
            effective_trainer = trainer if trainer is not None else self.trainer

            logger.info("ğŸš€ Optunaæœ€é©åŒ–ã‚’é–‹å§‹")
            logger.info(f"ğŸ¯ ç›®æ¨™è©¦è¡Œå›æ•°: {optimization_settings.n_calls}")
            logger.info(f"ğŸ¤– ä½¿ç”¨ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼: {type(effective_trainer).__name__}")

            # Optunaã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
            optimizer = OptunaOptimizer()
            logger.info("âœ… OptunaOptimizer ã‚’ä½œæˆã—ã¾ã—ãŸ")

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æº–å‚™
            if not optimization_settings.parameter_space:
                # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®å ´åˆã¯å°‚ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ä½¿ç”¨
                if hasattr(effective_trainer, "ensemble_config"):
                    ensemble_method = effective_trainer.ensemble_config.get(
                        "method", "stacking"
                    )
                    enabled_models = effective_trainer.ensemble_config.get(
                        "models", ["lightgbm", "xgboost"]
                    )
                    parameter_space = optimizer.get_ensemble_parameter_space(
                        ensemble_method, enabled_models
                    )
                    logger.info(
                        f"ğŸ“Š ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ä½¿ç”¨: {ensemble_method}, ãƒ¢ãƒ‡ãƒ«: {enabled_models}"
                    )
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ä½¿ç”¨
                    parameter_space = optimizer.get_default_parameter_space()
                    logger.info("ğŸ“Š ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ä½¿ç”¨")
            else:
                parameter_space = self._prepare_parameter_space(
                    optimization_settings.parameter_space
                )
            logger.info(
                f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æº–å‚™: {len(parameter_space)}å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"
            )

            # ç›®çš„é–¢æ•°ã‚’ä½œæˆ
            objective_function = self._create_objective_function(
                training_data=training_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                optimization_settings=optimization_settings,
                trainer=effective_trainer,  # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’æ¸¡ã™
                **training_params,
            )
            logger.info("ğŸ¯ ç›®çš„é–¢æ•°ã‚’ä½œæˆã—ã¾ã—ãŸ")

            # æœ€é©åŒ–ã‚’å®Ÿè¡Œ
            logger.info("ğŸ”„ æœ€é©åŒ–å‡¦ç†ã‚’é–‹å§‹...")
            optimization_result = optimizer.optimize(
                objective_function=objective_function,
                parameter_space=parameter_space,
                n_calls=optimization_settings.n_calls,
            )

            logger.info("ğŸ‰ æœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            logger.info(f"ğŸ† ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢: {optimization_result.best_score:.4f}")
            logger.info(f"âš™ï¸  æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {optimization_result.best_params}")
            logger.info(f"ğŸ“ˆ ç·è©•ä¾¡å›æ•°: {optimization_result.total_evaluations}")
            logger.info(f"â±ï¸  æœ€é©åŒ–æ™‚é–“: {optimization_result.optimization_time:.2f}ç§’")

            # Optunaãƒªã‚½ãƒ¼ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒ¡ãƒ¢ãƒªãƒ¼ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
            try:
                optimizer.cleanup()
            except Exception as cleanup_error:
                logger.warning(f"OptunaOptimizer ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {cleanup_error}")

            # æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
            final_training_params = {
                **training_params,
                **optimization_result.best_params,
            }
            final_result = effective_trainer.train_model(  # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½¿ç”¨
                training_data=training_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                save_model=save_model,
                model_name=model_name,
                **final_training_params,
            )

            # æœ€é©åŒ–æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            final_result["optimization_result"] = {
                "method": "optuna",
                "best_params": optimization_result.best_params,
                "best_score": optimization_result.best_score,
                "total_evaluations": optimization_result.total_evaluations,
                "optimization_time": optimization_result.optimization_time,
            }

            return final_result

        except Exception as e:
            logger.error(f"æœ€é©åŒ–å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            raise
        finally:
            # ä¾‹å¤–ãŒç™ºç”Ÿã—ãŸå ´åˆã§ã‚‚Optunaãƒªã‚½ãƒ¼ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if optimizer is not None:
                try:
                    optimizer.cleanup()
                except Exception as cleanup_error:
                    logger.warning(
                        f"ä¾‹å¤–å‡¦ç†ã§ã®OptunaOptimizer ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {cleanup_error}"
                    )

    def _prepare_parameter_space(
        self, parameter_space_config: Dict[str, Dict[str, Any]]
    ) -> Dict[str, ParameterSpace]:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“è¨­å®šã‚’ParameterSpaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›

        Args:
            parameter_space_config: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“è¨­å®š

        Returns:
            ParameterSpaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¾æ›¸
        """
        parameter_space = {}

        for param_name, param_config in parameter_space_config.items():
            param_type = param_config["type"]
            low = param_config.get("low")
            high = param_config.get("high")

            # integerå‹ã®å ´åˆã¯ã€lowã¨highã‚’æ•´æ•°ã«å¤‰æ›
            if param_type == "integer" and low is not None and high is not None:
                low = int(low)
                high = int(high)

            parameter_space[param_name] = ParameterSpace(
                type=param_type,
                low=low,
                high=high,
                categories=param_config.get("categories"),
            )

        return parameter_space

    def _create_objective_function(
        self,
        training_data: pd.DataFrame,
        optimization_settings: "OptimizationSettings",
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        trainer: Optional[Any] = None,  # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’å—ã‘å–ã‚‹
        **base_training_params,
    ) -> Callable[[Dict[str, Any]], float]:
        """
        æœ€é©åŒ–ã®ãŸã‚ã®ç›®çš„é–¢æ•°ã‚’ä½œæˆ

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            trainer: ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            **base_training_params: ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ç›®çš„é–¢æ•°ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã‚¹ã‚³ã‚¢ã‚’è¿”ã™é–¢æ•°ï¼‰
        """
        # è©¦è¡Œå›æ•°ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        evaluation_count = 0

        def objective_function(params: Dict[str, Any]) -> float:
            """
            ç›®çš„é–¢æ•°ï¼šä¸ãˆã‚‰ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒŸãƒ‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¿”ã™

            Args:
                params: æœ€é©åŒ–å¯¾è±¡ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

            Returns:
                è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆF1ã‚¹ã‚³ã‚¢ï¼‰
            """
            nonlocal evaluation_count
            evaluation_count += 1

            try:
                logger.info(
                    f"ğŸ” æœ€é©åŒ–è©¦è¡Œ {evaluation_count}/{optimization_settings.n_calls}"
                )
                logger.info(f"ğŸ“‹ è©¦è¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params}")

                # ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
                training_params = {**base_training_params, **params}

                # ä¸€æ™‚çš„ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆï¼ˆå…ƒã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
                temp_ensemble_config = {
                    "method": "stacking",
                    "stacking_params": {
                        "base_models": ["lightgbm", "xgboost"],
                        "meta_model": "lightgbm",
                        "cv_folds": 3,  # æœ€é©åŒ–ä¸­ã¯é«˜é€ŸåŒ–ã®ãŸã‚å°‘ãªã‚
                        "use_probas": True,
                        "random_state": 42,
                    },
                }

                temp_trainer = EnsembleTrainer(ensemble_config=temp_ensemble_config)

                # ãƒŸãƒ‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œï¼ˆä¿å­˜ã¯ã—ãªã„ï¼‰
                result = temp_trainer.train_model(
                    training_data=training_data,
                    funding_rate_data=funding_rate_data,
                    open_interest_data=open_interest_data,
                    save_model=False,  # æœ€é©åŒ–ä¸­ã¯ä¿å­˜ã—ãªã„
                    model_name=None,
                    **training_params,
                )

                # F1ã‚¹ã‚³ã‚¢ã‚’è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨
                f1_score = result.get("f1_score", 0.0)

                # ãƒã‚¯ãƒ­å¹³å‡F1ã‚¹ã‚³ã‚¢ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆ
                if "classification_report" in result:
                    macro_f1 = (
                        result["classification_report"]
                        .get("macro avg", {})
                        .get("f1-score", f1_score)
                    )
                    f1_score = macro_f1

                logger.info(f"ğŸ“Š è©¦è¡Œçµæœ: F1ã‚¹ã‚³ã‚¢={f1_score:.4f}")
                logger.info("-" * 50)
                return f1_score

            except Exception as e:
                logger.warning(f"ç›®çš„é–¢æ•°è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ä½ã„ã‚¹ã‚³ã‚¢ã‚’è¿”ã™
                return 0.0

        return objective_function

    def _cleanup_temporary_files(self, level: CleanupLevel):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # MLTrainingServiceã§ã¯ç‰¹ã«ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œæˆã—ãªã„ãŸã‚ã€ãƒ‘ã‚¹
        pass

    def _cleanup_cache(self, level: CleanupLevel):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # MLTrainingServiceã§ã¯ç‰¹ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ç®¡ç†ã—ãªã„ãŸã‚ã€ãƒ‘ã‚¹
        pass

    def _cleanup_models(self, level: CleanupLevel):
        """ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if hasattr(self, "trainer") and self.trainer:
                if hasattr(self.trainer, "cleanup_resources"):
                    self.trainer.cleanup_resources(level)
                    logger.debug("ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")

        except Exception as e:
            logger.warning(f"MLTrainingServiceãƒ¢ãƒ‡ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰
ml_training_service = MLTrainingService(trainer_type="ensemble")
