"""
ç‰¹å¾´é‡é¸æŠã‚·ã‚¹ãƒ†ãƒ 

åˆ†æå ±å‘Šæ›¸ã§ææ¡ˆã•ã‚ŒãŸçµ±è¨ˆçš„ç‰¹å¾´é‡é¸æŠã¨ML-basedç‰¹å¾´é‡é¸æŠã‚’å®Ÿè£…ã€‚
é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãªç‰¹å¾´é‡ã‚’åŠ¹ç‡çš„ã«é¸æŠã—ã¾ã™ã€‚
"""

import logging
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import (
    RFE,
    RFECV,
    SelectFromModel,
    SelectKBest,
    chi2,
    f_classif,
    mutual_info_classif,
)
from sklearn.inspection import permutation_importance
from sklearn.linear_model import LassoCV

logger = logging.getLogger(__name__)


class SelectionMethod(Enum):
    """ç‰¹å¾´é‡é¸æŠæ‰‹æ³•"""

    # çµ±è¨ˆçš„æ‰‹æ³•
    UNIVARIATE_F = "univariate_f"
    UNIVARIATE_CHI2 = "univariate_chi2"
    MUTUAL_INFO = "mutual_info"

    # ML-basedæ‰‹æ³•
    LASSO = "lasso"
    RANDOM_FOREST = "random_forest"
    RFE = "rfe"
    RFECV = "rfecv"
    PERMUTATION = "permutation"

    # çµ„ã¿åˆã‚ã›æ‰‹æ³•
    ENSEMBLE = "ensemble"


@dataclass
class FeatureSelectionConfig:
    """ç‰¹å¾´é‡é¸æŠè¨­å®š"""

    method: SelectionMethod = SelectionMethod.ENSEMBLE
    k_features: Optional[int] = None  # é¸æŠã™ã‚‹ç‰¹å¾´é‡æ•°
    percentile: float = 50  # ä¸Šä½ä½•%ã‚’é¸æŠã™ã‚‹ã‹
    cv_folds: int = 5  # RFECVã§ã®åˆ†å‰²æ•°
    random_state: int = 42
    n_jobs: int = -1

    # é–¾å€¤è¨­å®š
    importance_threshold: float = 0.01
    correlation_threshold: float = 0.95

    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š
    ensemble_methods: Optional[List[SelectionMethod]] = None
    ensemble_voting: str = "majority"  # "majority" or "unanimous"


class FeatureSelector:
    """
    ç‰¹å¾´é‡é¸æŠå™¨

    è¤‡æ•°ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦æœ€é©ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’é¸æŠã—ã¾ã™ã€‚
    """

    def __init__(self, config: Optional[FeatureSelectionConfig] = None):
        """
        åˆæœŸåŒ–

        Args:
            config: ç‰¹å¾´é‡é¸æŠè¨­å®š
        """
        self.config = config or FeatureSelectionConfig()
        if self.config.ensemble_methods is None:
            self.config.ensemble_methods = [
                SelectionMethod.MUTUAL_INFO,
                SelectionMethod.RANDOM_FOREST,
                SelectionMethod.LASSO,
            ]

        self.selected_features_ = None
        self.feature_scores_ = None
        self.selection_results_ = {}

    def fit_transform(
        self, X: pd.DataFrame, y: pd.Series
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        ç‰¹å¾´é‡é¸æŠã‚’å®Ÿè¡Œ

        Args:
            X: ç‰¹å¾´é‡DataFrame
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆSeries

        Returns:
            é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®DataFrameã¨é¸æŠçµæœã®è¾æ›¸
        """
        logger.info(f"ğŸ¯ ç‰¹å¾´é‡é¸æŠé–‹å§‹: {self.config.method.value}")
        logger.info(f"å…¥åŠ›ç‰¹å¾´é‡æ•°: {X.shape[1]}, ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")

        # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        X_processed, feature_names = self._preprocess_data(X)

        # ç‰¹å¾´é‡é¸æŠå®Ÿè¡Œ
        if self.config.method == SelectionMethod.ENSEMBLE:
            selected_features, results = self._ensemble_selection(
                X_processed, y, feature_names
            )
        else:
            selected_features, results = self._single_method_selection(
                X_processed, y, feature_names, self.config.method
            )

        # çµæœã®ä¿å­˜
        self.selected_features_ = selected_features
        self.selection_results_ = results

        # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã§DataFrameã‚’ä½œæˆ
        if selected_features:
            X_selected = X[selected_features].copy()
        else:
            # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ãŒãªã„å ´åˆã¯ç©ºã®DataFrameã‚’è¿”ã™
            X_selected = pd.DataFrame(index=X.index)

        # çµæœãŒDataFrameã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼
        if not isinstance(X_selected, pd.DataFrame):
            X_selected = pd.DataFrame(X_selected)

        logger.info(f"âœ… ç‰¹å¾´é‡é¸æŠå®Œäº†: {len(selected_features)}å€‹ã®ç‰¹å¾´é‡ã‚’é¸æŠ")
        logger.info(f"é¸æŠç‡: {len(selected_features) / X.shape[1] * 100:.1f}%")

        return X_selected, results

    def _preprocess_data(self, X: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
        """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        # æ¬ æå€¤ã®å‡¦ç†
        X_filled = X.fillna(X.median())

        # ç„¡é™å€¤ã®å‡¦ç†
        X_filled = X_filled.replace([np.inf, -np.inf], np.nan)
        X_filled = X_filled.fillna(X_filled.median())

        # å®šæ•°ç‰¹å¾´é‡ã®é™¤å»
        constant_features = X_filled.columns[X_filled.nunique() <= 1].tolist()
        if constant_features:
            logger.info(f"å®šæ•°ç‰¹å¾´é‡ã‚’é™¤å»: {len(constant_features)}å€‹")
            X_filled = X_filled.drop(columns=constant_features)

        # é«˜ç›¸é–¢ç‰¹å¾´é‡ã®é™¤å»
        X_filled = self._remove_highly_correlated_features(X_filled)

        return X_filled.values, X_filled.columns.tolist()

    def _remove_highly_correlated_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """é«˜ç›¸é–¢ç‰¹å¾´é‡ã‚’é™¤å»"""
        try:
            corr_matrix = X.corr().abs()
            upper_triangle = corr_matrix.where(
                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
            )

            to_drop = [
                column
                for column in upper_triangle.columns
                if any(upper_triangle[column] > self.config.correlation_threshold)
            ]

            if to_drop:
                logger.info(f"é«˜ç›¸é–¢ç‰¹å¾´é‡ã‚’é™¤å»: {len(to_drop)}å€‹")
                X = X.drop(columns=to_drop)

        except Exception as e:
            logger.warning(f"ç›¸é–¢é™¤å»ã‚¨ãƒ©ãƒ¼: {e}")

        return X

    def _ensemble_selection(
        self, X: np.ndarray, y: pd.Series, feature_names: List[str]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç‰¹å¾´é‡é¸æŠ"""
        logger.info("ğŸ”„ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç‰¹å¾´é‡é¸æŠã‚’å®Ÿè¡Œ")

        method_results = {}
        feature_votes = {name: 0 for name in feature_names}

        # ensemble_methods ãŒ None ã®å ´åˆã®å‡¦ç†
        ensemble_methods = self.config.ensemble_methods or [
            SelectionMethod.MUTUAL_INFO,
            SelectionMethod.RANDOM_FOREST,
            SelectionMethod.LASSO,
        ]

        # å„æ‰‹æ³•ã§ç‰¹å¾´é‡é¸æŠã‚’å®Ÿè¡Œ
        for method in ensemble_methods:
            try:
                selected_features, result = self._single_method_selection(
                    X, y, feature_names, method
                )
                method_results[method.value] = result

                # æŠ•ç¥¨
                for feature in selected_features:
                    feature_votes[feature] += 1

                logger.info(f"{method.value}: {len(selected_features)}å€‹é¸æŠ")

            except Exception as e:
                logger.warning(f"{method.value}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # æŠ•ç¥¨çµæœã«åŸºã¥ã„ã¦æœ€çµ‚é¸æŠ
        n_methods = len(ensemble_methods)

        if self.config.ensemble_voting == "unanimous":
            # å…¨æ‰‹æ³•ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã®ã¿
            threshold = n_methods
        else:
            # éåŠæ•°ã§é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡
            threshold = max(1, n_methods // 2)

        selected_features = [
            feature for feature, votes in feature_votes.items() if votes >= threshold
        ]

        # æœ€å°ç‰¹å¾´é‡æ•°ã®ä¿è¨¼
        if len(selected_features) < 5:
            # æŠ•ç¥¨æ•°é †ã§ãƒˆãƒƒãƒ—5ã‚’é¸æŠ
            sorted_features = sorted(
                feature_votes.items(), key=lambda x: x[1], reverse=True
            )
            selected_features = [f[0] for f in sorted_features[:5]]

        results = {
            "method": "ensemble",
            "ensemble_methods": [m.value for m in ensemble_methods],
            "method_results": method_results,
            "feature_votes": feature_votes,
            "voting_threshold": threshold,
            "selected_features": selected_features,
        }

        return selected_features, results

    def _single_method_selection(
        self,
        X: np.ndarray,
        y: pd.Series,
        feature_names: List[str],
        method: SelectionMethod,
    ) -> Tuple[List[str], Dict[str, Any]]:
        """å˜ä¸€æ‰‹æ³•ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ"""

        if method == SelectionMethod.UNIVARIATE_F:
            return self._univariate_selection(X, y, feature_names, f_classif)
        elif method == SelectionMethod.UNIVARIATE_CHI2:
            return self._univariate_selection(X, y, feature_names, chi2)
        elif method == SelectionMethod.MUTUAL_INFO:
            return self._mutual_info_selection(X, y, feature_names)
        elif method == SelectionMethod.LASSO:
            return self._lasso_selection(X, y, feature_names)
        elif method == SelectionMethod.RANDOM_FOREST:
            return self._random_forest_selection(X, y, feature_names)
        elif method == SelectionMethod.RFE:
            return self._rfe_selection(X, y, feature_names)
        elif method == SelectionMethod.RFECV:
            return self._rfecv_selection(X, y, feature_names)
        elif method == SelectionMethod.PERMUTATION:
            return self._permutation_selection(X, y, feature_names)
        else:
            raise ValueError(f"æœªå¯¾å¿œã®æ‰‹æ³•: {method}")

    def _univariate_selection(
        self, X: np.ndarray, y: pd.Series, feature_names: List[str], score_func
    ) -> Tuple[List[str], Dict[str, Any]]:
        """å˜å¤‰é‡çµ±è¨ˆçš„é¸æŠ"""
        try:
            # Chi2ã®å ´åˆã¯éè² å€¤ã«å¤‰æ›
            if score_func == chi2:
                X_transformed = X - X.min(axis=0) + 1e-8
            else:
                X_transformed = X

            k = self.config.k_features or max(5, int(len(feature_names) * 0.3))
            selector = SelectKBest(score_func=score_func, k=k)
            selector.fit(X_transformed, y)

            selected_mask = selector.get_support()
            # selected_mask ãŒ None ã®å ´åˆã®å‡¦ç†ã‚’è¿½åŠ 
            if selected_mask is None:
                selected_mask = np.zeros(len(feature_names), dtype=bool)
                selected_mask[: min(k, len(feature_names))] = True

            selected_features = [
                feature_names[i] for i in range(len(feature_names)) if selected_mask[i]
            ]

            results = {
                "method": score_func.__name__,
                "scores": (
                    selector.scores_.tolist() if hasattr(selector, "scores_") else []
                ),
                "selected_features": selected_features,
                "k": k,
            }

            return selected_features, results

        except Exception as e:
            logger.error(f"å˜å¤‰é‡é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€åˆã®5ã¤ã®ç‰¹å¾´é‡ã‚’è¿”ã™
            fallback_features = (
                feature_names[: min(5, len(feature_names))] if feature_names else []
            )
            return fallback_features, {"error": str(e)}

    def _mutual_info_selection(
        self, X: np.ndarray, y: pd.Series, feature_names: List[str]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """ç›¸äº’æƒ…å ±é‡ã«ã‚ˆã‚‹é¸æŠ"""
        try:
            k = self.config.k_features or max(5, int(len(feature_names) * 0.3))
            selector = SelectKBest(score_func=mutual_info_classif, k=k)
            selector.fit(X, y)

            selected_mask = selector.get_support()
            # selected_mask ãŒ None ã®å ´åˆã®å‡¦ç†ã‚’è¿½åŠ 
            if selected_mask is None:
                selected_mask = np.zeros(len(feature_names), dtype=bool)
                selected_mask[: min(k, len(feature_names))] = True

            selected_features = [
                feature_names[i] for i in range(len(feature_names)) if selected_mask[i]
            ]

            results = {
                "method": "mutual_info",
                "scores": (
                    selector.scores_.tolist() if hasattr(selector, "scores_") else []
                ),
                "selected_features": selected_features,
                "k": k,
            }

            return selected_features, results

        except Exception as e:
            logger.error(f"ç›¸äº’æƒ…å ±é‡é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€åˆã®5ã¤ã®ç‰¹å¾´é‡ã‚’è¿”ã™
            fallback_features = (
                feature_names[: min(5, len(feature_names))] if feature_names else []
            )
            return fallback_features, {"error": str(e)}

    def _lasso_selection(
        self, X: np.ndarray, y: pd.Series, feature_names: List[str]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """Lassoå›å¸°ã«ã‚ˆã‚‹é¸æŠ"""
        try:
            lasso = LassoCV(
                cv=self.config.cv_folds, random_state=self.config.random_state
            )
            lasso.fit(X, y)  # æ˜ç¤ºçš„ã«fitã‚’å®Ÿè¡Œ

            selector = SelectFromModel(
                lasso, threshold=self.config.importance_threshold, prefit=True
            )

            selected_mask = selector.get_support()
            # selected_mask ãŒ None ã®å ´åˆã®å‡¦ç†ã‚’è¿½åŠ 
            if selected_mask is None:
                # ä¿‚æ•°ã®çµ¶å¯¾å€¤ã«åŸºã¥ã„ã¦é¸æŠ
                importances = np.abs(lasso.coef_)
                threshold = np.sort(importances)[-min(5, len(importances))]
                selected_mask = importances >= threshold

            selected_features = [
                feature_names[i] for i in range(len(feature_names)) if selected_mask[i]
            ]

            # æœ€å°ç‰¹å¾´é‡æ•°ã®ä¿è¨¼
            if len(selected_features) < 5:
                importances = np.abs(lasso.coef_)
                top_indices = np.argsort(importances)[-min(5, len(importances)) :]
                selected_features = [feature_names[i] for i in top_indices]

            results = {
                "method": "lasso",
                "coefficients": lasso.coef_.tolist(),
                "alpha": lasso.alpha_,
                "selected_features": selected_features,
            }

            return selected_features, results

        except Exception as e:
            logger.error(f"Lassoé¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€åˆã®5ã¤ã®ç‰¹å¾´é‡ã‚’è¿”ã™
            fallback_features = (
                feature_names[: min(5, len(feature_names))] if feature_names else []
            )
            return fallback_features, {"error": str(e)}

    def _random_forest_selection(
        self, X: np.ndarray, y: pd.Series, feature_names: List[str]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã«ã‚ˆã‚‹é¸æŠ"""
        try:
            rf = RandomForestClassifier(
                n_estimators=100,
                random_state=self.config.random_state,
                n_jobs=self.config.n_jobs,
            )
            rf.fit(X, y)  # æ˜ç¤ºçš„ã«fitã‚’å®Ÿè¡Œ

            selector = SelectFromModel(
                rf, threshold=self.config.importance_threshold, prefit=True
            )

            selected_mask = selector.get_support()
            # selected_mask ãŒ None ã®å ´åˆã®å‡¦ç†ã‚’è¿½åŠ 
            if selected_mask is None:
                # é‡è¦åº¦ã«åŸºã¥ã„ã¦é¸æŠ
                importances = rf.feature_importances_
                threshold = np.sort(importances)[-min(5, len(importances))]
                selected_mask = importances >= threshold

            selected_features = [
                feature_names[i] for i in range(len(feature_names)) if selected_mask[i]
            ]

            # æœ€å°ç‰¹å¾´é‡æ•°ã®ä¿è¨¼
            if len(selected_features) < 5:
                importances = rf.feature_importances_
                top_indices = np.argsort(importances)[-min(5, len(importances)) :]
                selected_features = [feature_names[i] for i in top_indices]

            results = {
                "method": "random_forest",
                "feature_importances": rf.feature_importances_.tolist(),
                "selected_features": selected_features,
            }

            return selected_features, results

        except Exception as e:
            logger.error(f"ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆé¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€åˆã®5ã¤ã®ç‰¹å¾´é‡ã‚’è¿”ã™
            fallback_features = (
                feature_names[: min(5, len(feature_names))] if feature_names else []
            )
            return fallback_features, {"error": str(e)}

    def _rfe_selection(
        self, X: np.ndarray, y: pd.Series, feature_names: List[str]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """å†å¸°çš„ç‰¹å¾´é‡é™¤å»"""
        try:
            estimator = RandomForestClassifier(
                n_estimators=50, random_state=self.config.random_state
            )
            n_features = self.config.k_features or max(5, int(len(feature_names) * 0.3))
            selector = RFE(estimator, n_features_to_select=n_features)
            selector.fit(X, y)

            selected_mask = selector.get_support()
            # selected_mask ãŒ None ã®å ´åˆã®å‡¦ç†ã‚’è¿½åŠ 
            if selected_mask is None:
                selected_mask = np.zeros(len(feature_names), dtype=bool)
                selected_mask[: min(n_features, len(feature_names))] = True

            selected_features = [
                feature_names[i] for i in range(len(feature_names)) if selected_mask[i]
            ]

            results = {
                "method": "rfe",
                "ranking": (
                    selector.ranking_.tolist() if hasattr(selector, "ranking_") else []
                ),
                "selected_features": selected_features,
                "n_features": n_features,
            }

            return selected_features, results

        except Exception as e:
            logger.error(f"RFEé¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€åˆã®5ã¤ã®ç‰¹å¾´é‡ã‚’è¿”ã™
            fallback_features = (
                feature_names[: min(5, len(feature_names))] if feature_names else []
            )
            return fallback_features, {"error": str(e)}

    def _rfecv_selection(
        self, X: np.ndarray, y: pd.Series, feature_names: List[str]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãå†å¸°çš„ç‰¹å¾´é‡é™¤å»"""
        try:
            estimator = RandomForestClassifier(
                n_estimators=50, random_state=self.config.random_state
            )
            selector = RFECV(estimator, cv=self.config.cv_folds)
            selector.fit(X, y)

            selected_mask = selector.get_support()
            # selected_mask ãŒ None ã®å ´åˆã®å‡¦ç†ã‚’è¿½åŠ 
            if selected_mask is None:
                # cv_results_ ã‹ã‚‰æœ€é©ãªç‰¹å¾´é‡æ•°ã‚’å–å¾—
                if (
                    hasattr(selector, "cv_results_")
                    and "mean_test_score" in selector.cv_results_
                ):
                    n_features = np.argmax(selector.cv_results_["mean_test_score"]) + 1
                else:
                    n_features = min(5, len(feature_names))
                selected_mask = np.zeros(len(feature_names), dtype=bool)
                selected_mask[:n_features] = True

            selected_features = [
                feature_names[i] for i in range(len(feature_names)) if selected_mask[i]
            ]

            results = {
                "method": "rfecv",
                "ranking": (
                    selector.ranking_.tolist() if hasattr(selector, "ranking_") else []
                ),
                "cv_scores": (
                    selector.cv_results_["mean_test_score"].tolist()
                    if hasattr(selector, "cv_results_")
                    and "mean_test_score" in selector.cv_results_
                    else []
                ),
                "optimal_features": (
                    selector.n_features_
                    if hasattr(selector, "n_features_")
                    else len(selected_features)
                ),
                "selected_features": selected_features,
            }

            return selected_features, results

        except Exception as e:
            logger.error(f"RFECVé¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€åˆã®5ã¤ã®ç‰¹å¾´é‡ã‚’è¿”ã™
            fallback_features = (
                feature_names[: min(5, len(feature_names))] if feature_names else []
            )
            return fallback_features, {"error": str(e)}

    def _permutation_selection(
        self, X: np.ndarray, y: pd.Series, feature_names: List[str]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """é †åˆ—é‡è¦åº¦ã«ã‚ˆã‚‹é¸æŠ"""
        try:
            estimator = RandomForestClassifier(
                n_estimators=50, random_state=self.config.random_state
            )
            estimator.fit(X, y)

            perm_importance = permutation_importance(
                estimator, X, y, n_repeats=5, random_state=self.config.random_state
            )

            # é‡è¦åº¦ã®é–¾å€¤ä»¥ä¸Šã®ç‰¹å¾´é‡ã‚’é¸æŠ
            # å®‰å…¨ã«å±æ€§ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹
            if isinstance(perm_importance, dict):
                importances_mean = perm_importance.get(
                    "importances_mean", np.zeros(len(feature_names))
                )
            else:
                importances_mean = getattr(
                    perm_importance, "importances_mean", np.zeros(len(feature_names))
                )

            # numpyé…åˆ—ã«å¤‰æ›
            if not isinstance(importances_mean, np.ndarray):
                importances_mean = np.array(importances_mean)

            important_features = importances_mean > self.config.importance_threshold
            selected_features = [
                feature_names[i]
                for i in range(len(feature_names))
                if i < len(important_features) and important_features[i]
            ]

            # æœ€å°ç‰¹å¾´é‡æ•°ã®ä¿è¨¼
            if len(selected_features) < 5:
                top_indices = np.argsort(importances_mean)[
                    -min(5, len(importances_mean)) :
                ]
                selected_features = [feature_names[i] for i in top_indices]

            # importances_std ã®å‡¦ç†
            if isinstance(perm_importance, dict):
                importances_std = perm_importance.get("importances_std", [])
            else:
                importances_std = getattr(perm_importance, "importances_std", [])

            # numpyé…åˆ—ã«å¤‰æ›
            if not isinstance(importances_std, np.ndarray):
                importances_std = (
                    np.array(importances_std)
                    if len(importances_std) > 0
                    else np.zeros(len(importances_mean))
                )

            results = {
                "method": "permutation",
                "importances_mean": importances_mean.tolist(),
                "importances_std": importances_std.tolist(),
                "selected_features": selected_features,
            }

            return selected_features, results

        except Exception as e:
            logger.error(f"é †åˆ—é‡è¦åº¦é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€åˆã®5ã¤ã®ç‰¹å¾´é‡ã‚’è¿”ã™
            fallback_features = (
                feature_names[: min(5, len(feature_names))] if feature_names else []
            )
            return fallback_features, {"error": str(e)}
