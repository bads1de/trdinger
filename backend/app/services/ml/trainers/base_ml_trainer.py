"""
MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»å‰å‡¦ç†ãƒ»ä¿å­˜ã«é–¢ã‚ã‚‹å…±é€šãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import logging
from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

from ....config.unified_config import unified_config
from ....utils.error_handler import (
    DataError,
    ml_operation_context,
    safe_ml_operation,
)
from ..common.base_resource_manager import BaseResourceManager, CleanupLevel
from ..common.utils import (
    get_feature_importance_unified,
    prepare_data_for_prediction,
    get_t1_series,
)
from ..cross_validation import PurgedKFold
from ..common.exceptions import MLModelError
from ..feature_engineering.feature_engineering_service import FeatureEngineeringService
from ..label_generation.label_generation_service import LabelGenerationService
from ..common.registry import ModelMetadata
from ..models.model_manager import model_manager

logger = logging.getLogger(__name__)


class BaseMLTrainer(BaseResourceManager, ABC):
    """
    MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

    å…±é€šã®å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã—ã€å…·ä½“çš„ãªå®Ÿè£…ã¯ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§è¡Œã„ã¾ã™ã€‚
    å˜ä¸€è²¬ä»»åŸå‰‡ã«å¾“ã„ã€å­¦ç¿’ã«é–¢ã™ã‚‹è²¬ä»»ã®ã¿ã‚’æŒã¡ã¾ã™ã€‚
    """

    def __init__(
        self,
        trainer_config: Optional[Dict[str, Any]] = None,
        trainer_type: Optional[str] = None,
        model_type: Optional[str] = None,
    ):
        """
        åˆæœŸåŒ–

        Args:
            trainer_config: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®š
            trainer_type: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆäº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰
            model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆäº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰
        """
        # BaseResourceManagerã®åˆæœŸåŒ–
        super().__init__()

        self.feature_service = FeatureEngineeringService()
        self.label_service = LabelGenerationService()
        logger.debug(
            "ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã¨ãƒ©ãƒ™ãƒ«ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ"
        )

        self.trainer_config = trainer_config or {}

        # ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã¯ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ä¿æŒï¼ˆäº’æ›æ€§ï¼‰
        self.trainer_type = trainer_type or self.trainer_config.get("type", "single")
        self.model_type = model_type or self.trainer_config.get(
            "model_type", "lightgbm"
        )

        self.scaler = StandardScaler()
        self.feature_columns = None
        self.is_trained = False
        self._model = None
        self.current_model_path = None
        self.current_model_metadata = None

    @property
    def config(self):
        """ç¾åœ¨ã®çµ±ä¸€MLè¨­å®šã‚’å–å¾—"""
        return unified_config.ml

    @property
    def model(self) -> Any:
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        return self._model

    @safe_ml_operation(
        default_return={"success": False}, context="MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )
    def train_model(
        self,
        training_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        save_model: bool = True,
        model_name: Optional[str] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ï¼‰

        ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€ç‰¹å¾´é‡è¨ˆç®—ã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¾ãŸã¯ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆåˆ†å‰²ã€
        ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã€ãŠã‚ˆã³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã®ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚’ä¸€é€£ã®ãƒ•ãƒ­ãƒ¼ã¨ã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_model: å­¦ç¿’å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’æ°¸ç¶šåŒ–ã™ã‚‹ã‹ã©ã†ã‹
            model_name: ä¿å­˜æ™‚ã®ãƒ¢ãƒ‡ãƒ«åï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåï¼‰
            **training_params:
                - use_cross_validation (bool): CVã‚’å®Ÿè¡Œã™ã‚‹ã‹
                - test_size (float): ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆåˆ†å‰²æ¯”ç‡
                - cv_splits (int): CVåˆ†å‰²æ•°
                - random_state (int): ä¹±æ•°ã‚·ãƒ¼ãƒ‰

        Returns:
            å­¦ç¿’çµæœã€è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ç­‰ã‚’å«ã‚€è¾æ›¸
        """
        with ml_operation_context("MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’"):
            if training_data is None or len(training_data) < 100:
                raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

            # 1. ç‰¹å¾´é‡è¨ˆç®—ã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            X, y = self._prepare_training_data(
                self._calculate_features(
                    training_data, funding_rate_data, open_interest_data
                ),
                training_data,
                **training_params,
            )
            if X is None or X.empty:
                raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

            # 2. å­¦ç¿’å®Ÿè¡Œ (CV or Single)
            if training_params.get("use_cross_validation", False):
                cv_res = self._time_series_cross_validate(X, y, **training_params)
                # å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚å­¦ç¿’
                X_s = self._preprocess_data(X, X)[0]
                idx = int(len(X) * (1 - training_params.get("test_size", 0.2)))
                res = self._train_model_impl(
                    X_s.iloc[:idx],
                    X_s.iloc[idx:],
                    y.iloc[:idx],
                    y.iloc[idx:],
                    **training_params,
                )
                res.update(cv_res)
            else:
                X_tr, X_te, y_tr, y_te = self._split_data(X, y, **training_params)
                X_tr_s, X_te_s = self._preprocess_data(X_tr, X_te)
                res = self._train_model_impl(
                    X_tr_s, X_te_s, y_tr, y_te, **training_params
                )

            self.is_trained = True

            # 3. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if save_model:
                meta = ModelMetadata.from_training_result(
                    res,
                    training_params,
                    self.__class__.__name__,
                    len(self.feature_columns or []),
                )
                if not meta.validate()["is_valid"]:
                    logger.warning(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è­¦å‘Š: {meta.validate()['warnings']}")

                path = self.save_model(
                    model_name or self.config.model.auto_strategy_model_name,
                    meta.to_dict(),
                )
                res["model_path"] = self.current_model_path = path
                self.current_model_metadata = meta.to_dict()

            return self._format_training_result(res, X, y)

    @abstractmethod
    def predict(self, features_df: pd.DataFrame) -> np.ndarray:
        """
        äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """

    def predict_signal(self, features_df: pd.DataFrame) -> Dict[str, float]:
        """
        æœ€æ–°ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚·ã‚°ãƒŠãƒ«ï¼ˆæœ‰åŠ¹ç¢ºç‡ï¼‰ã‚’äºˆæ¸¬

        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã€æœŸå¾…ã•ã‚Œã‚‹å½¢å¼ã¸ã®å¤‰æ›ã€ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’è¡Œã„ã€
        æœ€çµ‚çš„ãªã€Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ãŒæœ‰åŠ¹ã§ã‚ã‚‹ç¢ºç‡ã€ã‚’è¿”ã—ã¾ã™ã€‚

        Args:
            features_df: ç‰¹å¾´é‡DataFrameï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰

        Returns:
            äºˆæ¸¬ç¢ºç‡ã®è¾æ›¸:
            - {"is_valid": float}: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãŒæœ‰åŠ¹ã§ã‚ã‚‹æœŸå¾…ç¢ºç‡ (0.0ã€œ1.0)
        """
        if not self.is_trained:
            logger.warning("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return self.config.prediction.get_default_predictions()

        try:
            # 1. å‰å‡¦ç†ï¼ˆã‚«ãƒ©ãƒ èª¿æ•´ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰- å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ç›´æ¥ä½¿ç”¨
            processed_features = prepare_data_for_prediction(
                features_df,
                expected_columns=self.feature_columns,
                scaler=self.scaler,
            )

            # 2. äºˆæ¸¬å®Ÿè¡Œï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã®predictã‚’å‘¼ã³å‡ºã—ï¼‰
            # predictã¯ç¢ºç‡é…åˆ—ã‚’è¿”ã™ã“ã¨ã‚’æœŸå¾…
            predictions = self.predict(processed_features)

            # 3. æœ€æ–°ã®äºˆæ¸¬çµæœã‚’å–å¾—ï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯æœ€å¾Œã®è¡Œï¼‰
            if predictions.ndim == 2:
                latest_pred = predictions[-1]
            else:
                latest_pred = predictions

            # 4. çµæœã®æ•´å½¢ï¼ˆäºŒå€¤åˆ†é¡å°‚ç”¨ï¼‰
            if latest_pred.shape[0] >= 2:
                # 2ã‚¯ãƒ©ã‚¹åˆ†é¡
                return {
                    "is_valid": float(latest_pred[1]),
                }
            else:
                logger.error(f"äºˆæœŸã—ãªã„ã‚¯ãƒ©ã‚¹æ•°: {latest_pred.shape[0]}")
                return self.config.prediction.get_default_predictions()

        except Exception as e:
            logger.error(f"ã‚·ã‚°ãƒŠãƒ«äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return self.config.prediction.get_default_predictions()

    @abstractmethod
    def _train_model_impl(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè£…ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        """

    def _calculate_features(
        self,
        ohlcv_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡é›†åˆã‚’è¨ˆç®—"""
        try:
            if ohlcv_data is None or ohlcv_data.empty:
                raise ValueError("OHLCVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

            profile = unified_config.ml.feature_engineering.profile
            logger.info(f"ğŸ“Š ç‰¹å¾´é‡è¨ˆç®—ã‚’å®Ÿè¡Œä¸­ï¼ˆprofile: {profile}ï¼‰...")

            basic_features = self.feature_service.calculate_advanced_features(
                ohlcv_data=ohlcv_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                profile=profile,
            )

            logger.info(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(basic_features.columns)}å€‹ã®ç‰¹å¾´é‡")
            return basic_features

        except Exception as e:
            logger.warning(f"ç‰¹å¾´é‡è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼ã€åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨: {e}")
            return ohlcv_data.copy()

    def _prepare_training_data(
        self, features_df: pd.DataFrame, ohlcv_df: pd.DataFrame, **training_params
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        try:
            features_clean, labels_numeric = self.label_service.prepare_labels(
                features_df, ohlcv_df, **training_params
            )

            self.feature_columns = features_clean.columns.tolist()
            return features_clean, labels_numeric

        except Exception as e:
            logger.error(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise DataError(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def _split_data(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ï¼ˆå¸¸ã«æ™‚ç³»åˆ—åˆ†å‰²ï¼‰"""
        test_size = training_params.get("test_size", 0.2)

        logger.info("ğŸ•’ æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨")
        n_samples = len(X)
        train_size = int(n_samples * (1 - test_size))

        X_train = X.iloc[:train_size].copy()
        X_test = X.iloc[train_size:].copy()
        y_train = y.iloc[:train_size].copy()
        y_test = y.iloc[train_size:].copy()

        return X_train, X_test, y_train, y_test

    def _time_series_cross_validate(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Dict[str, Any]:
        """æ™‚é–“è»¸ã‚’è€ƒæ…®ã—ãŸãƒ‘ãƒ¼ã‚¸ãƒ³ã‚°ãƒ»ã‚¨ãƒ³ãƒãƒ¼ã‚´ä»˜ãã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        n_splits = training_params.get("cv_splits", self.config.training.cv_folds)
        logger.info(f"ğŸ”„ æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ï¼ˆ{n_splits}åˆ†å‰²ï¼‰")

        t1_horizon_n = self.config.training.prediction_horizon

        t1 = get_t1_series(
            X.index,
            t1_horizon_n,
            timeframe=self.config.training.label_generation.timeframe,
        )

        pct_embargo = getattr(self.config.training, "pct_embargo", 0.01)
        splitter = PurgedKFold(n_splits=n_splits, t1=t1, pct_embargo=pct_embargo)

        cv_scores = []
        fold_results = []

        for fold, (train_idx, test_idx) in enumerate(splitter.split(X, y)):
            X_train_cv, X_test_cv = X.iloc[train_idx], X.iloc[test_idx]
            y_train_cv, y_test_cv = y.iloc[train_idx], y.iloc[test_idx]

            scaler = StandardScaler()
            X_train_scaled = pd.DataFrame(
                scaler.fit_transform(X_train_cv),
                columns=X_train_cv.columns,
                index=X_train_cv.index,
            )
            X_test_scaled = pd.DataFrame(
                scaler.transform(X_test_cv),
                columns=X_test_cv.columns,
                index=X_test_cv.index,
            )

            fold_result = self._train_fold_with_error_handling(
                fold + 1,
                X_train_scaled,
                X_test_scaled,
                y_train_cv,
                y_test_cv,
                X_train_cv,
                X_test_cv,
                training_params,
            )

            fold_results.append(fold_result)

            score = fold_result.get(
                "balanced_accuracy", fold_result.get("accuracy", 0.0)
            )
            cv_scores.append(score)

        mean_score = np.mean(cv_scores) if cv_scores else 0.0
        std_score = np.std(cv_scores) if cv_scores else 0.0

        logger.info(
            f"âœ… ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†: å¹³å‡ã‚¹ã‚³ã‚¢={mean_score:.4f} (+/- {std_score:.4f})"
        )

        return {
            "cv_scores": cv_scores,
            "mean_score": mean_score,
            "std_score": std_score,
            "fold_results": fold_results,
        }

    def _preprocess_data(
        self, X_train: pd.DataFrame, X_test: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰"""
        if self.scaler is None:
            self.scaler = StandardScaler()

        X_train_scaled = pd.DataFrame(
            self.scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index,
        )
        X_test_scaled = pd.DataFrame(
            self.scaler.transform(X_test), columns=X_test.columns, index=X_test.index
        )
        return X_train_scaled, X_test_scaled

    def _get_model_to_save(self) -> Any:
        """ä¿å­˜å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—"""
        return self._model

    def _get_model_specific_metadata(self, model_name: str) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return {}

    def save_model(
        self, model_name: str, metadata: Optional[Dict[str, Any]] = None
    ) -> Optional[str]:
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ°¸ç¶šåŒ–"""
        if not self.is_trained:
            raise MLModelError("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        final_metadata = {
            "model_type": self.__class__.__name__,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "is_trained": self.is_trained,
        }
        if metadata:
            final_metadata.update(metadata)

        final_metadata.update(self._get_model_specific_metadata(model_name))

        try:
            feature_importance = self.get_feature_importance(top_n=100)
            if feature_importance:
                final_metadata["feature_importance"] = feature_importance
        except Exception as e:
            logger.warning(f"ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—ã«å¤±æ•—: {e}")

        model_to_save = self._get_model_to_save()
        if model_to_save is None:
            logger.warning("ä¿å­˜å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ãŒNoneã§ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è‡ªä½“ã‚’ä¿å­˜ã—ã¾ã™ã€‚")
            model_to_save = self

        model_path = model_manager.save_model(
            model=model_to_save,
            model_name=model_name,
            metadata=final_metadata,
            scaler=self.scaler,
            feature_columns=self.feature_columns,
        )

        logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")
        return model_path

    def _format_training_result(
        self, training_result: Dict[str, Any], X: pd.DataFrame, y: pd.Series
    ) -> Dict[str, Any]:
        """å­¦ç¿’çµæœã‚’æ•´å½¢"""
        result = {
            "success": True,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "total_samples": len(X),
            **training_result,
        }
        return result

    def get_feature_importance(self, top_n: int = 10) -> Dict[str, float]:
        """ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—"""
        if not self.is_trained:
            logger.warning("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}

        return get_feature_importance_unified(
            self._model, self.feature_columns, top_n=top_n
        )

    @safe_ml_operation(
        default_return=False, context="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )
    def load_model(self, model_path: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        model_data = model_manager.load_model(model_path)

        if model_data is None:
            return False

        self._model = model_data.get("model")
        self.scaler = model_data.get("scaler")
        self.feature_columns = model_data.get("feature_columns")

        if self._model is None:
            raise MLModelError("ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        self.is_trained = True
        self.current_model_path = model_path
        self.current_model_metadata = model_data.get("metadata", {})
        return True

    def _cleanup_temporary_files(self, level: CleanupLevel):
        pass

    def _cleanup_cache(self, level: CleanupLevel):
        pass

    def _cleanup_models(self, level: CleanupLevel):
        try:
            if self.feature_service is not None:
                if hasattr(self.feature_service, "cleanup_resources"):
                    self.feature_service.cleanup_resources()

            self._model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False
            self.current_model_path = None
            self.current_model_metadata = None
        except Exception as e:
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}")
            self._model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False
            self.current_model_path = None
            self.current_model_metadata = None

    @safe_ml_operation(
        default_return={
            "fold": 0,
            "error": "ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            "accuracy": 0.0,
        },
        context="ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’",
    )
    def _train_fold_with_error_handling(
        self,
        fold: int,
        X_train_scaled: pd.DataFrame,
        X_test_scaled: pd.DataFrame,
        y_train_cv: pd.Series,
        y_test_cv: pd.Series,
        X_train_cv: pd.DataFrame,
        X_test_cv: pd.DataFrame,
        training_params: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ããƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’"""
        fold_result = self._train_model_impl(
            X_train_scaled,
            X_test_scaled,
            y_train_cv,
            y_test_cv,
            **training_params,
        )

        fold_result.update(
            {
                "fold": fold,
                "train_samples": len(X_train_cv),
                "test_samples": len(X_test_cv),
                "train_period": f"{X_train_cv.index[0]} ï½ {X_train_cv.index[-1]}",
                "test_period": f"{X_test_cv.index[0]} ï½ {X_test_cv.index[-1]}",
            }
        )
        return fold_result