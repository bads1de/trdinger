"""
ML ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã‚µãƒ¼ãƒ“ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆç‰¹ã«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼‰ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®
é«˜ãƒ¬ãƒ™ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚Optuna ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã—ã€ç›®çš„é–¢æ•°ã®å®šç¾©ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æ¢ç´¢ã€
å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã®å®Ÿè¡Œã‚’ç®¡ç†ã—ã¾ã™ã€‚

ä¸»ãªã‚¯ãƒ©ã‚¹:
    - OptimizationSettings: æœ€é©åŒ–ã®å®Ÿè¡Œè¨­å®šï¼ˆè©¦è¡Œå›æ•°ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ãªã©ï¼‰ã‚’å®šç¾©ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚
    - OptimizationService: æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’çµ±æ‹¬ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã€‚ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã¨é€£æºã—ã¦æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ã¾ã™ã€‚
"""

import logging
from typing import Any, Callable, Dict, Optional

import pandas as pd

from app.services.ml.ensemble.ensemble_trainer import EnsembleTrainer
from app.utils.error_handler import safe_operation

from .optuna_optimizer import OptunaOptimizer, ParameterSpace

logger = logging.getLogger(__name__)


class OptimizationSettings:
    """æœ€é©åŒ–è¨­å®šã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        enabled: bool = False,
        n_calls: int = 50,
        parameter_space: Optional[Dict[str, Dict[str, Any]]] = None,
    ):
        self.enabled = enabled
        self.n_calls = n_calls
        self.parameter_space = parameter_space or {}


class OptimizationService:
    """
    ML ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’çµ±æ‹¬ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹

    `OptunaOptimizer` ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹æˆã™ã‚‹
    å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•æ¢ç´¢ã—ã¾ã™ã€‚
    ç›®çš„é–¢æ•°ï¼ˆObjective Functionï¼‰ã®ç”Ÿæˆã€æ¢ç´¢ç©ºé–“ã®è¨­å®šã€
    CVï¼ˆäº¤å·®æ¤œè¨¼ï¼‰å›æ•°ã®èª¿æ•´ç­‰ã‚’è¡Œã„ã€æŒ‡å®šã•ã‚ŒãŸè©¦è¡Œå›æ•°å†…ã§
    ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ï¼ˆä¸»ã«ãƒã‚¯ãƒ­ F1 ã‚¹ã‚³ã‚¢ï¼‰ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚
    """

    def __init__(self):
        self.optimizer = OptunaOptimizer()

    @safe_operation(context="ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–", is_api_call=False)
    def optimize_parameters(
        self,
        trainer: Any,
        training_data: pd.DataFrame,
        optimization_settings: OptimizationSettings,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        model_name: Optional[str] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè¡Œ

        Optuna ã‚’ç”¨ã„ã¦ã€æŒ‡å®šã•ã‚ŒãŸè©¦è¡Œå›æ•°ï¼ˆn_callsï¼‰ã®ä¸­ã§
        æœ€ã‚‚é«˜ã„ F1 ã‚¹ã‚³ã‚¢ï¼ˆãƒã‚¯ãƒ­å¹³å‡ï¼‰ã‚’å‡ºã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã‚’æ¢ç´¢ã—ã¾ã™ã€‚
        å†…éƒ¨ã§ä¸€æ™‚çš„ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ç”Ÿæˆã—ã€è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚

        Args:
            trainer: ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            training_data: å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿
            optimization_settings: æœ€é©åŒ–ã®è¨­å®šï¼ˆæœ‰åŠ¹åŒ–ã€è©¦è¡Œå›æ•°ã€æ¢ç´¢ç©ºé–“ï¼‰
            funding_rate_data: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã® FR ãƒ‡ãƒ¼ã‚¿
            open_interest_data: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã® OI ãƒ‡ãƒ¼ã‚¿
            model_name: ä¿å­˜æ™‚ã®ãƒ¢ãƒ‡ãƒ«åï¼ˆæœ€é©åŒ–ä¸­ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã€è©•ä¾¡æ™‚é–“ç­‰ã‚’å«ã‚€çµæœè¾æ›¸
        """
        try:
            logger.info("ğŸš€ æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹")

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æº–å‚™
            parameter_space = self._prepare_parameter_space(
                trainer, optimization_settings
            )

            # ç›®çš„é–¢æ•°ã‚’ä½œæˆ
            objective_function = self._create_objective_function(
                trainer=trainer,
                training_data=training_data,
                optimization_settings=optimization_settings,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                **training_params,
            )

            # æœ€é©åŒ–ã‚’å®Ÿè¡Œ
            result = self.optimizer.optimize(
                objective_function=objective_function,
                parameter_space=parameter_space,
                n_calls=optimization_settings.n_calls,
            )

            return {
                "method": "optuna",
                "best_params": result.best_params,
                "best_score": result.best_score,
                "total_evaluations": result.total_evaluations,
                "optimization_time": result.optimization_time,
            }

        finally:
            # ç¢ºå®Ÿã«ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
            self.optimizer.cleanup()

    def optimize_full_pipeline(
        self,
        feature_superset: pd.DataFrame,
        labels: pd.Series,
        ohlcv_data: pd.DataFrame,
        n_trials: int = 50,
        test_ratio: float = 0.2,
        frac_diff_d_values: Optional[list] = None,
        fixed_label_params: Optional[Dict[str, Any]] = None,  # Added
    ) -> Dict[str, Any]:
        """
        ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° + ç‰¹å¾´é‡é¸æŠ + ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®åŒæ™‚æœ€é©åŒ–ï¼ˆCASHï¼‰

        ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒƒãƒˆã‹ã‚‰ç‰¹å®šã®då€¤ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã€ç‰¹å¾´é‡é¸æŠã¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã—ã¦è©•ä¾¡ã™ã‚‹ã“ã¨ã§ã€æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã‚’æ¢ç´¢ã—ã¾ã™ã€‚

        Args:
            feature_superset: create_feature_superset ã§ç”Ÿæˆã—ãŸå…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
            labels: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«
            n_trials: Optunaè©¦è¡Œå›æ•°
            test_ratio: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆæœ€çµ‚è©•ä¾¡ç”¨ï¼‰
            frac_diff_d_values: æ¢ç´¢ã™ã‚‹åˆ†æ•°æ¬¡å·®åˆ†ã®då€¤ãƒªã‚¹ãƒˆ
            fixed_label_params: ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å›ºå®šã™ã‚‹å ´åˆã®è¾æ›¸ (ä¾‹: {"tbm_horizon": 24})

        Returns:
            ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã€æ¯”è¼ƒçµæœãªã©ã‚’å«ã‚€è¾æ›¸
        """
        from app.services.ml.feature_engineering.feature_engineering_service import (
            FeatureEngineeringService,
        )
        from app.services.ml.feature_selection.feature_selector import FeatureSelector
        from app.services.ml.label_generation.presets import (
            triple_barrier_method_preset,
        )
        from lightgbm import LGBMClassifier
        from sklearn.metrics import balanced_accuracy_score as sklearn_metric

        if frac_diff_d_values is None:
            frac_diff_d_values = [0.3, 0.4, 0.5, 0.6]

        logger.info(f"ğŸš€ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒæ™‚æœ€é©åŒ–ã‚’é–‹å§‹: è©¦è¡Œå›æ•°={n_trials}")

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²åŸºæº–æ—¥ã‚’æ±ºå®š (feature_supersetã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨)
        n_samples = len(feature_superset)
        split_idx = int(n_samples * (1 - test_ratio))
        split_date = feature_superset.index[split_idx]

        logger.info(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²åŸºæº–æ—¥: {split_date}")

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å®šç¾©
        parameter_space = self._get_pipeline_parameter_space(
            frac_diff_d_values, fixed_label_params
        )

        # ç›®çš„é–¢æ•°ã‚’ä½œæˆ
        def objective_function(params: Dict[str, Any]) -> float:
            try:
                # 0. ãƒ©ãƒ™ãƒ«ã®å‹•çš„ç”Ÿæˆ (Triple Barrier Method)
                # å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆã—ã€ãªã‘ã‚Œã°Optunaã®ææ¡ˆå€¤(params)ã‚’ä½¿ç”¨
                label_params = params.copy()
                if fixed_label_params:
                    label_params.update(fixed_label_params)

                df_for_label = (
                    ohlcv_data if ohlcv_data is not None else feature_superset
                )

                current_labels = triple_barrier_method_preset(
                    df=df_for_label,
                    timeframe="1h",
                    horizon_n=label_params.get("tbm_horizon", 24),
                    pt=label_params.get("tbm_pt", 1.0),
                    sl=label_params.get("tbm_sl", 1.0),
                    min_ret=0.001,
                    price_column="close",
                    use_atr=True,
                )

                if current_labels.empty:
                    return 0.0

                # ... (ä»¥ä¸‹åŒæ§˜)

                # 1. ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ
                common_idx = feature_superset.index.intersection(current_labels.index)
                if len(common_idx) < 100:
                    return 0.0

                X_aligned = feature_superset.loc[common_idx]
                y_aligned = current_labels.loc[common_idx]

                # 2. TrainVal / Test åˆ†å‰² (split_dateåŸºæº–)
                mask_trainval = X_aligned.index < split_date
                X_trainval_curr = X_aligned[mask_trainval]
                y_trainval_curr = y_aligned[mask_trainval]

                if len(X_trainval_curr) < 50:
                    return 0.0

                # 3. FracDiff då€¤ã§ãƒ•ã‚£ãƒ«ã‚¿
                d_value = params["frac_diff_d"]
                X_filtered = FeatureEngineeringService.filter_superset_for_d(
                    X_trainval_curr, d_value
                )

                # é™¤å¤–ã‚«ãƒ©ãƒ 
                exclude_cols = ["open", "high", "low", "close", "volume"]
                feature_cols = [c for c in X_filtered.columns if c not in exclude_cols]
                X_features = X_filtered[feature_cols]

                # 4. å†…éƒ¨CVç”¨åˆ†å‰² (æ™‚ç³»åˆ—ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆ 20%)
                val_split_idx = int(len(X_features) * 0.8)

                X_train = X_features.iloc[:val_split_idx]
                y_train = y_trainval_curr.iloc[:val_split_idx]
                X_val = X_features.iloc[val_split_idx:]
                y_val = y_trainval_curr.iloc[val_split_idx:]

                # 5. ç‰¹å¾´é‡é¸æŠ
                selector = FeatureSelector(
                    method=params["selection_method"],
                    correlation_threshold=params["correlation_threshold"],
                    min_features=params["min_features"],
                    cv_folds=3,
                    cv_strategy="timeseries",
                    n_jobs=-1,
                )

                X_train_selected = selector.fit_transform(X_train, y_train)
                X_val_selected = selector.transform(X_val)

                # 6. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                model = LGBMClassifier(
                    learning_rate=params["learning_rate"],
                    num_leaves=params["num_leaves"],
                    n_estimators=100,
                    random_state=42,
                    verbosity=-1,
                    force_col_wise=True,
                )

                model.fit(X_train_selected, y_train)
                y_pred = model.predict(X_val_selected)

                # Balanced Accuracy
                score = sklearn_metric(y_val, y_pred)
                return score

            except Exception as e:
                logger.warning(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                return 0.0

        # æœ€é©åŒ–ã‚’å®Ÿè¡Œ
        result = self.optimizer.optimize(
            objective_function=objective_function,
            parameter_space=parameter_space,
            n_calls=n_trials,
        )

        best_params = result.best_params
        best_score = result.best_score

        # æœ€çµ‚è©•ä¾¡: ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡
        logger.info("ğŸ” ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡ä¸­...")

        # å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        final_params = best_params.copy()
        if fixed_label_params:
            final_params.update(fixed_label_params)

        df_for_label = ohlcv_data if ohlcv_data is not None else feature_superset

        labels_best = triple_barrier_method_preset(
            df=df_for_label,
            timeframe="1h",
            horizon_n=final_params.get("tbm_horizon", 24),
            pt=final_params.get("tbm_pt", 1.0),
            sl=final_params.get("tbm_sl", 1.0),
            min_ret=0.001,
            price_column="close",
            use_atr=True,
        )

        # ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ
        common_idx = feature_superset.index.intersection(labels_best.index)
        X_aligned = feature_superset.loc[common_idx]
        y_aligned = labels_best.loc[common_idx]

        # åˆ†å‰²
        mask_trainval = X_aligned.index < split_date
        X_trainval = X_aligned[mask_trainval]
        y_trainval = y_aligned[mask_trainval]
        X_test = X_aligned[~mask_trainval]
        y_test = y_aligned[~mask_trainval]

        # ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ•ãƒ«TrainValãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’
        d_value = best_params["frac_diff_d"]
        X_filtered = FeatureEngineeringService.filter_superset_for_d(
            X_trainval, d_value
        )
        X_test_filtered = FeatureEngineeringService.filter_superset_for_d(
            X_test, d_value
        )

        exclude_cols = ["open", "high", "low", "close", "volume"]
        feature_cols = [c for c in X_filtered.columns if c not in exclude_cols]
        X_features = X_filtered[feature_cols]
        X_test_features = X_test_filtered[feature_cols]

        selector = FeatureSelector(
            method=best_params["selection_method"],
            correlation_threshold=best_params["correlation_threshold"],
            min_features=best_params["min_features"],
            cv_folds=3,
            n_jobs=-1,
        )

        X_train_selected = selector.fit_transform(X_features, y_trainval)
        X_test_selected = selector.transform(X_test_features)

        model = LGBMClassifier(
            learning_rate=best_params["learning_rate"],
            num_leaves=best_params["num_leaves"],
            n_estimators=100,
            random_state=42,
            verbosity=-1,
            force_col_wise=True,
        )

        model.fit(X_train_selected, y_trainval)
        y_pred_test = model.predict(X_test_selected)
        test_score = sklearn_metric(y_test, y_pred_test)

        baseline_score = 0.0
        self.optimizer.cleanup()

        # å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚çµæœã«å«ã‚ã‚‹
        result_params = best_params.copy()
        if fixed_label_params:
            result_params.update(fixed_label_params)

        return {
            "best_params": result_params,
            "best_score": best_score,
            "test_score": test_score,
            "baseline_score": baseline_score,
            "improvement": test_score - baseline_score,
            "total_evaluations": result.total_evaluations,
            "optimization_time": result.optimization_time,
            "n_selected_features": X_train_selected.shape[1],
        }

    def optimize_meta_model_with_oof(
        self,
        primary_pipeline: Any,
        X_superset: pd.DataFrame,
        y_true: pd.Series,
        n_trials: int = 30,
        cv_splits: int = 5,
    ) -> Dict[str, Any]:
        """
        OOFäºˆæ¸¬ã‚’ç”¨ã„ãŸãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–
        ...
        """
        import numpy as np
        from sklearn.model_selection import TimeSeriesSplit, cross_val_score
        from app.services.ml.feature_engineering.feature_engineering_service import (
            FeatureEngineeringService,
        )
        from app.services.ml.feature_selection.feature_selector import FeatureSelector
        from lightgbm import LGBMClassifier

        logger.info("ğŸš€ ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ– (OOF) ã‚’é–‹å§‹")

        # 1. ä¸€æ¬¡ãƒ¢ãƒ‡ãƒ«ã® OOF äºˆæ¸¬ã‚’ç”Ÿæˆ
        tscv = TimeSeriesSplit(n_splits=cv_splits)
        oof_probs = pd.Series(np.nan, index=y_true.index)

        logger.info(
            f"[*] Generating OOF predictions for primary model ({cv_splits} folds)..."
        )
        for train_idx, val_idx in tscv.split(X_superset, y_true):
            X_train, X_val = X_superset.iloc[train_idx], X_superset.iloc[val_idx]
            y_train = y_true.iloc[train_idx]

            primary_pipeline.fit(X_train, y_train)
            oof_probs.iloc[val_idx] = primary_pipeline.predict_proba(X_val)[:, 1]

        # äºˆæ¸¬ãŒç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹ (æœ€åˆã®Foldã¯nanã«ãªã‚‹)
        valid_mask = oof_probs.notna()
        X_meta_full = X_superset[valid_mask]
        y_true_meta = y_true[valid_mask]
        primary_probs = oof_probs[valid_mask]

        # ãƒ¡ã‚¿ãƒ©ãƒ™ãƒ«ã®ç”Ÿæˆ: ä¸€æ¬¡ãƒ¢ãƒ‡ãƒ«ãŒã€Œ1ã€ã¨äºˆæ¸¬ã—ã€ã‹ã¤æ­£è§£ã‚‚ã€Œ1ã€ãªã‚‰ 1, å¤–ã‚ŒãŸã‚‰ 0
        # ã“ã“ã§ã¯é–¾å€¤ 0.5 ã§ã€Œã‚¨ãƒ³ãƒˆãƒªãƒ¼åˆ¤æ–­ã€ã‚’ã—ãŸãƒã‚¤ãƒ³ãƒˆã®ã¿ã‚’æŠ½å‡º
        entry_mask = primary_probs >= 0.5
        if entry_mask.sum() < 100:
            return {"status": "skipped", "reason": "too few entries for meta-learning"}

        X_meta = X_meta_full[entry_mask]
        # ãƒ¡ã‚¿ãƒ©ãƒ™ãƒ«: 1 = æˆåŠŸ(TP), 0 = å¤±æ•—(FP/ãƒ€ãƒã‚·)
        y_meta = (y_true_meta[entry_mask] == 1).astype(int)

        logger.info(
            f"[*] Meta-dataset prepared: {len(y_meta)} samples (Win Rate: {y_meta.mean():.2%})"
        )

        # 2. ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–
        # ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ã¯ã€Œãƒã‚¤ã‚¯ãƒ­ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ç³»ã€ãªã©ã®ç‰¹æ¨©ç‰¹å¾´é‡ã‚’å„ªå…ˆçš„ã«é¸ã°ã›ã‚‹
        # ã¾ãŸã€ä¸€æ¬¡ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡è‡ªä½“ã‚‚å¼·åŠ›ãªç‰¹å¾´é‡ã«ãªã‚‹
        micro_keywords = [
            "LS_",
            "OI_",
            "VPIN",
            "Roll_",
            "Amihud_",
            "Kyles_",
            "Spread",
            "Volume_CV",
        ]
        meta_feature_cols = [
            c for c in X_meta.columns if any(k in c for k in micro_keywords)
        ]

        X_meta_specialized = X_meta[meta_feature_cols].copy()
        X_meta_specialized["primary_prob"] = primary_probs

        logger.info(
            f"[*] Specialized meta-features: {len(meta_feature_cols)} microstructure cols + primary_prob"
        )

        frac_diff_d_values = [0.3, 0.4, 0.5]
        parameter_space = self._get_pipeline_parameter_space(frac_diff_d_values)

        def objective_function(params: Dict[str, Any]) -> float:
            try:
                # ç‰¹å¾´é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (FracDiffé©ç”¨æ¸ˆã¿ã®ã‚«ãƒ©ãƒ ã‹ã‚‰é¸æŠ)
                d_value = params["frac_diff_d"]
                X_filt = FeatureEngineeringService.filter_superset_for_d(
                    X_meta_specialized, d_value
                )

                # ç‰¹å¾´é‡é¸æŠ
                selector = FeatureSelector(
                    method=params["selection_method"],
                    min_features=params["min_features"],
                    cv_strategy="timeseries",
                )
                X_sel = selector.fit_transform(X_filt, y_meta)

                # ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ« (LightGBM)
                model = LGBMClassifier(
                    learning_rate=params["learning_rate"],
                    num_leaves=params["num_leaves"],
                    n_estimators=50,  # é«˜é€ŸåŒ–ã®ãŸã‚å°‘ãªã‚
                    class_weight="balanced",
                    random_state=42,
                    verbosity=-1,
                )

                # CVè©•ä¾¡ (F1ã‚¹ã‚³ã‚¢ã‚’é‡è¦–)
                scores = cross_val_score(
                    model, X_sel, y_meta, cv=TimeSeriesSplit(n_splits=3), scoring="f1"
                )
                return scores.mean()
            except Exception:
                return 0.0

        result = self.optimizer.optimize(
            objective_function, parameter_space, n_calls=n_trials
        )

        return {
            "best_params": result.best_params,
            "best_f1": result.best_score,
            "n_samples": len(y_meta),
            "base_win_rate": y_meta.mean(),
            "n_meta_features": X_meta_specialized.shape[1],
        }

    def _get_pipeline_parameter_space(
        self,
        frac_diff_d_values: list,
        fixed_label_params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, ParameterSpace]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒæ™‚æœ€é©åŒ–ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å®šç¾©"""

        space = {
            # Feature Engineering
            "frac_diff_d": ParameterSpace(
                type="categorical", categories=frac_diff_d_values
            ),
            # Feature Selection
            "selection_method": ParameterSpace(
                type="categorical", categories=["staged", "rfecv", "mutual_info"]
            ),
            "correlation_threshold": ParameterSpace(type="real", low=0.85, high=0.99),
            "min_features": ParameterSpace(type="integer", low=5, high=30),
            # Model (LightGBM)
            "learning_rate": ParameterSpace(type="real", low=0.005, high=0.1),
            "num_leaves": ParameterSpace(type="integer", low=16, high=128),
        }

        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå›ºå®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ ï¼‰
        fixed_keys = fixed_label_params.keys() if fixed_label_params else []

        if "tbm_pt" not in fixed_keys:
            space["tbm_pt"] = ParameterSpace(type="real", low=0.5, high=3.0)
        if "tbm_sl" not in fixed_keys:
            space["tbm_sl"] = ParameterSpace(type="real", low=0.5, high=3.0)
        if "tbm_horizon" not in fixed_keys:
            space["tbm_horizon"] = ParameterSpace(type="integer", low=4, high=48)

        return space

    def _evaluate_baseline(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_test: pd.DataFrame,
        y_test: pd.Series,
    ) -> float:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã§ã®è©•ä¾¡"""
        from app.services.ml.feature_engineering.feature_engineering_service import (
            FeatureEngineeringService,
        )
        from app.services.ml.feature_selection.feature_selector import FeatureSelector
        from lightgbm import LGBMClassifier
        from sklearn.metrics import balanced_accuracy_score as sklearn_metric

        try:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ d=0.4 ã§ãƒ•ã‚£ãƒ«ã‚¿
            X_train_filtered = FeatureEngineeringService.filter_superset_for_d(
                X_train, 0.4
            )
            X_test_filtered = FeatureEngineeringService.filter_superset_for_d(
                X_test, 0.4
            )

            exclude_cols = ["open", "high", "low", "close", "volume"]
            feature_cols = [
                c for c in X_train_filtered.columns if c not in exclude_cols
            ]
            X_features = X_train_filtered[feature_cols]
            X_test_features = X_test_filtered[feature_cols]

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡é¸æŠ
            selector = FeatureSelector(
                method="staged",
                correlation_threshold=0.90,
                min_features=10,
                cv_folds=3,
                cv_strategy="timeseries",
                n_jobs=-1,
            )

            X_train_selected = selector.fit_transform(X_features, y_train)
            X_test_selected = selector.transform(X_test_features)

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
            model = LGBMClassifier(
                learning_rate=0.05,
                num_leaves=31,
                n_estimators=100,
                random_state=42,
                verbosity=-1,
                force_col_wise=True,
            )

            model.fit(X_train_selected, y_train)
            y_pred = model.predict(X_test_selected)

            return sklearn_metric(y_test, y_pred)

        except Exception as e:
            logger.warning(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _prepare_parameter_space(
        self, trainer: Any, optimization_settings: OptimizationSettings
    ) -> Dict[str, ParameterSpace]:
        """
        æ¢ç´¢å¯¾è±¡ã¨ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å®šç¾©

        è¨­å®šã§æ¢ç´¢ç©ºé–“ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã—ã€
        ãã†ã§ãªã„å ´åˆã¯ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã«åŸºã¥ã„ãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç©ºé–“ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            trainer: å¯¾è±¡ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
            optimization_settings: æœ€é©åŒ–è¨­å®š

        Returns:
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã‚’ã‚­ãƒ¼ã€æ¢ç´¢ç¯„å›²ï¼ˆParameterSpaceï¼‰ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
        """
        if optimization_settings.parameter_space:
            return self._convert_parameter_space_config(
                optimization_settings.parameter_space
            )

        # EnsembleTrainerã®å ´åˆï¼ˆå˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚‚å«ã‚€ï¼‰
        if hasattr(trainer, "ensemble_config"):
            c = trainer.ensemble_config
            return self.optimizer.get_ensemble_parameter_space(
                c.get("method", "stacking"), c.get("models", ["lightgbm", "xgboost"])
            )

        return self.optimizer.get_default_parameter_space()

    def _convert_parameter_space_config(
        self, parameter_space_config: Dict[str, Dict[str, Any]]
    ) -> Dict[str, ParameterSpace]:
        """è¨­å®šè¾æ›¸ã‚’ParameterSpaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        return {
            name: ParameterSpace(
                type=cfg["type"],
                low=int(cfg["low"]) if cfg["type"] == "integer" else cfg.get("low"),
                high=int(cfg["high"]) if cfg["type"] == "integer" else cfg.get("high"),
                categories=cfg.get("categories"),
            )
            for name, cfg in parameter_space_config.items()
        }

    def _create_objective_function(
        self,
        trainer: Any,
        training_data: pd.DataFrame,
        optimization_settings: OptimizationSettings,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        **base_training_params,
    ) -> Callable[[Dict[str, Any]], float]:
        """
        Optuna ã«æ¸¡ã™ç›®çš„é–¢æ•°ï¼ˆObjective Functionï¼‰ã‚’ä½œæˆ

        å„è©¦è¡Œã§æ¸¡ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡ã‚’è¡Œã„ã€
        æœ€å¤§åŒ–ã™ã¹ãã‚¹ã‚³ã‚¢ï¼ˆF1 ã‚¹ã‚³ã‚¢ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚

        Args:
            trainer: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
            training_data: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
            optimization_settings: æœ€é©åŒ–è¨­å®š
            **base_training_params: å›ºå®šã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸ã‚’å—ã‘å–ã‚Šã‚¹ã‚³ã‚¢ï¼ˆfloatï¼‰ã‚’è¿”ã™é–¢æ•°
        """
        evaluation_count = 0

        def objective_function(params: Dict[str, Any]) -> float:
            nonlocal evaluation_count
            evaluation_count += 1

            try:
                logger.info(
                    f"ğŸ” è©¦è¡Œ {evaluation_count}/{optimization_settings.n_calls}: {params}"
                )

                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸
                training_params = {**base_training_params, **params}

                # ä¸€æ™‚çš„ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆ
                temp_trainer = self._create_temp_trainer(trainer, params)

                # å­¦ç¿’å®Ÿè¡Œï¼ˆä¿å­˜ãªã—ï¼‰
                result = temp_trainer.train_model(
                    training_data=training_data,
                    funding_rate_data=funding_rate_data,
                    open_interest_data=open_interest_data,
                    save_model=False,
                    model_name=None,
                    **training_params,
                )

                # ã‚¹ã‚³ã‚¢å–å¾—
                f1_score = result.get("f1_score", 0.0)
                if "classification_report" in result:
                    f1_score = (
                        result["classification_report"]
                        .get("macro avg", {})
                        .get("f1-score", f1_score)
                    )

                return f1_score

            except Exception as e:
                logger.warning(f"ç›®çš„é–¢æ•°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                return 0.0

        return objective_function

    def _create_temp_trainer(
        self, original_trainer: Any, params: Dict[str, Any]
    ) -> Any:
        """ä¸€æ™‚çš„ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆï¼ˆå…¨ã¦EnsembleTrainerã§çµ±ä¸€ï¼‰"""
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒEnsembleTrainerã§ã‚ã‚‹ã“ã¨ã‚’å‰æ
        if hasattr(original_trainer, "ensemble_config"):
            temp_config = original_trainer.ensemble_config.copy()

            # æœ€é©åŒ–ç”¨ã«CV foldsã‚’æ¸›ã‚‰ã™ï¼ˆé€Ÿåº¦å‘ä¸Šï¼‰
            if "stacking_params" in temp_config:
                stacking_params = temp_config["stacking_params"].copy()
                stacking_params["cv_folds"] = 3
                temp_config["stacking_params"] = stacking_params

            return EnsembleTrainer(ensemble_config=temp_config)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§EnsembleTrainerä½œæˆ
            return EnsembleTrainer(ensemble_config={"method": "stacking"})
