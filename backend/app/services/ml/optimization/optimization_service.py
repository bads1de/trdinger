"""
ML ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã‚µãƒ¼ãƒ“ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆç‰¹ã«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼‰ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®
é«˜ãƒ¬ãƒ™ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚Optuna ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã—ã€ç›®çš„é–¢æ•°ã®å®šç¾©ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æ¢ç´¢ã€
å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã®å®Ÿè¡Œã‚’ç®¡ç†ã—ã¾ã™ã€‚

ä¸»ãªã‚¯ãƒ©ã‚¹:
    - OptimizationSettings: æœ€é©åŒ–ã®å®Ÿè¡Œè¨­å®šï¼ˆè©¦è¡Œå›æ•°ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ãªã©ï¼‰ã‚’å®šç¾©ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚
    - OptimizationService: æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’çµ±æ‹¬ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã€‚ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã¨é€£æºã—ã¦æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ã¾ã™ã€‚
"""

import logging
from typing import Any, Callable, Dict, Optional

import pandas as pd

from app.services.ml.ensemble.ensemble_trainer import EnsembleTrainer
from app.utils.error_handler import safe_operation

from .optuna_optimizer import OptunaOptimizer, ParameterSpace

logger = logging.getLogger(__name__)


class OptimizationSettings:
    """æœ€é©åŒ–è¨­å®šã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        enabled: bool = False,
        n_calls: int = 50,
        parameter_space: Optional[Dict[str, Dict[str, Any]]] = None,
    ):
        self.enabled = enabled
        self.n_calls = n_calls
        self.parameter_space = parameter_space or {}


class OptimizationService:
    """
    ML ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’çµ±æ‹¬ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹

    `OptunaOptimizer` ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹æˆã™ã‚‹
    å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•æ¢ç´¢ã—ã¾ã™ã€‚
    ç›®çš„é–¢æ•°ï¼ˆObjective Functionï¼‰ã®ç”Ÿæˆã€æ¢ç´¢ç©ºé–“ã®è¨­å®šã€
    CVï¼ˆäº¤å·®æ¤œè¨¼ï¼‰å›æ•°ã®èª¿æ•´ç­‰ã‚’è¡Œã„ã€æŒ‡å®šã•ã‚ŒãŸè©¦è¡Œå›æ•°å†…ã§
    ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ï¼ˆä¸»ã«ãƒã‚¯ãƒ­ F1 ã‚¹ã‚³ã‚¢ï¼‰ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚
    """

    def __init__(self):
        self.optimizer = OptunaOptimizer()

    @safe_operation(context="ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–", is_api_call=False)
    def optimize_parameters(
        self,
        trainer: Any,
        training_data: pd.DataFrame,
        optimization_settings: OptimizationSettings,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        model_name: Optional[str] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè¡Œ

        Optuna ã‚’ç”¨ã„ã¦ã€æŒ‡å®šã•ã‚ŒãŸè©¦è¡Œå›æ•°ï¼ˆn_callsï¼‰ã®ä¸­ã§
        æœ€ã‚‚é«˜ã„ F1 ã‚¹ã‚³ã‚¢ï¼ˆãƒã‚¯ãƒ­å¹³å‡ï¼‰ã‚’å‡ºã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã‚’æ¢ç´¢ã—ã¾ã™ã€‚
        å†…éƒ¨ã§ä¸€æ™‚çš„ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ç”Ÿæˆã—ã€è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚

        Args:
            trainer: ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            training_data: å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿
            optimization_settings: æœ€é©åŒ–ã®è¨­å®šï¼ˆæœ‰åŠ¹åŒ–ã€è©¦è¡Œå›æ•°ã€æ¢ç´¢ç©ºé–“ï¼‰
            funding_rate_data: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã® FR ãƒ‡ãƒ¼ã‚¿
            open_interest_data: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã® OI ãƒ‡ãƒ¼ã‚¿
            model_name: ä¿å­˜æ™‚ã®ãƒ¢ãƒ‡ãƒ«åï¼ˆæœ€é©åŒ–ä¸­ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã€è©•ä¾¡æ™‚é–“ç­‰ã‚’å«ã‚€çµæœè¾æ›¸
        """
        try:
            logger.info("ğŸš€ æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹")

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’æº–å‚™
            parameter_space = self._prepare_parameter_space(
                trainer, optimization_settings
            )

            # ç›®çš„é–¢æ•°ã‚’ä½œæˆ
            objective_function = self._create_objective_function(
                trainer=trainer,
                training_data=training_data,
                optimization_settings=optimization_settings,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                **training_params,
            )

            # æœ€é©åŒ–ã‚’å®Ÿè¡Œ
            result = self.optimizer.optimize(
                objective_function=objective_function,
                parameter_space=parameter_space,
                n_calls=optimization_settings.n_calls,
            )

            return {
                "method": "optuna",
                "best_params": result.best_params,
                "best_score": result.best_score,
                "total_evaluations": result.total_evaluations,
                "optimization_time": result.optimization_time,
            }

        finally:
            # ç¢ºå®Ÿã«ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
            self.optimizer.cleanup()

    def optimize_full_pipeline(
        self,
        feature_superset: pd.DataFrame,
        labels: pd.Series,
        ohlcv_data: pd.DataFrame,  # Added for dynamic label generation
        n_trials: int = 50,
        test_ratio: float = 0.2,
        frac_diff_d_values: Optional[list] = None,
    ) -> Dict[str, Any]:
        """
        ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° + ç‰¹å¾´é‡é¸æŠ + ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®åŒæ™‚æœ€é©åŒ–ï¼ˆCASHï¼‰

        ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒƒãƒˆã‹ã‚‰ç‰¹å®šã®då€¤ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã€ç‰¹å¾´é‡é¸æŠã¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã—ã¦è©•ä¾¡ã™ã‚‹ã“ã¨ã§ã€æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã‚’æ¢ç´¢ã—ã¾ã™ã€‚

        Args:
            feature_superset: create_feature_superset ã§ç”Ÿæˆã—ãŸå…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
            labels: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«
            n_trials: Optunaè©¦è¡Œå›æ•°
            test_ratio: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆæœ€çµ‚è©•ä¾¡ç”¨ï¼‰
            frac_diff_d_values: æ¢ç´¢ã™ã‚‹åˆ†æ•°æ¬¡å·®åˆ†ã®då€¤ãƒªã‚¹ãƒˆ

        Returns:
            ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã€æ¯”è¼ƒçµæœãªã©ã‚’å«ã‚€è¾æ›¸
        """
        from app.services.ml.feature_engineering.feature_engineering_service import (
            FeatureEngineeringService,
        )
        from app.services.ml.feature_selection.feature_selector import FeatureSelector
        from app.services.ml.label_generation.presets import triple_barrier_method_preset
        from lightgbm import LGBMClassifier
        from sklearn.metrics import balanced_accuracy_score as sklearn_metric

        if frac_diff_d_values is None:
            frac_diff_d_values = [0.3, 0.4, 0.5, 0.6]

        logger.info(f"ğŸš€ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒæ™‚æœ€é©åŒ–ã‚’é–‹å§‹: è©¦è¡Œå›æ•°={n_trials}")

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²åŸºæº–æ—¥ã‚’æ±ºå®š (feature_supersetã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨)
        n_samples = len(feature_superset)
        split_idx = int(n_samples * (1 - test_ratio))
        split_date = feature_superset.index[split_idx]

        logger.info(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²åŸºæº–æ—¥: {split_date}")

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å®šç¾©
        parameter_space = self._get_pipeline_parameter_space(frac_diff_d_values)

        # ç›®çš„é–¢æ•°ã‚’ä½œæˆ
        def objective_function(params: Dict[str, Any]) -> float:
            try:
                # 0. ãƒ©ãƒ™ãƒ«ã®å‹•çš„ç”Ÿæˆ (Triple Barrier Method)
                # ohlcv_data ãŒæ¸¡ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ feature_superset ã‹ã‚‰æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒï¼ˆè¿‘ä¼¼ï¼‰
                # ãŸã ã—æ­£ç¢ºãªHigh/LowãŒå¿…è¦ãªã®ã§ã€ohlcv_dataã¯å¿…é ˆæ¨å¥¨
                
                df_for_label = ohlcv_data if ohlcv_data is not None else feature_superset
                
                # feature_supersetã«high/lowãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯presetså´ã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚
                # ã“ã“ã§ã¯ohlcv_dataãŒã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã™ã‚‹ï¼ˆoptimize_full_pipelineã®å¼•æ•°ã§å¿…é ˆåŒ–ã—ãŸæ–¹ãŒè‰¯ã„ãŒï¼‰
                
                current_labels = triple_barrier_method_preset(
                    df=df_for_label,
                    timeframe="1h", # ä»®å›ºå®šã€‚feature_supersetã‹ã‚‰æ¨æ¸¬ã§ãã‚‹ã¨ãƒ™ã‚¹ãƒˆ
                    horizon_n=params["tbm_horizon"],
                    pt=params["tbm_pt"],
                    sl=params["tbm_sl"],
                    min_ret=0.001,
                    price_column="close",
                    use_atr=True, # ATRãƒ™ãƒ¼ã‚¹ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨
                )
                
                if current_labels.empty:
                    return 0.0
                
                # 1. ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ
                common_idx = feature_superset.index.intersection(current_labels.index)
                if len(common_idx) < 100:
                    return 0.0
                    
                X_aligned = feature_superset.loc[common_idx]
                y_aligned = current_labels.loc[common_idx]

                # 2. TrainVal / Test åˆ†å‰² (split_dateåŸºæº–)
                # objective_functionå†…ã§ã¯ TrainVal ã®ã¿ã‚’ã•ã‚‰ã« Train/Val ã«åˆ†å‰²ã—ã¦è©•ä¾¡
                mask_trainval = X_aligned.index < split_date
                X_trainval_curr = X_aligned[mask_trainval]
                y_trainval_curr = y_aligned[mask_trainval]
                
                if len(X_trainval_curr) < 50:
                    return 0.0

                # 3. FracDiff då€¤ã§ãƒ•ã‚£ãƒ«ã‚¿
                d_value = params["frac_diff_d"]
                X_filtered = FeatureEngineeringService.filter_superset_for_d(
                    X_trainval_curr, d_value
                )

                # é™¤å¤–ã‚«ãƒ©ãƒ 
                exclude_cols = ["open", "high", "low", "close", "volume"]
                feature_cols = [c for c in X_filtered.columns if c not in exclude_cols]
                X_features = X_filtered[feature_cols]

                # 4. å†…éƒ¨CVç”¨åˆ†å‰² (æ™‚ç³»åˆ—ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆ 20%)
                val_split_idx = int(len(X_features) * 0.8)
                
                X_train = X_features.iloc[:val_split_idx]
                y_train = y_trainval_curr.iloc[:val_split_idx]
                X_val = X_features.iloc[val_split_idx:]
                y_val = y_trainval_curr.iloc[val_split_idx:]

                # 5. ç‰¹å¾´é‡é¸æŠ
                selector = FeatureSelector(
                    method=params["selection_method"],
                    correlation_threshold=params["correlation_threshold"],
                    min_features=params["min_features"],
                    cv_folds=3,
                    cv_strategy="timeseries",
                    n_jobs=-1,
                )

                X_train_selected = selector.fit_transform(X_train, y_train)
                X_val_selected = selector.transform(X_val)

                # 6. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                model = LGBMClassifier(
                    learning_rate=params["learning_rate"],
                    num_leaves=params["num_leaves"],
                    n_estimators=100,
                    random_state=42,
                    verbosity=-1,
                    force_col_wise=True,
                )

                model.fit(X_train_selected, y_train)
                y_pred = model.predict(X_val_selected)

                # Balanced Accuracy
                score = sklearn_metric(y_val, y_pred)
                return score

            except Exception as e:
                logger.warning(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                return 0.0

        # æœ€é©åŒ–ã‚’å®Ÿè¡Œ
        result = self.optimizer.optimize(
            objective_function=objective_function,
            parameter_space=parameter_space,
            n_calls=n_trials,
        )

        best_params = result.best_params
        best_score = result.best_score

        # æœ€çµ‚è©•ä¾¡: ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡
        logger.info("ğŸ” ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡ä¸­...")
        
        # ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ©ãƒ™ãƒ«å†ç”Ÿæˆ
        df_for_label = ohlcv_data if ohlcv_data is not None else feature_superset
        
        labels_best = triple_barrier_method_preset(
            df=df_for_label,
            timeframe="1h",
            horizon_n=best_params["tbm_horizon"],
            pt=best_params["tbm_pt"],
            sl=best_params["tbm_sl"],
            min_ret=0.001,
            price_column="close",
            use_atr=True,
        )
        
        # ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ
        common_idx = feature_superset.index.intersection(labels_best.index)
        X_aligned = feature_superset.loc[common_idx]
        y_aligned = labels_best.loc[common_idx]
        
        # åˆ†å‰²
        mask_trainval = X_aligned.index < split_date
        X_trainval = X_aligned[mask_trainval]
        y_trainval = y_aligned[mask_trainval]
        X_test = X_aligned[~mask_trainval]
        y_test = y_aligned[~mask_trainval]

        # ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ•ãƒ«TrainValãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’
        d_value = best_params["frac_diff_d"]
        X_filtered = FeatureEngineeringService.filter_superset_for_d(
            X_trainval, d_value
        )
        X_test_filtered = FeatureEngineeringService.filter_superset_for_d(
            X_test, d_value
        )

        exclude_cols = ["open", "high", "low", "close", "volume"]
        feature_cols = [c for c in X_filtered.columns if c not in exclude_cols]
        X_features = X_filtered[feature_cols]
        X_test_features = X_test_filtered[feature_cols]

        selector = FeatureSelector(
            method=best_params["selection_method"],
            correlation_threshold=best_params["correlation_threshold"],
            min_features=best_params["min_features"],
            cv_folds=3,
            n_jobs=-1,
        )

        X_train_selected = selector.fit_transform(X_features, y_trainval)
        X_test_selected = selector.transform(X_test_features)

        model = LGBMClassifier(
            learning_rate=best_params["learning_rate"],
            num_leaves=best_params["num_leaves"],
            n_estimators=100,
            random_state=42,
            verbosity=-1,
            force_col_wise=True,
        )

        model.fit(X_train_selected, y_trainval)
        y_pred_test = model.predict(X_test_selected)
        test_score = sklearn_metric(y_test, y_pred_test)

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        # æ³¨: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚‚ãƒ©ãƒ™ãƒ«ç”ŸæˆãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯å›ºå®šï¼ˆtrend_scanning_1hç›¸å½“ï¼‰ã§æ¯”è¼ƒã™ã‚‹ã®ãŒå¦¥å½“
        # ã—ã‹ã—å®Ÿè£…ãŒè¤‡é›‘ã«ãªã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã€Œãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®TrainValã‚¹ã‚³ã‚¢ã€ã¨æ¯”è¼ƒã™ã‚‹å½¢ã«ã™ã‚‹ã‹ã€
        # ã¾ãŸã¯å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†è¨ˆç®—ã™ã‚‹ã€‚
        # ç°¡æ˜“çš„ã«0.0ã‚’è¿”ã™ï¼ˆå‹•çš„ãƒ©ãƒ™ãƒ«ã®å ´åˆã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®ç›´æ¥æ¯”è¼ƒã¯é›£ã—ã„ï¼‰
        baseline_score = 0.0 

        self.optimizer.cleanup()

        return {
            "best_params": best_params,
            "best_score": best_score,
            "test_score": test_score,
            "baseline_score": baseline_score,
            "improvement": test_score - baseline_score,
            "total_evaluations": result.total_evaluations,
            "optimization_time": result.optimization_time,
            "n_selected_features": X_train_selected.shape[1],
        }

    def _get_pipeline_parameter_space(
        self, frac_diff_d_values: list
    ) -> Dict[str, ParameterSpace]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒæ™‚æœ€é©åŒ–ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å®šç¾©"""
        return {
            # Feature Engineering
            "frac_diff_d": ParameterSpace(
                type="categorical", categories=frac_diff_d_values
            ),
            # Feature Selection
            "selection_method": ParameterSpace(
                type="categorical", categories=["staged", "rfecv", "mutual_info"]
            ),
            "correlation_threshold": ParameterSpace(type="real", low=0.85, high=0.99),
            "min_features": ParameterSpace(type="integer", low=5, high=30),
            # Model (LightGBM)
            "learning_rate": ParameterSpace(type="real", low=0.005, high=0.1),
            "num_leaves": ParameterSpace(type="integer", low=16, high=128),
            # Label Generation (Triple Barrier)
            "tbm_pt": ParameterSpace(type="real", low=0.5, high=3.0),
            "tbm_sl": ParameterSpace(type="real", low=0.5, high=3.0),
            "tbm_horizon": ParameterSpace(type="integer", low=4, high=48),
        }

    def _evaluate_baseline(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_test: pd.DataFrame,
        y_test: pd.Series,
    ) -> float:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã§ã®è©•ä¾¡"""
        from app.services.ml.feature_engineering.feature_engineering_service import (
            FeatureEngineeringService,
        )
        from app.services.ml.feature_selection.feature_selector import FeatureSelector
        from lightgbm import LGBMClassifier
        from sklearn.metrics import balanced_accuracy_score as sklearn_metric

        try:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ d=0.4 ã§ãƒ•ã‚£ãƒ«ã‚¿
            X_train_filtered = FeatureEngineeringService.filter_superset_for_d(
                X_train, 0.4
            )
            X_test_filtered = FeatureEngineeringService.filter_superset_for_d(
                X_test, 0.4
            )

            exclude_cols = ["open", "high", "low", "close", "volume"]
            feature_cols = [
                c for c in X_train_filtered.columns if c not in exclude_cols
            ]
            X_features = X_train_filtered[feature_cols]
            X_test_features = X_test_filtered[feature_cols]

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡é¸æŠ
            selector = FeatureSelector(
                method="staged",
                correlation_threshold=0.90,
                min_features=10,
                cv_folds=3,
                cv_strategy="timeseries",
                n_jobs=-1,
            )

            X_train_selected = selector.fit_transform(X_features, y_train)
            X_test_selected = selector.transform(X_test_features)

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
            model = LGBMClassifier(
                learning_rate=0.05,
                num_leaves=31,
                n_estimators=100,
                random_state=42,
                verbosity=-1,
                force_col_wise=True,
            )

            model.fit(X_train_selected, y_train)
            y_pred = model.predict(X_test_selected)

            return sklearn_metric(y_test, y_pred)

        except Exception as e:
            logger.warning(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _prepare_parameter_space(
        self, trainer: Any, optimization_settings: OptimizationSettings
    ) -> Dict[str, ParameterSpace]:
        """
        æ¢ç´¢å¯¾è±¡ã¨ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’å®šç¾©

        è¨­å®šã§æ¢ç´¢ç©ºé–“ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã—ã€
        ãã†ã§ãªã„å ´åˆã¯ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã«åŸºã¥ã„ãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç©ºé–“ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            trainer: å¯¾è±¡ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
            optimization_settings: æœ€é©åŒ–è¨­å®š

        Returns:
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã‚’ã‚­ãƒ¼ã€æ¢ç´¢ç¯„å›²ï¼ˆParameterSpaceï¼‰ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
        """
        if optimization_settings.parameter_space:
            return self._convert_parameter_space_config(
                optimization_settings.parameter_space
            )

        # EnsembleTrainerã®å ´åˆï¼ˆå˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚‚å«ã‚€ï¼‰
        if hasattr(trainer, "ensemble_config"):
            c = trainer.ensemble_config
            return self.optimizer.get_ensemble_parameter_space(
                c.get("method", "stacking"), c.get("models", ["lightgbm", "xgboost"])
            )

        return self.optimizer.get_default_parameter_space()

    def _convert_parameter_space_config(
        self, parameter_space_config: Dict[str, Dict[str, Any]]
    ) -> Dict[str, ParameterSpace]:
        """è¨­å®šè¾æ›¸ã‚’ParameterSpaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        return {
            name: ParameterSpace(
                type=cfg["type"],
                low=int(cfg["low"]) if cfg["type"] == "integer" else cfg.get("low"),
                high=int(cfg["high"]) if cfg["type"] == "integer" else cfg.get("high"),
                categories=cfg.get("categories"),
            )
            for name, cfg in parameter_space_config.items()
        }

    def _create_objective_function(
        self,
        trainer: Any,
        training_data: pd.DataFrame,
        optimization_settings: OptimizationSettings,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        **base_training_params,
    ) -> Callable[[Dict[str, Any]], float]:
        """
        Optuna ã«æ¸¡ã™ç›®çš„é–¢æ•°ï¼ˆObjective Functionï¼‰ã‚’ä½œæˆ

        å„è©¦è¡Œã§æ¸¡ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡ã‚’è¡Œã„ã€
        æœ€å¤§åŒ–ã™ã¹ãã‚¹ã‚³ã‚¢ï¼ˆF1 ã‚¹ã‚³ã‚¢ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚

        Args:
            trainer: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
            training_data: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
            optimization_settings: æœ€é©åŒ–è¨­å®š
            **base_training_params: å›ºå®šã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸ã‚’å—ã‘å–ã‚Šã‚¹ã‚³ã‚¢ï¼ˆfloatï¼‰ã‚’è¿”ã™é–¢æ•°
        """
        evaluation_count = 0

        def objective_function(params: Dict[str, Any]) -> float:
            nonlocal evaluation_count
            evaluation_count += 1

            try:
                logger.info(
                    f"ğŸ” è©¦è¡Œ {evaluation_count}/{optimization_settings.n_calls}: {params}"
                )

                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸
                training_params = {**base_training_params, **params}

                # ä¸€æ™‚çš„ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆ
                temp_trainer = self._create_temp_trainer(trainer, params)

                # å­¦ç¿’å®Ÿè¡Œï¼ˆä¿å­˜ãªã—ï¼‰
                result = temp_trainer.train_model(
                    training_data=training_data,
                    funding_rate_data=funding_rate_data,
                    open_interest_data=open_interest_data,
                    save_model=False,
                    model_name=None,
                    **training_params,
                )

                # ã‚¹ã‚³ã‚¢å–å¾—
                f1_score = result.get("f1_score", 0.0)
                if "classification_report" in result:
                    f1_score = (
                        result["classification_report"]
                        .get("macro avg", {})
                        .get("f1-score", f1_score)
                    )

                return f1_score

            except Exception as e:
                logger.warning(f"ç›®çš„é–¢æ•°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                return 0.0

        return objective_function

    def _create_temp_trainer(
        self, original_trainer: Any, params: Dict[str, Any]
    ) -> Any:
        """ä¸€æ™‚çš„ãªãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆï¼ˆå…¨ã¦EnsembleTrainerã§çµ±ä¸€ï¼‰"""
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒEnsembleTrainerã§ã‚ã‚‹ã“ã¨ã‚’å‰æ
        if hasattr(original_trainer, "ensemble_config"):
            temp_config = original_trainer.ensemble_config.copy()

            # æœ€é©åŒ–ç”¨ã«CV foldsã‚’æ¸›ã‚‰ã™ï¼ˆé€Ÿåº¦å‘ä¸Šï¼‰
            if "stacking_params" in temp_config:
                stacking_params = temp_config["stacking_params"].copy()
                stacking_params["cv_folds"] = 3
                temp_config["stacking_params"] = stacking_params

            return EnsembleTrainer(ensemble_config=temp_config)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§EnsembleTrainerä½œæˆ
            return EnsembleTrainer(ensemble_config={"method": "stacking"})
