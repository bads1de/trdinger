"""
MLãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®å…±é€šé–¢æ•°ã‚’æä¾›ã—ã¾ã™ã€‚
ä¸€èˆ¬æŒ‡æ¨™ã¨ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ï¼ˆFakeout Detectionï¼‰ç”¨ã®æŒ‡æ¨™ã‚’ã‚«ãƒãƒ¼ã—ã¾ã™ã€‚
"""

import logging
from typing import Any, Dict, Optional

import numpy as np
import pandas as pd
from sklearn.metrics import precision_recall_curve

# çµ±ä¸€ã•ã‚ŒãŸMetricsCalculatorã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ..evaluation.metrics import metrics_collector

logger = logging.getLogger(__name__)


def evaluate_model_predictions(
    y_true: pd.Series,
    y_pred: np.ndarray,
    y_pred_proba: Optional[np.ndarray] = None,
) -> Dict[str, Any]:
    """
    äºˆæ¸¬çµæœã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®å…±é€šé–¢æ•°

    Args:
        y_true: å®Ÿéš›ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤
        y_pred: äºˆæ¸¬å€¤
        y_pred_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
    """
    y_true_array = y_true.values if hasattr(y_true, "values") else y_true
    return metrics_collector.calculate_comprehensive_metrics(
        y_true_array, y_pred, y_pred_proba
    )


def get_default_metrics() -> Dict[str, float]:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¾æ›¸ã‚’è¿”ã™ï¼ˆå…¨ã¦0.0åˆæœŸåŒ–ï¼‰"""
    keys = [
        "accuracy", "precision", "recall", "f1_score", "auc_score", "auc_roc", "auc_pr",
        "balanced_accuracy", "matthews_corrcoef", "cohen_kappa", "specificity", "sensitivity",
        "npv", "ppv", "log_loss", "brier_score", "loss", "val_accuracy", "val_loss", "training_time"
    ]
    return {k: 0.0 for k in keys}


# --- ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°è©•ä¾¡ ---


def evaluate_meta_labeling(
    y_true: pd.Series,
    y_pred: np.ndarray,
    y_pred_proba: Optional[np.ndarray] = None,
    threshold: float = 0.5,
) -> Dict[str, Any]:
    """ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ç”¨ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    y_t = y_true.values if hasattr(y_true, "values") else y_true
    res = metrics_collector.calculate_comprehensive_metrics(y_t, y_pred, y_pred_proba) or {}

    # å¿…é ˆã‚­ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä¿è¨¼
    for key in ["precision", "recall", "f1_score", "accuracy", "specificity", 
                "true_positives", "true_negatives", "false_positives", "false_negatives"]:
        if key not in res:
            res[key] = 0.0 if "positives" not in key and "negatives" not in key else 0

    p = res.get("precision", 0.0)
    res.update({
        "win_rate": p,
        "signal_adoption_rate": np.sum(y_pred) / len(y_pred) if len(y_pred) > 0 else 0.0,
        "expected_value": (p * 1.0) + ((1 - p) * -1.0),
        "total_samples": len(y_t),
        "positive_samples": int(np.sum(y_t)),
        "negative_samples": int(len(y_t) - np.sum(y_t))
    })
    
    for k, v in [("meta_f1", "f1_score"), ("meta_precision", "precision"), ("meta_recall", "recall")]:
        if v in res:
            res[k] = res[v]

    return res


def print_meta_labeling_report(
    y_true: pd.Series,
    y_pred: np.ndarray,
    y_pred_proba: Optional[np.ndarray] = None,
) -> None:
    """ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›"""
    metrics = evaluate_meta_labeling(y_true, y_pred, y_pred_proba)

    print("\n" + "=" * 60)
    print("ğŸ“Š Meta-Labeling Evaluation Report (Fakeout Detection)")
    print("=" * 60)

    print("\nğŸ¯ æœ€é‡è¦æŒ‡æ¨™ï¼ˆPrimary Metricsï¼‰:")
    print(f"  Precision (Win Rate):  {metrics['precision']:.4f}  â˜…æœ€é‡è¦â˜…")
    print(f"  F1-Score:              {metrics['f1_score']:.4f}")

    print("\nğŸ“ˆ è£œåŠ©æŒ‡æ¨™ï¼ˆSecondary Metricsï¼‰:")
    print(f"  Recall (Sensitivity):  {metrics['recall']:.4f}")
    print(f"  Specificity:           {metrics['specificity']:.4f}")
    print(f"  Accuracy:              {metrics['accuracy']:.4f}")

    print("\nğŸ’° å®Ÿç”¨æŒ‡æ¨™ï¼ˆPractical Metricsï¼‰:")
    print(f"  Signal Adoption Rate:  {metrics['signal_adoption_rate']:.2%}")
    print(f"  Expected Value:        {metrics['expected_value']:.4f}")

    print("\nğŸ”¢ Confusion Matrix:")
    print(f"  True Positives (TP):   {metrics['true_positives']}")
    print(f"  True Negatives (TN):   {metrics['true_negatives']}")
    print(f"  False Positives (FP):  {metrics['false_positives']}")
    print(f"  False Negatives (FN):  {metrics['false_negatives']}")

    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ:")
    print(f"  Total Samples:         {metrics['total_samples']}")
    print(f"  Positive Samples:      {metrics['positive_samples']}")
    print(f"  Negative Samples:      {metrics['negative_samples']}")

    if "roc_auc" in metrics:
        print("\nğŸ² ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™:")
        print(f"  ROC-AUC:               {metrics['roc_auc']:.4f}")
        print(f"  PR-AUC:                {metrics['pr_auc']:.4f}")

    print("\n" + "=" * 60)
    print("\nğŸ’¡ è§£é‡ˆã‚¬ã‚¤ãƒ‰:")
    if metrics["precision"] >= 0.60:
        print("  âœ… Precision >= 60%: å„ªç§€ãªãƒ¢ãƒ‡ãƒ«ã§ã™")
    elif metrics["precision"] >= 0.55:
        print("  âš ï¸  Precision 55-60%: å®Ÿç”¨çš„ã§ã™ãŒæ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
    else:
        print("  âŒ Precision < 55%: ãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„ãŒå¿…è¦ã§ã™")

    if metrics["signal_adoption_rate"] < 0.1:
        print("  âš ï¸  ã‚·ã‚°ãƒŠãƒ«æ¡æŠç‡ãŒä½ã„ï¼ˆ<10%ï¼‰: æ©Ÿä¼šæå¤±ã®å¯èƒ½æ€§")
    elif metrics["signal_adoption_rate"] > 0.5:
        print("  âš ï¸  ã‚·ã‚°ãƒŠãƒ«æ¡æŠç‡ãŒé«˜ã„ï¼ˆ>50%ï¼‰: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒç”˜ã„å¯èƒ½æ€§")
    print("=" * 60 + "\n")


def find_optimal_threshold(
    y_true: pd.Series,
    y_pred_proba: np.ndarray,
    metric: str = "precision",
    min_recall: float = 0.3,
) -> Dict[str, Any]:
    """æœ€é©ãªç¢ºç‡é–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹"""
    y_true_array = y_true.values if hasattr(y_true, "values") else y_true

    if len(y_pred_proba.shape) > 1:
        proba_positive = y_pred_proba[:, 1]
    else:
        proba_positive = y_pred_proba

    precisions, recalls, thresholds = precision_recall_curve(
        y_true_array, proba_positive
    )

    valid_indices = recalls[:-1] >= min_recall
    if not np.any(valid_indices):
        logger.warning(f"Recall >= {min_recall} ã‚’æº€ãŸã™é–¾å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {"optimal_threshold": 0.5, "precision": 0.0, "recall": 0.0, "f1": 0.0}

    valid_precisions = precisions[:-1][valid_indices]
    valid_recalls = recalls[:-1][valid_indices]
    valid_thresholds = thresholds[valid_indices]

    if metric == "precision":
        best_idx = np.argmax(valid_precisions)
    elif metric == "f1":
        f1_scores = 2 * (valid_precisions * valid_recalls) / (valid_precisions + valid_recalls + 1e-10)
        best_idx = np.argmax(f1_scores)
    else:
        raise ValueError(f"Unknown metric: {metric}")

    optimal_threshold = valid_thresholds[best_idx]
    metrics = evaluate_meta_labeling(y_true, (proba_positive >= optimal_threshold).astype(int), y_pred_proba, threshold=optimal_threshold)

    return {
        "optimal_threshold": float(optimal_threshold),
        "precision": metrics["precision"],
        "recall": metrics["recall"],
        "f1_score": metrics["f1_score"],
        "signal_adoption_rate": metrics["signal_adoption_rate"],
        "expected_value": metrics["expected_value"],
    }
