"""
ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ï¼ˆFakeout Detectionï¼‰ç”¨è©•ä¾¡ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

Meta-Labelingã§ã¯ Precisionï¼ˆé©åˆç‡ï¼‰ãŒæœ€é‡è¦æŒ‡æ¨™ã¨ãªã‚Šã¾ã™ã€‚
ã€Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ã—ãŸæ™‚ã«ã©ã‚Œã ã‘å‹ã¦ã‚‹ã‹ã€ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
"""

import logging
from typing import Any, Dict, Optional

import numpy as np
import pandas as pd
from sklearn.metrics import precision_recall_curve

logger = logging.getLogger(__name__)


def evaluate_meta_labeling(
    y_true: pd.Series,
    y_pred: np.ndarray,
    y_pred_proba: Optional[np.ndarray] = None,
    threshold: float = 0.5,
) -> Dict[str, Any]:
    """ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ç”¨ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    from ..evaluation.metrics import metrics_collector
    
    # çµ±ä¸€è©•ä¾¡å™¨ã§åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
    y_t = y_true.values if hasattr(y_true, "values") else y_true
    res = metrics_collector.calculate_comprehensive_metrics(y_t, y_pred, y_pred_proba)

    # ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°å›ºæœ‰ã®æŒ‡æ¨™ã‚’è¿½åŠ 
    p = res.get("precision", 0.0)
    res.update({
        "win_rate": p,
        "signal_adoption_rate": np.sum(y_pred) / len(y_pred) if len(y_pred) > 0 else 0.0,
        "expected_value": (p * 1.0) + ((1 - p) * -1.0),
        "total_samples": len(y_t),
        "positive_samples": int(np.sum(y_t)),
        "negative_samples": int(len(y_t) - np.sum(y_t))
    })
    
    # äº’æ›æ€§ã®ãŸã‚ã®ã‚­ãƒ¼è¿½åŠ 
    for k, v in [("meta_f1", "f1_score"), ("meta_precision", "precision"), ("meta_recall", "recall")]:
        if v in res:
            res[k] = res[v]

    return res


def print_meta_labeling_report(
    y_true: pd.Series,
    y_pred: np.ndarray,
    y_pred_proba: Optional[np.ndarray] = None,
) -> None:
    """
    ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›

    Args:
        y_true: å®Ÿéš›ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤
        y_pred: äºˆæ¸¬å€¤
        y_pred_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    metrics = evaluate_meta_labeling(y_true, y_pred, y_pred_proba)

    print("\n" + "=" * 60)
    print("ğŸ“Š Meta-Labeling Evaluation Report (Fakeout Detection)")
    print("=" * 60)

    print("\nğŸ¯ æœ€é‡è¦æŒ‡æ¨™ï¼ˆPrimary Metricsï¼‰:")
    print(f"  Precision (Win Rate):  {metrics['precision']:.4f}  â˜…æœ€é‡è¦â˜…")
    print(f"  F1-Score:              {metrics['f1_score']:.4f}")

    print("\nğŸ“ˆ è£œåŠ©æŒ‡æ¨™ï¼ˆSecondary Metricsï¼‰:")
    print(f"  Recall (Sensitivity):  {metrics['recall']:.4f}")
    print(f"  Specificity:           {metrics['specificity']:.4f}")
    print(f"  Accuracy:              {metrics['accuracy']:.4f}")

    print("\nğŸ’° å®Ÿç”¨æŒ‡æ¨™ï¼ˆPractical Metricsï¼‰:")
    print(f"  Signal Adoption Rate:  {metrics['signal_adoption_rate']:.2%}")
    print(f"  Expected Value:        {metrics['expected_value']:.4f}")

    print("\nğŸ”¢ Confusion Matrix:")
    print(f"  True Positives (TP):   {metrics['true_positives']}")
    print(f"  True Negatives (TN):   {metrics['true_negatives']}")
    print(f"  False Positives (FP):  {metrics['false_positives']}")
    print(f"  False Negatives (FN):  {metrics['false_negatives']}")

    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ:")
    print(f"  Total Samples:         {metrics['total_samples']}")
    print(f"  Positive Samples:      {metrics['positive_samples']}")
    print(f"  Negative Samples:      {metrics['negative_samples']}")

    if "roc_auc" in metrics:
        print("\nğŸ² ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™:")
        print(f"  ROC-AUC:               {metrics['roc_auc']:.4f}")
        print(f"  PR-AUC:                {metrics['pr_auc']:.4f}")

    print("\n" + "=" * 60)

    # è§£é‡ˆã‚¬ã‚¤ãƒ‰
    print("\nğŸ’¡ è§£é‡ˆã‚¬ã‚¤ãƒ‰:")
    if metrics["precision"] >= 0.60:
        print("  âœ… Precision >= 60%: å„ªç§€ãªãƒ¢ãƒ‡ãƒ«ã§ã™")
    elif metrics["precision"] >= 0.55:
        print("  âš ï¸  Precision 55-60%: å®Ÿç”¨çš„ã§ã™ãŒæ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
    else:
        print("  âŒ Precision < 55%: ãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„ãŒå¿…è¦ã§ã™")

    if metrics["signal_adoption_rate"] < 0.1:
        print("  âš ï¸  ã‚·ã‚°ãƒŠãƒ«æ¡æŠç‡ãŒä½ã„ï¼ˆ<10%ï¼‰: æ©Ÿä¼šæå¤±ã®å¯èƒ½æ€§")
    elif metrics["signal_adoption_rate"] > 0.5:
        print("  âš ï¸  ã‚·ã‚°ãƒŠãƒ«æ¡æŠç‡ãŒé«˜ã„ï¼ˆ>50%ï¼‰: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒç”˜ã„å¯èƒ½æ€§")

    print("=" * 60 + "\n")


def find_optimal_threshold(
    y_true: pd.Series,
    y_pred_proba: np.ndarray,
    metric: str = "precision",
    min_recall: float = 0.3,
) -> Dict[str, Any]:
    """
    æœ€é©ãªç¢ºç‡é–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹

    Meta-Labelingã§ã¯ Precision ã‚’æœ€å¤§åŒ–ã—ã¤ã¤ã€
    Recall ãŒä¸€å®šä»¥ä¸Šï¼ˆæ©Ÿä¼šæå¤±ã‚’é¿ã‘ã‚‹ï¼‰ã«ãªã‚‹é–¾å€¤ã‚’æ¢ã—ã¾ã™ã€‚

    Args:
        y_true: å®Ÿéš›ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤
        y_pred_proba: äºˆæ¸¬ç¢ºç‡
        metric: æœ€é©åŒ–ã™ã‚‹æŒ‡æ¨™ï¼ˆ"precision", "f1"ï¼‰
        min_recall: æœ€å°Recallåˆ¶ç´„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.3ï¼‰

    Returns:
        æœ€é©é–¾å€¤ã¨å„ç¨®æŒ‡æ¨™ã®è¾æ›¸
    """
    y_true_array = y_true.values if hasattr(y_true, "values") else y_true

    if len(y_pred_proba.shape) > 1:
        proba_positive = y_pred_proba[:, 1]
    else:
        proba_positive = y_pred_proba

    # Precision-Recallæ›²ç·šã‚’è¨ˆç®—
    precisions, recalls, thresholds = precision_recall_curve(
        y_true_array, proba_positive
    )

    # Recallåˆ¶ç´„ã‚’æº€ãŸã™é–¾å€¤ã®ã¿ã‚’è€ƒæ…®
    valid_indices = recalls[:-1] >= min_recall

    if not np.any(valid_indices):
        logger.warning(f"Recall >= {min_recall} ã‚’æº€ãŸã™é–¾å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {
            "optimal_threshold": 0.5,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
        }

    valid_precisions = precisions[:-1][valid_indices]
    valid_recalls = recalls[:-1][valid_indices]
    valid_thresholds = thresholds[valid_indices]

    if metric == "precision":
        # Precisionã‚’æœ€å¤§åŒ–
        best_idx = np.argmax(valid_precisions)
    elif metric == "f1":
        # F1-Scoreã‚’æœ€å¤§åŒ–
        f1_scores = (
            2
            * (valid_precisions * valid_recalls)
            / (valid_precisions + valid_recalls + 1e-10)
        )
        best_idx = np.argmax(f1_scores)
    else:
        raise ValueError(f"Unknown metric: {metric}")

    optimal_threshold = valid_thresholds[best_idx]

    # æœ€é©é–¾å€¤ã§ã®è©•ä¾¡
    y_pred_optimal = (proba_positive >= optimal_threshold).astype(int)
    metrics = evaluate_meta_labeling(
        y_true, y_pred_optimal, y_pred_proba, threshold=optimal_threshold
    )

    result = {
        "optimal_threshold": float(optimal_threshold),
        "precision": metrics["precision"],
        "recall": metrics["recall"],
        "f1_score": metrics["f1_score"],
        "signal_adoption_rate": metrics["signal_adoption_rate"],
        "expected_value": metrics["expected_value"],
    }

    logger.info(
        f"æœ€é©é–¾å€¤: {optimal_threshold:.3f} "
        f"(Precision={metrics['precision']:.3f}, "
        f"Recall={metrics['recall']:.3f})"
    )

    return result



