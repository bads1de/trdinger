"""
ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ï¼ˆFakeout Detectionï¼‰ç”¨è©•ä¾¡ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

Meta-Labelingã§ã¯ Precisionï¼ˆé©åˆç‡ï¼‰ãŒæœ€é‡è¦æŒ‡æ¨™ã¨ãªã‚Šã¾ã™ã€‚
ã€Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ã—ãŸæ™‚ã«ã©ã‚Œã ã‘å‹ã¦ã‚‹ã‹ã€ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
"""

import logging
from typing import Any, Dict, Optional

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    average_precision_score,
    confusion_matrix,
    f1_score,
    precision_recall_curve,
    precision_score,
    recall_score,
    roc_auc_score,
)

logger = logging.getLogger(__name__)


def evaluate_meta_labeling(
    y_true: pd.Series,
    y_pred: np.ndarray,
    y_pred_proba: Optional[np.ndarray] = None,
    threshold: float = 0.5,
) -> Dict[str, Any]:
    """
    ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ç”¨ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—

    Precisionï¼ˆé©åˆç‡ï¼‰ã‚’æœ€é‡è¦æŒ‡æ¨™ã¨ã—ã€
    ã€ŒMLãƒ¢ãƒ‡ãƒ«ãŒOKã¨åˆ¤å®šã—ãŸã‚·ã‚°ãƒŠãƒ«ã®å‹ç‡ã€ã‚’æ¸¬å®šã—ã¾ã™ã€‚

    Args:
        y_true: å®Ÿéš›ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ï¼ˆ0=å¤±æ•—, 1=æˆåŠŸï¼‰
        y_pred: äºˆæ¸¬å€¤ï¼ˆ0 or 1ï¼‰
        y_pred_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        threshold: ç¢ºç‡é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5ï¼‰

    Returns:
        è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
    """
    # numpyé…åˆ—ã«å¤‰æ›
    y_true_array = y_true.values if hasattr(y_true, "values") else y_true

    # ç¢ºç‡ã‹ã‚‰äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã‚’ç”Ÿæˆï¼ˆé–¾å€¤èª¿æ•´å¯èƒ½ï¼‰
    if y_pred_proba is not None and len(y_pred_proba.shape) > 1:
        # 2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®å ´åˆã€ã‚¯ãƒ©ã‚¹1ã®ç¢ºç‡ã‚’ä½¿ç”¨
        y_pred_from_proba = (y_pred_proba[:, 1] >= threshold).astype(int)
    else:
        y_pred_from_proba = y_pred

    # Confusion Matrix
    tn, fp, fn, tp = confusion_matrix(y_true_array, y_pred_from_proba).ravel()

    # Precisionï¼ˆé©åˆç‡ï¼‰- æœ€é‡è¦æŒ‡æ¨™
    # MLãŒOKã¨è¨€ã£ãŸæ™‚ã«å®Ÿéš›ã«æˆåŠŸã—ãŸå‰²åˆ
    precision = precision_score(y_true_array, y_pred_from_proba, zero_division=0.0)

    # Recallï¼ˆå†ç¾ç‡ï¼‰
    # å®Ÿéš›ã®æˆåŠŸã‚·ã‚°ãƒŠãƒ«ã‚’ã©ã‚Œã ã‘æ‹¾ãˆãŸã‹
    recall = recall_score(y_true_array, y_pred_from_proba, zero_division=0.0)

    # F1-Scoreï¼ˆç²¾åº¦ã¨å†ç¾ç‡ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
    f1 = f1_score(y_true_array, y_pred_from_proba, zero_division=0.0)

    # Accuracyï¼ˆå…¨ä½“ã®æ­£ç­”ç‡ï¼‰- ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ã§ã¯ã‚ã¾ã‚Šé‡è¦–ã—ãªã„
    accuracy = accuracy_score(y_true_array, y_pred_from_proba)

    # Specificityï¼ˆç‰¹ç•°åº¦ï¼‰
    # å¤±æ•—ã‚·ã‚°ãƒŠãƒ«ã‚’æ­£ã—ãè¦‹æŠœã‘ãŸå‰²åˆ
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # Positive Predictive Value (PPV) = Precision
    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0

    # Negative Predictive Value (NPV)
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0

    # Win Rateï¼ˆå‹ç‡ï¼‰= Precision ã¨åŒã˜ã ãŒæ˜ç¤ºçš„ã«
    win_rate = precision

    # ã‚·ã‚°ãƒŠãƒ«æ¡æŠç‡ï¼ˆä½•%ã®ã‚·ã‚°ãƒŠãƒ«ã‚’æ¡ç”¨ã—ãŸã‹ï¼‰
    signal_adoption_rate = np.sum(y_pred_from_proba) / len(y_pred_from_proba)

    # Expected Valueï¼ˆæœŸå¾…å€¤ï¼‰ã®ç°¡æ˜“è¨ˆç®—
    # å‹ã¡æ™‚ã®åˆ©ç›Šã‚’1ã€è² ã‘æ™‚ã®æå¤±ã‚’-1ã¨ä»®å®š
    expected_value = (precision * 1.0) + ((1 - precision) * -1.0)

    result = {
        # === ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°æœ€é‡è¦æŒ‡æ¨™ ===
        "precision": precision,  # æœ€é‡è¦: MLãŒOKã¨è¨€ã£ãŸæ™‚ã®å‹ç‡
        "win_rate": win_rate,  # Precision ã¨åŒã˜ã ãŒæ˜ç¤ºçš„
        "f1_score": f1,  # ç²¾åº¦ã¨å†ç¾ç‡ã®ãƒãƒ©ãƒ³ã‚¹
        # === è£œåŠ©æŒ‡æ¨™ ===
        "recall": recall,  # æˆåŠŸã‚·ã‚°ãƒŠãƒ«ã®æ¤œå‡ºç‡
        "accuracy": accuracy,  # å…¨ä½“ã®æ­£ç­”ç‡
        "specificity": specificity,  # å¤±æ•—ã‚·ã‚°ãƒŠãƒ«ã®æ¤œå‡ºç‡
        # === å®Ÿç”¨æŒ‡æ¨™ ===
        "signal_adoption_rate": signal_adoption_rate,  # ã‚·ã‚°ãƒŠãƒ«æ¡æŠç‡
        "expected_value": expected_value,  # æœŸå¾…å€¤ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        # === Confusion Matrix ===
        "true_positives": int(tp),
        "true_negatives": int(tn),
        "false_positives": int(fp),
        "false_negatives": int(fn),
        # === ãã®ä»– ===
        "ppv": ppv,
        "npv": npv,
        "total_samples": len(y_true_array),
        "positive_samples": int(np.sum(y_true_array)),
        "negative_samples": int(len(y_true_array) - np.sum(y_true_array)),
    }

    # ç¢ºç‡ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã€ROC-AUCã¨PR-AUCã‚’è¨ˆç®—
    if y_pred_proba is not None:
        try:
            if len(y_pred_proba.shape) > 1:
                proba_positive = y_pred_proba[:, 1]
            else:
                proba_positive = y_pred_proba

            # ROC-AUC
            roc_auc = roc_auc_score(y_true_array, proba_positive)
            result["roc_auc"] = roc_auc

            # PR-AUCï¼ˆPrecision-Recall AUCï¼‰- ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ã§é‡è¦
            pr_auc = average_precision_score(y_true_array, proba_positive)
            result["pr_auc"] = pr_auc

        except Exception as e:
            logger.warning(f"ROC/PR-AUCè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            result["roc_auc"] = 0.0
            result["pr_auc"] = 0.0

    return result


def print_meta_labeling_report(
    y_true: pd.Series,
    y_pred: np.ndarray,
    y_pred_proba: Optional[np.ndarray] = None,
) -> None:
    """
    ãƒ¡ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›

    Args:
        y_true: å®Ÿéš›ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤
        y_pred: äºˆæ¸¬å€¤
        y_pred_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    metrics = evaluate_meta_labeling(y_true, y_pred, y_pred_proba)

    print("\n" + "=" * 60)
    print("ğŸ“Š Meta-Labeling Evaluation Report (Fakeout Detection)")
    print("=" * 60)

    print("\nğŸ¯ æœ€é‡è¦æŒ‡æ¨™ï¼ˆPrimary Metricsï¼‰:")
    print(f"  Precision (Win Rate):  {metrics['precision']:.4f}  â˜…æœ€é‡è¦â˜…")
    print(f"  F1-Score:              {metrics['f1_score']:.4f}")

    print("\nğŸ“ˆ è£œåŠ©æŒ‡æ¨™ï¼ˆSecondary Metricsï¼‰:")
    print(f"  Recall (Sensitivity):  {metrics['recall']:.4f}")
    print(f"  Specificity:           {metrics['specificity']:.4f}")
    print(f"  Accuracy:              {metrics['accuracy']:.4f}")

    print("\nğŸ’° å®Ÿç”¨æŒ‡æ¨™ï¼ˆPractical Metricsï¼‰:")
    print(f"  Signal Adoption Rate:  {metrics['signal_adoption_rate']:.2%}")
    print(f"  Expected Value:        {metrics['expected_value']:.4f}")

    print("\nğŸ”¢ Confusion Matrix:")
    print(f"  True Positives (TP):   {metrics['true_positives']}")
    print(f"  True Negatives (TN):   {metrics['true_negatives']}")
    print(f"  False Positives (FP):  {metrics['false_positives']}")
    print(f"  False Negatives (FN):  {metrics['false_negatives']}")

    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ:")
    print(f"  Total Samples:         {metrics['total_samples']}")
    print(f"  Positive Samples:      {metrics['positive_samples']}")
    print(f"  Negative Samples:      {metrics['negative_samples']}")

    if "roc_auc" in metrics:
        print("\nğŸ² ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™:")
        print(f"  ROC-AUC:               {metrics['roc_auc']:.4f}")
        print(f"  PR-AUC:                {metrics['pr_auc']:.4f}")

    print("\n" + "=" * 60)

    # è§£é‡ˆã‚¬ã‚¤ãƒ‰
    print("\nğŸ’¡ è§£é‡ˆã‚¬ã‚¤ãƒ‰:")
    if metrics["precision"] >= 0.60:
        print("  âœ… Precision >= 60%: å„ªç§€ãªãƒ¢ãƒ‡ãƒ«ã§ã™")
    elif metrics["precision"] >= 0.55:
        print("  âš ï¸  Precision 55-60%: å®Ÿç”¨çš„ã§ã™ãŒæ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
    else:
        print("  âŒ Precision < 55%: ãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„ãŒå¿…è¦ã§ã™")

    if metrics["signal_adoption_rate"] < 0.1:
        print("  âš ï¸  ã‚·ã‚°ãƒŠãƒ«æ¡æŠç‡ãŒä½ã„ï¼ˆ<10%ï¼‰: æ©Ÿä¼šæå¤±ã®å¯èƒ½æ€§")
    elif metrics["signal_adoption_rate"] > 0.5:
        print("  âš ï¸  ã‚·ã‚°ãƒŠãƒ«æ¡æŠç‡ãŒé«˜ã„ï¼ˆ>50%ï¼‰: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒç”˜ã„å¯èƒ½æ€§")

    print("=" * 60 + "\n")


def find_optimal_threshold(
    y_true: pd.Series,
    y_pred_proba: np.ndarray,
    metric: str = "precision",
    min_recall: float = 0.3,
) -> Dict[str, Any]:
    """
    æœ€é©ãªç¢ºç‡é–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹

    Meta-Labelingã§ã¯ Precision ã‚’æœ€å¤§åŒ–ã—ã¤ã¤ã€
    Recall ãŒä¸€å®šä»¥ä¸Šï¼ˆæ©Ÿä¼šæå¤±ã‚’é¿ã‘ã‚‹ï¼‰ã«ãªã‚‹é–¾å€¤ã‚’æ¢ã—ã¾ã™ã€‚

    Args:
        y_true: å®Ÿéš›ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤
        y_pred_proba: äºˆæ¸¬ç¢ºç‡
        metric: æœ€é©åŒ–ã™ã‚‹æŒ‡æ¨™ï¼ˆ"precision", "f1"ï¼‰
        min_recall: æœ€å°Recallåˆ¶ç´„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.3ï¼‰

    Returns:
        æœ€é©é–¾å€¤ã¨å„ç¨®æŒ‡æ¨™ã®è¾æ›¸
    """
    y_true_array = y_true.values if hasattr(y_true, "values") else y_true

    if len(y_pred_proba.shape) > 1:
        proba_positive = y_pred_proba[:, 1]
    else:
        proba_positive = y_pred_proba

    # Precision-Recallæ›²ç·šã‚’è¨ˆç®—
    precisions, recalls, thresholds = precision_recall_curve(
        y_true_array, proba_positive
    )

    # Recallåˆ¶ç´„ã‚’æº€ãŸã™é–¾å€¤ã®ã¿ã‚’è€ƒæ…®
    valid_indices = recalls[:-1] >= min_recall

    if not np.any(valid_indices):
        logger.warning(f"Recall >= {min_recall} ã‚’æº€ãŸã™é–¾å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {
            "optimal_threshold": 0.5,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
        }

    valid_precisions = precisions[:-1][valid_indices]
    valid_recalls = recalls[:-1][valid_indices]
    valid_thresholds = thresholds[valid_indices]

    if metric == "precision":
        # Precisionã‚’æœ€å¤§åŒ–
        best_idx = np.argmax(valid_precisions)
    elif metric == "f1":
        # F1-Scoreã‚’æœ€å¤§åŒ–
        f1_scores = (
            2
            * (valid_precisions * valid_recalls)
            / (valid_precisions + valid_recalls + 1e-10)
        )
        best_idx = np.argmax(f1_scores)
    else:
        raise ValueError(f"Unknown metric: {metric}")

    optimal_threshold = valid_thresholds[best_idx]

    # æœ€é©é–¾å€¤ã§ã®è©•ä¾¡
    y_pred_optimal = (proba_positive >= optimal_threshold).astype(int)
    metrics = evaluate_meta_labeling(
        y_true, y_pred_optimal, y_pred_proba, threshold=optimal_threshold
    )

    result = {
        "optimal_threshold": float(optimal_threshold),
        "precision": metrics["precision"],
        "recall": metrics["recall"],
        "f1_score": metrics["f1_score"],
        "signal_adoption_rate": metrics["signal_adoption_rate"],
        "expected_value": metrics["expected_value"],
    }

    logger.info(
        f"æœ€é©é–¾å€¤: {optimal_threshold:.3f} "
        f"(Precision={metrics['precision']:.3f}, "
        f"Recall={metrics['recall']:.3f})"
    )

    return result
