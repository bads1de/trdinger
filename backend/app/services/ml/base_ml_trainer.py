"""
MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»å‰å‡¦ç†ãƒ»ä¿å­˜ã«é–¢ã‚ã‚‹å…±é€šãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã™ã‚‹æŠ½è±¡åŸºç›¤ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
å…·ä½“çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„æœ€é©åŒ–æ‰‹æ³•ã®è©³ç´°èª¬æ˜ã¯Docstringã«å«ã‚ã¾ã›ã‚“ã€‚
ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ãŒãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®å­¦ç¿’å‡¦ç†ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
"""

import logging
from abc import ABC
from typing import Any, Dict, Optional, Tuple, cast

import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.preprocessing import StandardScaler

from .ml_metadata import ModelMetadata
from ...utils.data_processing import data_processor as data_preprocessor
from ...utils.label_generation import LabelGenerator, ThresholdMethod
from .exceptions import (
    MLModelError,
    ModelError,
)
from ...utils.unified_error_handler import (
    ml_operation_context,
    safe_ml_operation,
)
from .config import ml_config
from .common.base_resource_manager import BaseResourceManager, CleanupLevel
from .feature_engineering.automl_features.automl_config import AutoMLConfig
from .feature_engineering.feature_engineering_service import FeatureEngineeringService
from .model_manager import model_manager

logger = logging.getLogger(__name__)


class BaseMLTrainer(BaseResourceManager, ABC):
    """
    MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

    å…±é€šã®å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã—ã€å…·ä½“çš„ãªå®Ÿè£…ã¯ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§è¡Œã„ã¾ã™ã€‚
    å˜ä¸€è²¬ä»»åŸå‰‡ã«å¾“ã„ã€å­¦ç¿’ã«é–¢ã™ã‚‹è²¬ä»»ã®ã¿ã‚’æŒã¡ã¾ã™ã€‚
    """

    def __init__(
        self,
        automl_config: Optional[Dict[str, Any]] = None,
        trainer_config: Optional[Dict[str, Any]] = None,
        trainer_type: Optional[str] = None,
        model_type: Optional[str] = None,
    ):
        """
        åˆæœŸåŒ–

        Args:
            automl_config: AutoMLè¨­å®šï¼ˆè¾æ›¸å½¢å¼ï¼‰
            trainer_config: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šï¼ˆå˜ä¸€ãƒ¢ãƒ‡ãƒ«/ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šï¼‰
            trainer_type: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆè„†å¼±æ€§ä¿®æ­£ï¼‰
            model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆè„†å¼±æ€§ä¿®æ­£ï¼‰
        """
        # BaseResourceManagerã®åˆæœŸåŒ–
        super().__init__()

        self.config = ml_config

        # AutoMLè¨­å®šã®å‡¦ç†
        if automl_config:
            # AutoMLConfig.from_dict ã«çµ±ä¸€
            automl_config_obj = AutoMLConfig.from_dict(automl_config)
            self.feature_service = FeatureEngineeringService(
                automl_config=automl_config_obj
            )
            self.use_automl = True
            logger.debug("ğŸ¤– AutoMLç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
        else:
            # å¾“æ¥ã®åŸºæœ¬ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨
            self.feature_service = FeatureEngineeringService()
            self.use_automl = False

        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šã®å‡¦ç†ï¼ˆè„†å¼±æ€§ä¿®æ­£ï¼‰
        self.trainer_config = trainer_config or {}

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®å„ªå…ˆé †ä½: ç›´æ¥æŒ‡å®š > trainer_config > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        self.trainer_type = trainer_type or self.trainer_config.get(
            "type", "single"
        )  # "single" or "ensemble"

        self.model_type = model_type or self.trainer_config.get(
            "model_type", "lightgbm"
        )
        self.ensemble_config = self.trainer_config.get("ensemble_config", {})

        self.scaler = StandardScaler()
        self.feature_columns = None
        self.is_trained = False
        self.model = None
        self.models = {}  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ã®è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ ¼ç´
        # å‘¼ã³å‡ºã—å…ƒãŒè¾æ›¸ã‚’æ¸¡ã™æƒ³å®šã®ãŸã‚ã€ãã®ã¾ã¾ä¿æŒï¼ˆç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹å†…ã§ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ï¼‰
        self.automl_config = automl_config
        self.last_training_results = None  # æœ€å¾Œã®å­¦ç¿’çµæœã‚’ä¿æŒ

    # é‡è¤‡ãƒ­ã‚¸ãƒƒã‚¯å‰Šé™¤:
    # _create_automl_config_from_dict ã¯ AutoMLConfig.from_dict ã«çµ±ä¸€ã—ãŸãŸã‚ä¸è¦

    @safe_ml_operation(default_return={}, context="MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    def train_model(
        self,
        training_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        save_model: bool = True,
        model_name: Optional[str] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_model: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸

        Raises:
            DataError: ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ãªå ´åˆ
            ModelError: å­¦ç¿’ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        with ml_operation_context("MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’"):
            # 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            self._validate_training_data(training_data)

            # 2. ç‰¹å¾´é‡ã‚’è¨ˆç®—
            features_df = self._calculate_features(
                training_data, funding_rate_data, open_interest_data
            )

            # 3. å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            X, y = self._prepare_training_data(features_df, **training_params)

            # 4. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            use_cross_validation = training_params.get("use_cross_validation", False)

            if use_cross_validation:
                # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
                cv_result = self._time_series_cross_validate(X, y, **training_params)

                # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
                logger.info("ğŸ¯ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ä¸­...")
                X_scaled = self._preprocess_data(X, X)[0]  # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

                # ãƒ€ãƒŸãƒ¼ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€å¾Œã®20%ï¼‰ã‚’ä½œæˆ
                test_size = training_params.get("test_size", 0.2)
                n_samples = len(X)
                train_size = int(n_samples * (1 - test_size))

                X_train_final = X_scaled.iloc[:train_size]
                X_test_final = X_scaled.iloc[train_size:]
                y_train_final = y.iloc[:train_size]
                y_test_final = y.iloc[train_size:]

                training_result = self._train_model_impl(
                    X_train_final,
                    X_test_final,
                    y_train_final,
                    y_test_final,
                    **training_params,
                )

                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¿½åŠ 
                training_result.update(cv_result)

            else:
                # é€šå¸¸ã®å˜ä¸€åˆ†å‰²å­¦ç¿’
                # 4. ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
                X_train, X_test, y_train, y_test = self._split_data(
                    X, y, **training_params
                )

                # 5. ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†
                X_train_scaled, X_test_scaled = self._preprocess_data(X_train, X_test)

                # 6. ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
                training_result = self._train_model_impl(
                    X_train_scaled, X_test_scaled, y_train, y_test, **training_params
                )

            # 7. å­¦ç¿’å®Œäº†ãƒ•ãƒ©ã‚°ã‚’è¨­å®šï¼ˆä¿å­˜å‰ã«è¨­å®šï¼‰
            self.is_trained = True

            # 8. ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            # save_modelãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®‰å…¨ãªå‡¦ç†
            should_save_model = bool(save_model) if save_model is not None else True
            if should_save_model:
                # training_resultã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                # ModelMetadata dataclassã‚’ä½¿ç”¨ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                model_metadata = ModelMetadata.from_training_result(
                    training_result=training_result,
                    training_params=training_params,
                    model_type=self.__class__.__name__,
                    feature_count=(
                        len(self.feature_columns) if self.feature_columns else 0
                    ),
                )

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
                model_metadata.log_summary()

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
                validation_result = model_metadata.validate()
                if not validation_result["is_valid"]:
                    logger.warning("ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å•é¡ŒãŒã‚ã‚Šã¾ã™:")
                    for error in validation_result["errors"]:
                        logger.warning(f"  ã‚¨ãƒ©ãƒ¼: {error}")
                for warning in validation_result["warnings"]:
                    logger.warning(f"  è­¦å‘Š: {warning}")

                model_path = self.save_model(
                    model_name or self.config.model.AUTO_STRATEGY_MODEL_NAME,
                    model_metadata.to_dict(),
                )
                training_result["model_path"] = model_path

            # 9. å­¦ç¿’çµæœã‚’æ•´å½¢
            result = self._format_training_result(training_result, X, y)

            logger.info("MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
            return result

    @safe_ml_operation(default_return={}, context="ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    def evaluate_model(
        self,
        test_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> Dict[str, Any]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

        Args:
            test_data: ãƒ†ã‚¹ãƒˆç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        if not self.is_trained:
            raise ModelError("è©•ä¾¡å¯¾è±¡ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        # ç‰¹å¾´é‡ã‚’è¨ˆç®—
        features_df = self._calculate_features(
            test_data, funding_rate_data, open_interest_data
        )

        # äºˆæ¸¬ã‚’å®Ÿè¡Œ
        predictions = self.predict(features_df)

        # è©•ä¾¡çµæœã‚’ä½œæˆ
        evaluation_result = {
            "predictions": predictions,
            "test_samples": len(test_data),
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "model_status": "trained" if self.is_trained else "not_trained",
        }

        return evaluation_result

    def predict(self, features_df: pd.DataFrame) -> np.ndarray:
        """
        çµ±åˆã•ã‚ŒãŸäºˆæ¸¬å®Ÿè¡Œ

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        if not self.is_trained:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        if self.trainer_type == "ensemble":
            return self._predict_ensemble(features_df)
        else:
            return self._predict_single(features_df)

    def _train_model_impl(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        çµ±åˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè£…

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        if self.trainer_type == "ensemble":
            return self._train_ensemble_model(
                X_train, X_test, y_train, y_test, **training_params
            )
        else:
            return self._train_single_model(
                X_train, X_test, y_train, y_test, **training_params
            )

    def _train_single_model(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        try:
            logger.info(f"ğŸ¤– å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹: {self.model_type}")

            # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨å­¦ç¿’
            from .single_model.single_model_trainer import SingleModelTrainer

            # ä¸€æ™‚çš„ã«SingleModelTrainerã‚’ä½¿ç”¨ï¼ˆå¾Œã§çµ±åˆï¼‰
            trainer = SingleModelTrainer(
                model_type=self.model_type, automl_config=self.automl_config
            )

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            X_combined = pd.concat([X_train, X_test])
            y_combined = pd.concat([y_train, y_test])
            training_data = X_combined.copy()
            training_data["target"] = y_combined

            # å­¦ç¿’å®Ÿè¡Œ
            result = trainer.train_model(training_data, **training_params)

            # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            self.model = trainer.model
            self.is_trained = True

            logger.info(f"âœ… å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†: {self.model_type}")
            return result

        except Exception as e:
            logger.error(f"âŒ å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _train_ensemble_model(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        try:
            logger.info(
                f"ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’é–‹å§‹: {self.ensemble_config.get('method', 'bagging')}"
            )

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆã¨å­¦ç¿’
            from .ensemble.ensemble_trainer import EnsembleTrainer

            # ä¸€æ™‚çš„ã«EnsembleTrainerã‚’ä½¿ç”¨ï¼ˆå¾Œã§çµ±åˆï¼‰
            trainer = EnsembleTrainer(
                ensemble_config=self.ensemble_config, automl_config=self.automl_config
            )

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            X_combined = pd.concat([X_train, X_test])
            y_combined = pd.concat([y_train, y_test])
            training_data = X_combined.copy()
            training_data["target"] = y_combined

            # å­¦ç¿’å®Ÿè¡Œ
            result = trainer.train_model(training_data, **training_params)

            # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆEnsembleTrainerã«å§”è­²ã—ã€BaseMLTrainerã§ã¯ä¿å­˜ã—ãªã„ï¼‰
            self.models = trainer.models
            self.model = trainer  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è‡ªä½“ã‚’ä¿å­˜
            self.is_trained = True

            # EnsembleTrainerãŒæ—¢ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã„ã‚‹ãŸã‚ã€BaseMLTrainerã§ã¯é‡è¤‡ä¿å­˜ã‚’é¿ã‘ã‚‹
            self._ensemble_trainer = trainer  # å‚ç…§ã‚’ä¿æŒ

            logger.info(
                f"âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’å®Œäº†: {self.ensemble_config.get('method', 'bagging')}"
            )
            return result

        except Exception as e:
            logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _predict_single(self, features_df: pd.DataFrame) -> np.ndarray:
        """
        å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        if self.model is None:
            raise ValueError("å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # ç‰¹å¾´é‡ã®å‰å‡¦ç†
            processed_features = self._preprocess_features_for_prediction(features_df)

            # äºˆæ¸¬å®Ÿè¡Œ
            if hasattr(self.model, "predict"):
                predictions = self.model.predict(processed_features)
            else:
                # SingleModelTrainerã®å ´åˆ
                predictions = self.model.predict(features_df)

            return predictions

        except Exception as e:
            logger.error(f"âŒ å˜ä¸€ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _predict_ensemble(self, features_df: pd.DataFrame) -> np.ndarray:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        if self.model is None:
            raise ValueError("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # EnsembleTrainerã®äºˆæ¸¬ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            predictions = self.model.predict(features_df)
            return predictions

        except Exception as e:
            logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _preprocess_features_for_prediction(
        self, features_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡å‰å‡¦ç†

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            å‰å‡¦ç†æ¸ˆã¿ç‰¹å¾´é‡
        """
        try:
            # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®é¸æŠ
            if self.feature_columns is not None:
                # å­¦ç¿’æ™‚ã®ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®ã¿ã‚’ä½¿ç”¨
                available_columns = [
                    col for col in self.feature_columns if col in features_df.columns
                ]
                processed_features = features_df[available_columns].copy()
            else:
                processed_features = features_df.copy()

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if hasattr(self, "scaler") and self.scaler is not None:
                try:
                    processed_features = pd.DataFrame(
                        self.scaler.transform(processed_features),
                        columns=processed_features.columns,
                        index=processed_features.index,
                    )
                except Exception as e:
                    logger.warning(f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")

            return processed_features

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return features_df

    def _validate_training_data(self, training_data: pd.DataFrame) -> None:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        if training_data is None or training_data.empty:
            raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

        required_columns = ["Open", "High", "Low", "Close", "Volume"]
        missing_columns = [
            col for col in required_columns if col not in training_data.columns
        ]
        if missing_columns:
            raise DataError(f"å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")

        if len(training_data) < 100:
            raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½100è¡Œå¿…è¦ï¼‰")

    def _calculate_features(
        self,
        ohlcv_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """
        ç‰¹å¾´é‡ã‚’è¨ˆç®—ï¼ˆFeatureEngineeringServiceã«å®Œå…¨å§”è­²ï¼‰

        è²¬å‹™åˆ†å‰²ã«ã‚ˆã‚Šã€å…·ä½“çš„ãªç‰¹å¾´é‡è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã¯
        FeatureEngineeringServiceã«ç§»è­²ã•ã‚Œã¾ã—ãŸã€‚
        """
        try:
            # AutoMLã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯æ‹¡å¼µç‰¹å¾´é‡è¨ˆç®—ã‚’å®Ÿè¡Œ
            if self.use_automl and hasattr(
                self.feature_service, "calculate_enhanced_features"
            ):
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’è¨ˆç®—ï¼ˆAutoMLç‰¹å¾´é‡ç”Ÿæˆç”¨ï¼‰
                target = self._calculate_target_for_automl(ohlcv_data)

                logger.info("ğŸ¤– AutoMLæ‹¡å¼µç‰¹å¾´é‡è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")
                return self.feature_service.calculate_enhanced_features(
                    ohlcv_data=ohlcv_data,
                    funding_rate_data=funding_rate_data,
                    open_interest_data=open_interest_data,
                    automl_config=self.automl_config,
                    target=target,
                )
            else:
                # åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—ï¼ˆFear & Greed ãƒ‡ãƒ¼ã‚¿è‡ªå‹•å–å¾—ã‚’æœ‰åŠ¹åŒ–ï¼‰
                logger.info("ğŸ“Š åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")
                return self.feature_service.calculate_advanced_features(
                    ohlcv_data=ohlcv_data,
                    funding_rate_data=funding_rate_data,
                    open_interest_data=open_interest_data,
                    auto_fetch_fear_greed=True,  # è‡ªå‹•å–å¾—ã‚’æœ‰åŠ¹åŒ–
                )

        except Exception as e:
            logger.warning(f"æ‹¡å¼µç‰¹å¾´é‡è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼ã€åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬ç‰¹å¾´é‡ã®ã¿
            return self.feature_service.calculate_advanced_features(
                ohlcv_data,
                funding_rate_data,
                open_interest_data,
                auto_fetch_fear_greed=False,
            )

    def _calculate_target_for_automl(
        self, ohlcv_data: pd.DataFrame
    ) -> Optional[pd.Series]:
        """
        AutoMLç‰¹å¾´é‡ç”Ÿæˆç”¨ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’è¨ˆç®—

        ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã¯label_generation.pyã«ç§»ç®¡ã•ã‚Œã¾ã—ãŸã€‚
        """
        from ...utils.label_generation import calculate_target_for_automl

        return calculate_target_for_automl(ohlcv_data, self.config)

    # _get_fear_greed_data ãƒ¡ã‚½ãƒƒãƒ‰ã¯ FeatureEngineeringService ã«ç§»å‹•ã•ã‚Œã¾ã—ãŸ

    def _prepare_training_data(
        self, features_df: pd.DataFrame, **training_params
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """
        å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆutils/data_processing.pyã«å§”è­²ï¼‰

        è²¬å‹™åˆ†å‰²ã«ã‚ˆã‚Šã€å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã¯
        utils/data_processing.pyã«ç§»è­²ã•ã‚Œã¾ã—ãŸã€‚
        """
        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆå™¨ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆLabelGeneratorWrapperã¯å‰Šé™¤ï¼‰
        from ...utils.label_generation import LabelGenerator

        label_generator = LabelGenerator()

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’å§”è­²
        features_clean, labels_clean, threshold_info = (
            data_preprocessor.prepare_training_data(
                features_df, label_generator, **training_params
            )
        )

        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’ä¿å­˜
        self.feature_columns = features_clean.columns.tolist()

        return features_clean, labels_clean

    def _split_data(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ï¼ˆæ™‚ç³»åˆ—å¯¾å¿œï¼‰

        æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€å°†æ¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ã“ã¨ã‚’é˜²ããŸã‚ã€
        æ™‚é–“é †åºã‚’ä¿æŒã—ãŸåˆ†å‰²ã‚’è¡Œã„ã¾ã™ã€‚
        """
        test_size = training_params.get("test_size", 0.2)
        random_state = training_params.get("random_state", 42)
        use_time_series_split = training_params.get("use_time_series_split", True)

        if use_time_series_split:
            # æ™‚ç³»åˆ—åˆ†å‰²ï¼šæ™‚é–“é †åºã‚’ä¿æŒã—ã¦åˆ†å‰²
            logger.info("ğŸ•’ æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰")

            # ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’å–å¾—
            n_samples = len(X)
            train_size = int(n_samples * (1 - test_size))

            # æ™‚é–“é †åºã‚’ä¿æŒã—ã¦åˆ†å‰²
            X_train = X.iloc[:train_size].copy()
            X_test = X.iloc[train_size:].copy()
            y_train = y.iloc[:train_size].copy()
            y_test = y.iloc[train_size:].copy()

            logger.info(
                f"æ™‚ç³»åˆ—åˆ†å‰²çµæœ: å­¦ç¿’={len(X_train)}ã‚µãƒ³ãƒ—ãƒ«, ãƒ†ã‚¹ãƒˆ={len(X_test)}ã‚µãƒ³ãƒ—ãƒ«"
            )
            logger.info(f"å­¦ç¿’æœŸé–“: {X_train.index[0]} ï½ {X_train.index[-1]}")
            logger.info(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {X_test.index[0]} ï½ {X_test.index[-1]}")

        else:
            # å¾“æ¥ã®ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰

            # å±¤åŒ–æŠ½å‡ºã¯ã€ãƒ©ãƒ™ãƒ«ãŒ2ç¨®é¡ä»¥ä¸Šã‚ã‚‹å ´åˆã«ã®ã¿æœ‰åŠ¹
            stratify_param = y if y.nunique() > 1 else None
            if stratify_param is None:
                logger.warning(
                    "ãƒ©ãƒ™ãƒ«ãŒ1ç¨®é¡ä»¥ä¸‹ã®ãŸã‚ã€å±¤åŒ–æŠ½å‡ºãªã—ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¾ã™ã€‚"
                )

            # train_test_splitã¯ãƒªã‚¹ãƒˆã‚’è¿”ã™ãŸã‚ã€ä¸€åº¦å¤‰æ•°ã«å—ã‘ã¦ã‹ã‚‰ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹
            splits = train_test_split(
                X,
                y,
                test_size=test_size,
                random_state=random_state,
                stratify=stratify_param,
            )

            # å‹ãƒã‚§ãƒƒã‚«ãƒ¼ã®ãŸã‚ã«æ˜ç¤ºçš„ã«ã‚­ãƒ£ã‚¹ãƒˆ
            X_train = cast(pd.DataFrame, splits[0])
            X_test = cast(pd.DataFrame, splits[1])
            y_train = cast(pd.Series, splits[2])
            y_test = cast(pd.Series, splits[3])

        # åˆ†å‰²å¾Œã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã‚’ç¢ºèª
        logger.info("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
        for label_value in sorted(y_train.unique()):
            count = (y_train == label_value).sum()
            percentage = count / len(y_train) * 100
            logger.info(f"  ãƒ©ãƒ™ãƒ« {label_value}: {count}ã‚µãƒ³ãƒ—ãƒ« ({percentage:.1f}%)")

        logger.info("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
        for label_value in sorted(y_test.unique()):
            count = (y_test == label_value).sum()
            percentage = count / len(y_test) * 100
            logger.info(f"  ãƒ©ãƒ™ãƒ« {label_value}: {count}ã‚µãƒ³ãƒ—ãƒ« ({percentage:.1f}%)")

        return X_train, X_test, y_train, y_test

    def _time_series_cross_validate(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Dict[str, Any]:
        """
        æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

        ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æ¤œè¨¼ã‚’è¡Œã„ã€ã‚ˆã‚Šå …ç‰¢ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚’æä¾›ã—ã¾ã™ã€‚

        Args:
            X: ç‰¹å¾´é‡DataFrame
            y: ãƒ©ãƒ™ãƒ«Series
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®è¾æ›¸
        """
        n_splits = training_params.get("cv_splits", 5)
        max_train_size = training_params.get("max_train_size", None)

        logger.info(f"ğŸ”„ æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ï¼ˆ{n_splits}åˆ†å‰²ï¼‰")

        # TimeSeriesSplitã‚’åˆæœŸåŒ–
        tscv = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size)

        cv_scores = []
        fold_results = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
            logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold}/{n_splits} ã‚’å®Ÿè¡Œä¸­...")

            # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
            X_train_cv = X.iloc[train_idx]
            X_test_cv = X.iloc[test_idx]
            y_train_cv = y.iloc[train_idx]
            y_test_cv = y.iloc[test_idx]

            # ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†
            X_train_scaled, X_test_scaled = self._preprocess_data(X_train_cv, X_test_cv)

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’ã‚’å®Ÿè¡Œ
            fold_result = self._train_fold_with_error_handling(
                fold,
                X_train_scaled,
                X_test_scaled,
                y_train_cv,
                y_test_cv,
                X_train_cv,
                X_test_cv,
                training_params,
            )

            cv_scores.append(fold_result.get("accuracy", 0.0))
            fold_results.append(fold_result)

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’é›†è¨ˆ
        cv_result = {
            "cv_scores": cv_scores,
            "cv_mean": np.mean(cv_scores),
            "cv_std": np.std(cv_scores),
            "cv_min": np.min(cv_scores),
            "cv_max": np.max(cv_scores),
            "fold_results": fold_results,
            "n_splits": n_splits,
        }

        logger.info("æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†:")
        logger.info(
            f"  å¹³å‡ç²¾åº¦: {cv_result['cv_mean']:.4f} Â± {cv_result['cv_std']:.4f}"
        )
        logger.info(f"  æœ€å°ç²¾åº¦: {cv_result['cv_min']:.4f}")
        logger.info(f"  æœ€å¤§ç²¾åº¦: {cv_result['cv_max']:.4f}")

        return cv_result

    def _preprocess_data(
        self, X_train: pd.DataFrame, X_test: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰"""
        # LightGBMãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦
        if hasattr(self, "model_type") and "LightGBM" in str(self.model_type):
            return X_train, X_test

        # ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ
        assert self.scaler is not None, "ScalerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        X_train_scaled = pd.DataFrame(
            self.scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index,
        )

        X_test_scaled = pd.DataFrame(
            self.scaler.transform(X_test), columns=X_test.columns, index=X_test.index
        )

        return X_train_scaled, X_test_scaled

    def save_model(
        self, model_name: str, metadata: Optional[Dict[str, Any]] = None
    ) -> Optional[str]:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        if not self.is_trained:
            raise ModelError("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã¡ã‚‰ã«å§”è­²
        if hasattr(self, "_ensemble_trainer") and self._ensemble_trainer:
            logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã«ä¿å­˜ã‚’å§”è­²ã—ã¾ã™")
            return self._ensemble_trainer.save_model(model_name, metadata)

        # åŸºæœ¬çš„ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        final_metadata = {
            "model_type": self.__class__.__name__,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "is_trained": self.is_trained,
        }
        # æä¾›ã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°
        if metadata:
            final_metadata.update(metadata)

        # ç‰¹å¾´é‡é‡è¦åº¦ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        try:
            feature_importance = self.get_feature_importance(top_n=100)
            if feature_importance:
                final_metadata["feature_importance"] = feature_importance
                logger.info(
                    f"ç‰¹å¾´é‡é‡è¦åº¦ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ : {len(feature_importance)}å€‹"
                )
        except Exception as e:
            logger.warning(f"ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—ã«å¤±æ•—: {e}")

        # çµ±ä¸€ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚’ä½¿ç”¨
        model_path = model_manager.save_model(
            model=self,  # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼å…¨ä½“ã‚’ä¿å­˜
            model_name=model_name,
            metadata=final_metadata,
            scaler=self.scaler,
            feature_columns=self.feature_columns,
        )

        logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")
        return model_path

    def _format_training_result(
        self, training_result: Dict[str, Any], X: pd.DataFrame, y: pd.Series
    ) -> Dict[str, Any]:
        """å­¦ç¿’çµæœã‚’æ•´å½¢"""
        result = {
            "success": True,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "total_samples": len(X),
            **training_result,
        }

        return result

    def get_feature_importance(self, top_n: int = 10) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—

        Args:
            top_n: ä¸Šä½Nå€‹ã®ç‰¹å¾´é‡

        Returns:
            ç‰¹å¾´é‡é‡è¦åº¦ã®è¾æ›¸
        """
        if not self.is_trained:
            logger.warning("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã¡ã‚‰ã«å§”è­²
        if hasattr(self, "_ensemble_trainer") and self._ensemble_trainer:
            if hasattr(self._ensemble_trainer, "get_feature_importance"):
                try:
                    feature_importance = self._ensemble_trainer.get_feature_importance()
                    if feature_importance:
                        # ä¸Šä½Nå€‹ã‚’å–å¾—
                        sorted_importance = sorted(
                            feature_importance.items(), key=lambda x: x[1], reverse=True
                        )[:top_n]
                        logger.info(
                            f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‹ã‚‰ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—: {len(sorted_importance)}å€‹"
                        )
                        return dict(sorted_importance)
                except Exception as e:
                    logger.error(
                        f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‹ã‚‰ã®ç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"
                    )

        # ãƒ¢ãƒ‡ãƒ«ãŒç‰¹å¾´é‡é‡è¦åº¦ã‚’æä¾›ã™ã‚‹å ´åˆ
        if hasattr(self.model, "get_feature_importance"):
            try:
                feature_importance = self.model.get_feature_importance(top_n)
                if feature_importance:
                    logger.info(
                        f"ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—: {len(feature_importance)}å€‹"
                    )
                    return feature_importance
            except Exception as e:
                logger.error(f"ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®ç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        # LightGBMãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
        if hasattr(self.model, "feature_importance") and self.feature_columns:
            try:
                importance_scores = self.model.feature_importance(
                    importance_type="gain"
                )
                feature_importance = dict(zip(self.feature_columns, importance_scores))

                # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½Nå€‹ã‚’å–å¾—
                sorted_importance = sorted(
                    feature_importance.items(), key=lambda x: x[1], reverse=True
                )[:top_n]

                logger.info(
                    f"LightGBMã‹ã‚‰ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—: {len(sorted_importance)}å€‹"
                )
                return dict(sorted_importance)
            except Exception as e:
                logger.error(f"LightGBMç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                return {}

        logger.warning("ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç‰¹å¾´é‡é‡è¦åº¦ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“")
        return {}

    @safe_ml_operation(
        default_return=False, context="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )
    def load_model(self, model_path: str) -> bool:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿

        Args:
            model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            èª­ã¿è¾¼ã¿æˆåŠŸãƒ•ãƒ©ã‚°
        """
        model_data = model_manager.load_model(model_path)

        if model_data is None:
            return False

        # ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„è¦ç´ ã‚’å–å¾—
        self.model = model_data.get("model")
        self.scaler = model_data.get("scaler")
        self.feature_columns = model_data.get("feature_columns")

        if self.model is None:
            raise MLModelError("ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        self.is_trained = True
        return True

    def _cleanup_temporary_files(self, level: CleanupLevel):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # BaseMLTrainerã§ã¯ç‰¹ã«ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œæˆã—ãªã„ãŸã‚ã€ãƒ‘ã‚¹
        pass

    def _cleanup_cache(self, level: CleanupLevel):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.feature_service is not None:
                if hasattr(self.feature_service, "clear_automl_cache"):
                    self.feature_service.clear_automl_cache()
                    logger.debug("ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}")

    def _cleanup_models(self, level: CleanupLevel):
        """ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.feature_service is not None:
                if hasattr(self.feature_service, "cleanup_resources"):
                    self.feature_service.cleanup_resources()
                    logger.debug("ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")

            # ãƒ¢ãƒ‡ãƒ«ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
            self.model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False

            # AutoMLè¨­å®šã‚’ã‚¯ãƒªã‚¢ï¼ˆTHOROUGH ãƒ¬ãƒ™ãƒ«ã®å ´åˆã®ã¿ï¼‰
            if level == CleanupLevel.THOROUGH:
                self.automl_config = None

        except Exception as e:
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¯ç¶šè¡Œ
            self.model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False
            if level == CleanupLevel.THOROUGH:
                self.automl_config = None

    @safe_ml_operation(
        default_return={
            "fold": 0,
            "error": "ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            "accuracy": 0.0,
        },
        context="ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’",
    )
    def _train_fold_with_error_handling(
        self,
        fold: int,
        X_train_scaled: pd.DataFrame,
        X_test_scaled: pd.DataFrame,
        y_train_cv: pd.Series,
        y_test_cv: pd.Series,
        X_train_cv: pd.DataFrame,
        X_test_cv: pd.DataFrame,
        training_params: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ããƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’

        Args:
            fold: ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ç•ªå·
            X_train_scaled: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¸ˆã¿å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test_scaled: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train_cv: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test_cv: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            X_train_cv: å…ƒã®å­¦ç¿’ç”¨ç‰¹å¾´é‡ï¼ˆæœŸé–“æƒ…å ±ç”¨ï¼‰
            X_test_cv: å…ƒã®ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡ï¼ˆæœŸé–“æƒ…å ±ç”¨ï¼‰
            training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’çµæœã®è¾æ›¸
        """
        # ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        fold_result = self._train_model_impl(
            X_train_scaled,
            X_test_scaled,
            y_train_cv,
            y_test_cv,
            **training_params,
        )

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’è¿½åŠ 
        fold_result.update(
            {
                "fold": fold,
                "train_samples": len(X_train_cv),
                "test_samples": len(X_test_cv),
                "train_period": f"{X_train_cv.index[0]} ï½ {X_train_cv.index[-1]}",
                "test_period": f"{X_test_cv.index[0]} ï½ {X_test_cv.index[-1]}",
            }
        )

        logger.info(
            f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold} å®Œäº†: ç²¾åº¦={fold_result.get('accuracy', 0.0):.4f}"
        )

        return fold_result
