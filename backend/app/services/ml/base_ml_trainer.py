"""
MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»å‰å‡¦ç†ãƒ»ä¿å­˜ã«é–¢ã‚ã‚‹å…±é€šãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã™ã‚‹æŠ½è±¡åŸºç›¤ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
å…·ä½“çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„æœ€é©åŒ–æ‰‹æ³•ã®è©³ç´°èª¬æ˜ã¯Docstringã«å«ã‚ã¾ã›ã‚“ã€‚
ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ãŒãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®å­¦ç¿’å‡¦ç†ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
"""

import logging

from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

from ...config.unified_config import unified_config
from ...utils.error_handler import (
    DataError,
    ml_operation_context,
    safe_ml_operation,
)
from .cross_validation import PurgedKFold


from .common.base_resource_manager import BaseResourceManager, CleanupLevel
from .common.evaluation_utils import evaluate_model_predictions
from .common.ml_utils import get_feature_importance_unified, prepare_data_for_prediction
from .config import ml_config
from .exceptions import MLModelError
from .feature_engineering.feature_engineering_service import FeatureEngineeringService
from .ml_metadata import ModelMetadata
from .model_manager import model_manager
from .label_generation.label_generation_service import LabelGenerationService

logger = logging.getLogger(__name__)


class BaseMLTrainer(BaseResourceManager, ABC):
    """
    MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

    å…±é€šã®å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã—ã€å…·ä½“çš„ãªå®Ÿè£…ã¯ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§è¡Œã„ã¾ã™ã€‚
    å˜ä¸€è²¬ä»»åŸå‰‡ã«å¾“ã„ã€å­¦ç¿’ã«é–¢ã™ã‚‹è²¬ä»»ã®ã¿ã‚’æŒã¡ã¾ã™ã€‚
    """

    def __init__(
        self,
        trainer_config: Optional[Dict[str, Any]] = None,
        trainer_type: Optional[str] = None,
        model_type: Optional[str] = None,
    ):
        """
        åˆæœŸåŒ–

        Args:
            trainer_config: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®š
            trainer_type: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆäº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰
            model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆäº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰
        """
        # BaseResourceManagerã®åˆæœŸåŒ–
        super().__init__()

        self.config = ml_config

        self.feature_service = FeatureEngineeringService()
        self.label_service = LabelGenerationService()
        logger.debug(
            "ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã¨ãƒ©ãƒ™ãƒ«ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ"
        )

        self.trainer_config = trainer_config or {}

        # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®è¨­å®šï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ç¶­æŒï¼‰
        self.trainer_type = trainer_type or self.trainer_config.get("type", "single")
        self.model_type = model_type or self.trainer_config.get(
            "model_type", "lightgbm"
        )
        self.ensemble_config = self.trainer_config.get("ensemble_config", {})

        self.scaler = StandardScaler()
        self.feature_columns = None
        self.is_trained = False
        self._model = None
        self.last_training_results = None

    @safe_ml_operation(
        default_return={"success": False}, context="MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )
    def train_model(
        self,
        training_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        save_model: bool = True,
        model_name: Optional[str] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_model: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸
        """
        with ml_operation_context("MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’"):
            # 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            self._validate_training_data(training_data)

            # 2. ç‰¹å¾´é‡ã‚’è¨ˆç®—
            features_df = self._calculate_features(
                training_data, funding_rate_data, open_interest_data
            )

            # 3. å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            X, y = self._prepare_training_data(features_df, **training_params)

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if X is None or X.empty or y is None or y.empty:
                raise DataError("å‰å‡¦ç†å¾Œã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

            # 4. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            use_cross_validation = training_params.get("use_cross_validation", False)

            if use_cross_validation:
                # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
                cv_result = self._time_series_cross_validate(X, y, **training_params)

                # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
                logger.info("ğŸ¯ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ä¸­...")
                X_scaled = self._preprocess_data(X, X)[0]  # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

                # ãƒ€ãƒŸãƒ¼ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€å¾Œã®20%ï¼‰ã‚’ä½œæˆ
                test_size = training_params.get("test_size", 0.2)
                n_samples = len(X)
                train_size = int(n_samples * (1 - test_size))

                X_train_final = X_scaled.iloc[:train_size]
                X_test_final = X_scaled.iloc[train_size:]
                y_train_final = y.iloc[:train_size]
                y_test_final = y.iloc[train_size:]

                training_result = self._train_model_impl(
                    X_train_final,
                    X_test_final,
                    y_train_final,
                    y_test_final,
                    **training_params,
                )

                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¿½åŠ 
                training_result.update(cv_result)

            else:
                # é€šå¸¸ã®å˜ä¸€åˆ†å‰²å­¦ç¿’
                # 4. ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
                X_train, X_test, y_train, y_test = self._split_data(
                    X, y, **training_params
                )

                # 5. ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†
                X_train_scaled, X_test_scaled = self._preprocess_data(X_train, X_test)

                # 6. ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
                training_result = self._train_model_impl(
                    X_train_scaled, X_test_scaled, y_train, y_test, **training_params
                )

            # 7. å­¦ç¿’å®Œäº†ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
            self.is_trained = True

            # 8. ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            should_save_model = bool(save_model) if save_model is not None else True
            if should_save_model:
                model_metadata = ModelMetadata.from_training_result(
                    training_result=training_result,
                    training_params=training_params,
                    model_type=self.__class__.__name__,
                    feature_count=(
                        len(self.feature_columns) if self.feature_columns else 0
                    ),
                )

                model_metadata.log_summary()

                validation_result = model_metadata.validate()
                if not validation_result["is_valid"]:
                    logger.warning(
                        f"ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å•é¡Œ: {validation_result['errors']}"
                    )

                model_path = self.save_model(
                    model_name or self.config.model.AUTO_STRATEGY_MODEL_NAME,
                    model_metadata.to_dict(),
                )
                training_result["model_path"] = model_path

            # 9. å­¦ç¿’çµæœã‚’æ•´å½¢
            result = self._format_training_result(training_result, X, y)

            logger.info("MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
            return result

    @safe_ml_operation(default_return={}, context="ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    def evaluate_model(
        self,
        test_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> Dict[str, Any]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

        Args:
            test_data: ãƒ†ã‚¹ãƒˆç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        if not self.is_trained:
            raise MLModelError("è©•ä¾¡å¯¾è±¡ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        # ç‰¹å¾´é‡ã‚’è¨ˆç®—
        features_df = self._calculate_features(
            test_data, funding_rate_data, open_interest_data
        )

        # äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
        # predictã¯ç¢ºç‡ã‚’è¿”ã™ï¼ˆSingleModelTrainer, StackingEnsembleã¨ã‚‚ã«ï¼‰
        predictions_proba = self.predict(features_df)

        # ã‚¯ãƒ©ã‚¹äºˆæ¸¬ï¼ˆç¢ºç‡æœ€å¤§ï¼‰
        if predictions_proba.ndim == 2:
            predictions_class = np.argmax(predictions_proba, axis=1)
        else:
            predictions_class = (predictions_proba > 0.5).astype(int)

        # è©•ä¾¡çµæœã‚’ä½œæˆ
        evaluation_result = {
            "predictions_proba": predictions_proba,
            "predictions_class": predictions_class,
            "test_samples": len(features_df),
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "model_status": "trained" if self.is_trained else "not_trained",
        }

        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        try:
            # ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆNaNã¯å‰Šé™¤ã•ã‚Œã‚‹ï¼‰
            # configã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ
            features_clean, labels_numeric = self.label_service.prepare_labels(
                features_df,
                prediction_horizon=self.config.training.PREDICTION_HORIZON,
            )

            if len(labels_numeric) > 0:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬å€¤ã‚’ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ
                # features_cleanã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾å¿œã™ã‚‹äºˆæ¸¬å€¤ã‚’æŠ½å‡º
                valid_indices = features_df.index.get_indexer(features_clean.index)

                # äºˆæ¸¬å€¤ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                y_pred_class_aligned = predictions_class[valid_indices]
                y_pred_proba_aligned = (
                    predictions_proba[valid_indices]
                    if predictions_proba.ndim == 2
                    else predictions_proba[valid_indices]
                )

                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆå…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ï¼‰
                metrics = evaluate_model_predictions(
                    labels_numeric, y_pred_class_aligned, y_pred_proba_aligned
                )

                evaluation_result.update(metrics)
                logger.info("âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—å®Œäº†")
            else:
                logger.warning(
                    "è©•ä¾¡ç”¨ãƒ©ãƒ™ãƒ«ãŒç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å¯èƒ½æ€§ï¼‰"
                )

        except Exception as e:
            logger.warning(f"è©•ä¾¡ç”¨ãƒ©ãƒ™ãƒ«ç”Ÿæˆã¾ãŸã¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—å¤±æ•—: {e}")

        return evaluation_result

    @abstractmethod
    def predict(self, features_df: pd.DataFrame) -> np.ndarray:
        """
        äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        pass

    def predict_signal(self, features_df: pd.DataFrame) -> Dict[str, float]:
        """
        ã‚·ã‚°ãƒŠãƒ«äºˆæ¸¬ï¼ˆã‚¯ãƒ©ã‚¹ç¢ºç‡ï¼‰ã‚’å®Ÿè¡Œ
        å‰å‡¦ç†ã‹ã‚‰äºˆæ¸¬ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¾ã§ã‚’ä¸€è²«ã—ã¦è¡Œã†

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬ç¢ºç‡ã®è¾æ›¸ {"up": float, "down": float, "range": float}
        """
        if not self.is_trained:
            logger.warning("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return self.config.prediction.get_default_predictions()

        try:
            # 1. å‰å‡¦ç†ï¼ˆã‚«ãƒ©ãƒ èª¿æ•´ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰
            processed_features = self._preprocess_features_for_prediction(features_df)

            # 2. äºˆæ¸¬å®Ÿè¡Œï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã®predictã‚’å‘¼ã³å‡ºã—ï¼‰
            # predictã¯ç¢ºç‡é…åˆ—ã‚’è¿”ã™ã“ã¨ã‚’æœŸå¾…
            predictions = self.predict(processed_features)

            # 3. æœ€æ–°ã®äºˆæ¸¬çµæœã‚’å–å¾—ï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯æœ€å¾Œã®è¡Œï¼‰
            if predictions.ndim == 2:
                latest_pred = predictions[-1]
            else:
                latest_pred = predictions

            # 4. çµæœã®æ•´å½¢
            if latest_pred.shape[0] == 3:
                # 3ã‚¯ãƒ©ã‚¹åˆ†é¡ (down, range, up)
                return {
                    "down": float(latest_pred[0]),
                    "range": float(latest_pred[1]),
                    "up": float(latest_pred[2]),
                }
            elif latest_pred.shape[0] == 2:
                # 2ã‚¯ãƒ©ã‚¹åˆ†é¡ (range, trend) ã¾ãŸã¯ (class0, class1)
                # æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã«åˆã‚ã›ã¦ range, trend ã¨ã™ã‚‹
                # ãŸã ã—ã€(down, up)ã®å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã®ã§æ³¨æ„ãŒå¿…è¦ã ãŒã€
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ…£ç¿’ã¨ã—ã¦ (range, trend) ãŒå¤šã„ã¨ä»®å®š
                return {
                    "range": float(latest_pred[0]),
                    "trend": float(latest_pred[1]),
                }
            else:
                logger.error(f"äºˆæœŸã—ãªã„ã‚¯ãƒ©ã‚¹æ•°: {latest_pred.shape[0]}")
                return self.config.prediction.get_default_predictions()

        except Exception as e:
            logger.error(f"ã‚·ã‚°ãƒŠãƒ«äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return self.config.prediction.get_default_predictions()

    @abstractmethod
    def _train_model_impl(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè£…ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        pass

    def _preprocess_features_for_prediction(
        self, features_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡å‰å‡¦ç†
        - å¿…è¦ãªã‚«ãƒ©ãƒ ã®æŠ½å‡º
        - æ¬ æã‚«ãƒ©ãƒ ã®è£œå®Œï¼ˆ0åŸ‹ã‚ï¼‰
        - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            å‰å‡¦ç†æ¸ˆã¿ç‰¹å¾´é‡
        """
        try:
            return prepare_data_for_prediction(
                features_df,
                expected_columns=self.feature_columns,
                scaler=self.scaler,
            )
        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return features_df

    def _validate_training_data(self, training_data: pd.DataFrame) -> None:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        if training_data is None or training_data.empty:
            raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

        required_columns = ["open", "high", "low", "close", "volume"]
        missing_columns = [
            col for col in required_columns if col not in training_data.columns
        ]
        if missing_columns:
            raise DataError(f"å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")

        if len(training_data) < 100:
            raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½100è¡Œå¿…è¦ï¼‰")

    def _calculate_features(
        self,
        ohlcv_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """ç‰¹å¾´é‡ã‚’è¨ˆç®—"""
        try:
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            if ohlcv_data is None or ohlcv_data.empty:
                raise ValueError("OHLCVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

            # è¨­å®šã‹ã‚‰profileã‚’å–å¾—
            profile = unified_config.ml.feature_engineering.profile
            logger.info(f"ğŸ“Š ç‰¹å¾´é‡è¨ˆç®—ã‚’å®Ÿè¡Œä¸­ï¼ˆprofile: {profile}ï¼‰...")

            # åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—
            basic_features = self.feature_service.calculate_advanced_features(
                ohlcv_data=ohlcv_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                profile=profile,
            )

            logger.info(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(basic_features.columns)}å€‹ã®ç‰¹å¾´é‡")
            return basic_features

        except Exception as e:
            logger.warning(f"ç‰¹å¾´é‡è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼ã€åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆç°¡ç•¥åŒ–ï¼‰
            return ohlcv_data.copy()

    def _prepare_training_data(
        self, features_df: pd.DataFrame, **training_params
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        try:
            features_clean, labels_numeric = self.label_service.prepare_labels(
                features_df, **training_params
            )

            self.feature_columns = features_clean.columns.tolist()
            return features_clean, labels_numeric

        except Exception as e:
            logger.error(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise DataError(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def _split_data(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ï¼ˆå¸¸ã«æ™‚ç³»åˆ—åˆ†å‰²ï¼‰"""
        test_size = training_params.get("test_size", 0.2)

        logger.info("ğŸ•’ æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨")
        n_samples = len(X)
        train_size = int(n_samples * (1 - test_size))

        X_train = X.iloc[:train_size].copy()
        X_test = X.iloc[train_size:].copy()
        y_train = y.iloc[:train_size].copy()
        y_test = y.iloc[train_size:].copy()

        return X_train, X_test, y_train, y_test

    def _time_series_cross_validate(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        n_splits = training_params.get(
            "cv_splits", self.config.training.CROSS_VALIDATION_FOLDS
        )
        logger.info(f"ğŸ”„ æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ï¼ˆ{n_splits}åˆ†å‰²ï¼‰")

        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        t1_horizon_n = self.config.training.PREDICTION_HORIZON

        # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ã—ã¦t1ã‚’è¨ˆç®—ï¼ˆæ™‚é–“è¶³ã¯è‡ªå‹•æ¨å®šï¼‰
        from .common.time_series_utils import get_t1_series

        t1 = get_t1_series(X.index, t1_horizon_n)

        pct_embargo = getattr(self.config.training, "PCT_EMBARGO", 0.01)
        splitter = PurgedKFold(n_splits=n_splits, t1=t1, pct_embargo=pct_embargo)

        cv_scores = []
        fold_results = []

        for fold, (train_idx, test_idx) in enumerate(splitter.split(X, y)):
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
            X_train_cv, X_test_cv = X.iloc[train_idx], X.iloc[test_idx]
            y_train_cv, y_test_cv = y.iloc[train_idx], y.iloc[test_idx]

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaler = StandardScaler()
            X_train_scaled = pd.DataFrame(
                scaler.fit_transform(X_train_cv),
                columns=X_train_cv.columns,
                index=X_train_cv.index,
            )
            X_test_scaled = pd.DataFrame(
                scaler.transform(X_test_cv),
                columns=X_test_cv.columns,
                index=X_test_cv.index,
            )

            # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§å­¦ç¿’ãƒ»è©•ä¾¡
            fold_result = self._train_fold_with_error_handling(
                fold + 1,
                X_train_scaled,
                X_test_scaled,
                y_train_cv,
                y_test_cv,
                X_train_cv,
                X_test_cv,
                training_params,
            )

            fold_results.append(fold_result)

            # ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ï¼ˆaccuracyã‚’åŸºæœ¬ã¨ã™ã‚‹ãŒã€åˆ©ç”¨å¯èƒ½ãªã‚‰balanced_accuracyã‚’ä½¿ç”¨ï¼‰
            score = fold_result.get(
                "balanced_accuracy", fold_result.get("accuracy", 0.0)
            )
            cv_scores.append(score)

        # å¹³å‡ã‚¹ã‚³ã‚¢ã¨æ¨™æº–åå·®
        mean_score = np.mean(cv_scores) if cv_scores else 0.0
        std_score = np.std(cv_scores) if cv_scores else 0.0

        logger.info(
            f"âœ… ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†: å¹³å‡ã‚¹ã‚³ã‚¢={mean_score:.4f} (+/- {std_score:.4f})"
        )

        return {
            "cv_scores": cv_scores,
            "mean_score": mean_score,
            "std_score": std_score,
            "fold_results": fold_results,
        }

    def _preprocess_data(
        self, X_train: pd.DataFrame, X_test: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰"""
        if self.scaler is None:
            self.scaler = StandardScaler()

        X_train_scaled = pd.DataFrame(
            self.scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index,
        )
        X_test_scaled = pd.DataFrame(
            self.scaler.transform(X_test), columns=X_test.columns, index=X_test.index
        )
        return X_train_scaled, X_test_scaled

    def _get_model_to_save(self) -> Any:
        """
        ä¿å­˜å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å¯èƒ½
        """
        return self._model

    def _get_model_specific_metadata(self, model_name: str) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å¯èƒ½
        """
        return {}

    def save_model(
        self, model_name: str, metadata: Optional[Dict[str, Any]] = None
    ) -> Optional[str]:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        if not self.is_trained:
            raise MLModelError("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        final_metadata = {
            "model_type": self.__class__.__name__,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "is_trained": self.is_trained,
        }
        if metadata:
            final_metadata.update(metadata)

        # ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        final_metadata.update(self._get_model_specific_metadata(model_name))

        try:
            feature_importance = self.get_feature_importance(top_n=100)
            if feature_importance:
                final_metadata["feature_importance"] = feature_importance
        except Exception as e:
            logger.warning(f"ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—ã«å¤±æ•—: {e}")

        # ä¿å­˜å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        model_to_save = self._get_model_to_save()
        if model_to_save is None:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: selfã‚’ä¿å­˜ï¼ˆãŸã ã—æ¨å¥¨ã•ã‚Œãªã„ï¼‰
            logger.warning("ä¿å­˜å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ãŒNoneã§ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è‡ªä½“ã‚’ä¿å­˜ã—ã¾ã™ã€‚")
            model_to_save = self

        # çµ±ä¸€ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚’ä½¿ç”¨
        model_path = model_manager.save_model(
            model=model_to_save,
            model_name=model_name,
            metadata=final_metadata,
            scaler=self.scaler,
            feature_columns=self.feature_columns,
        )

        logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")
        return model_path

    def _format_training_result(
        self, training_result: Dict[str, Any], X: pd.DataFrame, y: pd.Series
    ) -> Dict[str, Any]:
        """å­¦ç¿’çµæœã‚’æ•´å½¢"""
        result = {
            "success": True,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "total_samples": len(X),
            **training_result,
        }
        return result

    def get_feature_importance(self, top_n: int = 10) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè£…ï¼‰
        ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å¯èƒ½
        """
        if not self.is_trained:
            logger.warning("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}

        return get_feature_importance_unified(
            self._model, self.feature_columns, top_n=top_n
        )

    @safe_ml_operation(
        default_return=False, context="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )
    def load_model(self, model_path: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        model_data = model_manager.load_model(model_path)

        if model_data is None:
            return False

        self._model = model_data.get("model")
        self.scaler = model_data.get("scaler")
        self.feature_columns = model_data.get("feature_columns")

        if self._model is None:
            raise MLModelError("ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        self.is_trained = True
        return True

    def _cleanup_temporary_files(self, level: CleanupLevel):
        pass

    def _cleanup_cache(self, level: CleanupLevel):
        pass

    def _cleanup_models(self, level: CleanupLevel):
        try:
            if self.feature_service is not None:
                if hasattr(self.feature_service, "cleanup_resources"):
                    self.feature_service.cleanup_resources()

            self._model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False
        except Exception as e:
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}")
            self._model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False

    @safe_ml_operation(
        default_return={
            "fold": 0,
            "error": "ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            "accuracy": 0.0,
        },
        context="ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’",
    )
    def _train_fold_with_error_handling(
        self,
        fold: int,
        X_train_scaled: pd.DataFrame,
        X_test_scaled: pd.DataFrame,
        y_train_cv: pd.Series,
        y_test_cv: pd.Series,
        X_train_cv: pd.DataFrame,
        X_test_cv: pd.DataFrame,
        training_params: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ããƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’"""
        fold_result = self._train_model_impl(
            X_train_scaled,
            X_test_scaled,
            y_train_cv,
            y_test_cv,
            **training_params,
        )

        fold_result.update(
            {
                "fold": fold,
                "train_samples": len(X_train_cv),
                "test_samples": len(X_test_cv),
                "train_period": f"{X_train_cv.index[0]} ï½ {X_train_cv.index[-1]}",
                "test_period": f"{X_test_cv.index[0]} ï½ {X_test_cv.index[-1]}",
            }
        )
        return fold_result

    def _prepare_combined_training_data(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
    ) -> pd.DataFrame:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆæº–å‚™ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰"""
        X_combined = pd.concat([X_train, X_test])
        y_combined = pd.concat([y_train, y_test])
        training_data = X_combined.copy()
        training_data["target"] = y_combined
        return training_data
