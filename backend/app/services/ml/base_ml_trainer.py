"""
MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»å‰å‡¦ç†ãƒ»ä¿å­˜ã«é–¢ã‚ã‚‹å…±é€šãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã™ã‚‹æŠ½è±¡åŸºç›¤ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
å…·ä½“çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„æœ€é©åŒ–æ‰‹æ³•ã®è©³ç´°èª¬æ˜ã¯Docstringã«å«ã‚ã¾ã›ã‚“ã€‚
ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ãŒãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®å­¦ç¿’å‡¦ç†ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
"""

import logging
from abc import ABC
from typing import Any, Dict, Optional, Tuple, cast

import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.preprocessing import StandardScaler

from ...config.unified_config import unified_config
from ...utils.data_processing import data_processor as data_preprocessor
from ...utils.error_handler import (
    DataError,
    ml_operation_context,
    safe_ml_operation,
)
from ...utils.label_generation.presets import (
    apply_preset_by_name,
    forward_classification_preset,
    get_common_presets,
)
from .common.base_resource_manager import BaseResourceManager, CleanupLevel
from .config import ml_config
from .exceptions import MLModelError
from .feature_engineering.feature_engineering_service import FeatureEngineeringService
from .ml_metadata import ModelMetadata
from .model_manager import model_manager

logger = logging.getLogger(__name__)


class BaseMLTrainer(BaseResourceManager, ABC):
    """
    MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

    å…±é€šã®å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã—ã€å…·ä½“çš„ãªå®Ÿè£…ã¯ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§è¡Œã„ã¾ã™ã€‚
    å˜ä¸€è²¬ä»»åŸå‰‡ã«å¾“ã„ã€å­¦ç¿’ã«é–¢ã™ã‚‹è²¬ä»»ã®ã¿ã‚’æŒã¡ã¾ã™ã€‚
    """

    def __init__(
        self,
        trainer_config: Optional[Dict[str, Any]] = None,
        trainer_type: Optional[str] = None,
        model_type: Optional[str] = None,
    ):
        """
        åˆæœŸåŒ–

        Args:
            trainer_config: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šï¼ˆå˜ä¸€ãƒ¢ãƒ‡ãƒ«/ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šï¼‰
            trainer_type: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆè„†å¼±æ€§ä¿®æ­£ï¼‰
            model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆè„†å¼±æ€§ä¿®æ­£ï¼‰
        """
        # BaseResourceManagerã®åˆæœŸåŒ–
        super().__init__()

        self.config = ml_config

        self.feature_service = FeatureEngineeringService()
        logger.debug("ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®šã®å‡¦ç†ï¼ˆè„†å¼±æ€§ä¿®æ­£ï¼‰
        self.trainer_config = trainer_config or {}

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®å„ªå…ˆé †ä½: ç›´æ¥æŒ‡å®š > trainer_config > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        self.trainer_type = trainer_type or self.trainer_config.get(
            "type", "single"
        )  # "single" or "ensemble"

        self.model_type = model_type or self.trainer_config.get(
            "model_type", "lightgbm"
        )
        self.ensemble_config = self.trainer_config.get("ensemble_config", {})

        self.scaler = StandardScaler()
        self.feature_columns = None
        self.is_trained = False
        self._model = None  # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆå±æ€§ã¨ã—ã¦å®šç¾©ï¼ˆå­ã‚¯ãƒ©ã‚¹ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã¨ç«¶åˆã‚’å›é¿ï¼‰
        self.models = {}  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ã®è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ ¼ç´
        self.last_training_results = None  # æœ€å¾Œã®å­¦ç¿’çµæœã‚’ä¿æŒ

    @safe_ml_operation(default_return={}, context="MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    def train_model(
        self,
        training_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        save_model: bool = True,
        model_name: Optional[str] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_model: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸

        Raises:
            DataError: ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ãªå ´åˆ
            ModelError: å­¦ç¿’ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        with ml_operation_context("MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’"):
            # 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            self._validate_training_data(training_data)

            # 2. ç‰¹å¾´é‡ã‚’è¨ˆç®—
            features_df = self._calculate_features(
                training_data, funding_rate_data, open_interest_data
            )

            # 3. å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            X, y = self._prepare_training_data(features_df, **training_params)

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if X is None or X.empty or y is None or y.empty:
                raise DataError("å‰å‡¦ç†å¾Œã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

            # 4. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            use_cross_validation = training_params.get("use_cross_validation", False)

            if use_cross_validation:
                # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
                cv_result = self._time_series_cross_validate(X, y, **training_params)

                # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
                logger.info("ğŸ¯ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ä¸­...")
                X_scaled = self._preprocess_data(X, X)[0]  # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

                # ãƒ€ãƒŸãƒ¼ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€å¾Œã®20%ï¼‰ã‚’ä½œæˆ
                test_size = training_params.get("test_size", 0.2)
                n_samples = len(X)
                train_size = int(n_samples * (1 - test_size))

                X_train_final = X_scaled.iloc[:train_size]
                X_test_final = X_scaled.iloc[train_size:]
                y_train_final = y.iloc[:train_size]
                y_test_final = y.iloc[train_size:]

                training_result = self._train_model_impl(
                    X_train_final,
                    X_test_final,
                    y_train_final,
                    y_test_final,
                    **training_params,
                )

                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¿½åŠ 
                training_result.update(cv_result)

            else:
                # é€šå¸¸ã®å˜ä¸€åˆ†å‰²å­¦ç¿’
                # 4. ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
                X_train, X_test, y_train, y_test = self._split_data(
                    X, y, **training_params
                )

                # 5. ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†
                X_train_scaled, X_test_scaled = self._preprocess_data(X_train, X_test)

                # 6. ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
                training_result = self._train_model_impl(
                    X_train_scaled, X_test_scaled, y_train, y_test, **training_params
                )

            # 7. å­¦ç¿’å®Œäº†ãƒ•ãƒ©ã‚°ã‚’è¨­å®šï¼ˆä¿å­˜å‰ã«è¨­å®šï¼‰
            self.is_trained = True

            # 8. ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            # save_modelãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®‰å…¨ãªå‡¦ç†
            should_save_model = bool(save_model) if save_model is not None else True
            if should_save_model:
                # training_resultã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                # ModelMetadata dataclassã‚’ä½¿ç”¨ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                model_metadata = ModelMetadata.from_training_result(
                    training_result=training_result,
                    training_params=training_params,
                    model_type=self.__class__.__name__,
                    feature_count=(
                        len(self.feature_columns) if self.feature_columns else 0
                    ),
                )

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
                model_metadata.log_summary()

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
                validation_result = model_metadata.validate()
                if not validation_result["is_valid"]:
                    logger.warning("ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å•é¡ŒãŒã‚ã‚Šã¾ã™:")
                    for error in validation_result["errors"]:
                        logger.warning(f"  ã‚¨ãƒ©ãƒ¼: {error}")
                for warning in validation_result["warnings"]:
                    logger.warning(f"  è­¦å‘Š: {warning}")

                model_path = self.save_model(
                    model_name or self.config.model.AUTO_STRATEGY_MODEL_NAME,
                    model_metadata.to_dict(),
                )
                training_result["model_path"] = model_path

            # 9. å­¦ç¿’çµæœã‚’æ•´å½¢
            result = self._format_training_result(training_result, X, y)

            logger.info("MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
            return result

    @safe_ml_operation(default_return={}, context="ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    def evaluate_model(
        self,
        test_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> Dict[str, Any]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

        Args:
            test_data: ãƒ†ã‚¹ãƒˆç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        if not self.is_trained:
            raise MLModelError("è©•ä¾¡å¯¾è±¡ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        # ç‰¹å¾´é‡ã‚’è¨ˆç®—
        features_df = self._calculate_features(
            test_data, funding_rate_data, open_interest_data
        )

        # äºˆæ¸¬ã‚’å®Ÿè¡Œ
        predictions = self.predict(features_df)

        # è©•ä¾¡çµæœã‚’ä½œæˆ
        evaluation_result = {
            "predictions": predictions,
            "test_samples": len(test_data),
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "model_status": "trained" if self.is_trained else "not_trained",
        }

        return evaluation_result

    def predict(self, features_df: pd.DataFrame) -> np.ndarray:
        """
        çµ±åˆã•ã‚ŒãŸäºˆæ¸¬å®Ÿè¡Œ

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        if not self.is_trained:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        if self.trainer_type == "ensemble":
            return self._predict_ensemble(features_df)
        else:
            return self._predict_single(features_df)

    def _train_model_impl(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        çµ±åˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè£…

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        if self.trainer_type == "ensemble":
            return self._train_ensemble_model(
                X_train, X_test, y_train, y_test, **training_params
            )
        else:
            return self._train_single_model(
                X_train, X_test, y_train, y_test, **training_params
            )

    def _train_single_model(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆçµ±åˆå®Ÿè£…ï¼‰

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        try:
            logger.info(f"ğŸ¤– å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹: {self.model_type}")

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆæ—§å®Ÿè£…ã¨ã®äº’æ›æ€§ç¶­æŒï¼‰
            training_data = self._prepare_combined_training_data(
                X_train, X_test, y_train, y_test
            )

            # çµ±åˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè¡Œ
            result = self._execute_single_model_training(
                training_data, **training_params
            )

            # çµæœã®å¾Œå‡¦ç†
            self.is_trained = True

            logger.info(f"âœ… å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†: {self.model_type}")
            return result

        except Exception as e:
            logger.error(f"âŒ å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _train_ensemble_model(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆçµ±åˆå®Ÿè£…ï¼‰

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾ƒ
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        try:
            logger.info(
                f"ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’é–‹å§‹: {self.ensemble_config.get('method', 'stacking')}"
            )

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆæ—§å®Ÿè£…ã¨ã®äº’æ›æ€§ç¶­æŒï¼‰
            training_data = self._prepare_combined_training_data(
                X_train, X_test, y_train, y_test
            )

            # çµ±åˆã•ã‚ŒãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’å®Ÿè¡Œ
            result = self._execute_ensemble_model_training(
                training_data, **training_params
            )

            # çµæœã®å¾Œå‡¦ç†
            self.is_trained = True

            logger.info(
                f"âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’å®Œäº†: {self.ensemble_config.get('method', 'stacking')}"
            )
            return result

        except Exception as e:
            logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _predict_single(self, features_df: pd.DataFrame) -> np.ndarray:
        """
        å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        if self._model is None:
            raise ValueError("å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # ç‰¹å¾´é‡ã®å‰å‡¦ç†
            processed_features = self._preprocess_features_for_prediction(features_df)

            # äºˆæ¸¬å®Ÿè¡Œ
            if hasattr(self._model, "predict"):
                predictions = self._model.predict(processed_features)
            else:
                # SingleModelTrainerã®å ´åˆ
                predictions = self._model.predict(features_df)

            return predictions

        except Exception as e:
            logger.error(f"âŒ å˜ä¸€ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _predict_ensemble(self, features_df: pd.DataFrame) -> np.ndarray:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        if self._model is None:
            raise ValueError("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # EnsembleTrainerã®äºˆæ¸¬ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            predictions = self._model.predict(features_df)
            return predictions

        except Exception as e:
            logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _preprocess_features_for_prediction(
        self, features_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡å‰å‡¦ç†

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            å‰å‡¦ç†æ¸ˆã¿ç‰¹å¾´é‡
        """
        try:
            # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®é¸æŠ
            if self.feature_columns is not None:
                # å­¦ç¿’æ™‚ã®ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®ã¿ã‚’ä½¿ç”¨
                available_columns = [
                    col for col in self.feature_columns if col in features_df.columns
                ]
                processed_features = features_df[available_columns].copy()
            else:
                processed_features = features_df.copy()

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if hasattr(self, "scaler") and self.scaler is not None:
                try:
                    processed_features = pd.DataFrame(
                        self.scaler.transform(processed_features),
                        columns=processed_features.columns,
                        index=processed_features.index,
                    )
                except Exception as e:
                    logger.warning(f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")

            return processed_features

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return features_df

    def _validate_training_data(self, training_data: pd.DataFrame) -> None:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        if training_data is None or training_data.empty:
            raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

        required_columns = ["open", "high", "low", "close", "volume"]
        missing_columns = [
            col for col in required_columns if col not in training_data.columns
        ]
        if missing_columns:
            raise DataError(f"å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")

        if len(training_data) < 100:
            raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½100è¡Œå¿…è¦ï¼‰")

    def _calculate_features(
        self,
        ohlcv_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """
        ç‰¹å¾´é‡ã‚’è¨ˆç®—ï¼ˆFeatureEngineeringServiceã«å®Œå…¨å§”è­²ï¼‰

        è²¬å‹™åˆ†å‰²ã«ã‚ˆã‚Šã€å…·ä½“çš„ãªç‰¹å¾´é‡è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã¯
        FeatureEngineeringServiceã«ç§»è­²ã•ã‚Œã¾ã—ãŸã€‚

        è¨­å®šã‹ã‚‰feature profileã‚’èª­ã¿è¾¼ã¿ã€FeatureEngineeringServiceã«æ¸¡ã—ã¾ã™ã€‚
        """
        try:
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            if ohlcv_data is None or ohlcv_data.empty:
                raise ValueError("OHLCVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

            # å¿…è¦ãªåˆ—ã®å­˜åœ¨ç¢ºèª
            required_columns = ["open", "high", "low", "close", "volume"]
            missing_columns = [
                col for col in required_columns if col not in ohlcv_data.columns
            ]
            if missing_columns:
                raise ValueError(f"å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")

            # è¨­å®šã‹ã‚‰profileã‚’å–å¾—
            profile = unified_config.ml.feature_engineering.profile
            logger.info(f"ğŸ“Š ç‰¹å¾´é‡è¨ˆç®—ã‚’å®Ÿè¡Œä¸­ï¼ˆprofile: {profile}ï¼‰...")

            # åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—ï¼ˆautofeatæ©Ÿèƒ½ã¯å‰Šé™¤æ¸ˆã¿ï¼‰
            basic_features = self.feature_service.calculate_advanced_features(
                ohlcv_data=ohlcv_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                profile=profile,
            )

            # ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(basic_features.columns)}å€‹ã®ç‰¹å¾´é‡")

            # åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—å¾Œã®æ¤œè¨¼
            if basic_features is not None and not basic_features.empty:
                return basic_features
            else:
                raise ValueError("åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—ã‚‚å¤±æ•—ã—ã¾ã—ãŸ")

        except Exception as e:
            logger.warning(f"æ‹¡å¼µç‰¹å¾´é‡è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼ã€åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨: {e}")

            try:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬ç‰¹å¾´é‡ã®ã¿ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
                logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—ã‚’å®Ÿè¡Œ")
                basic_features = self.feature_service.calculate_advanced_features(
                    ohlcv_data,
                    funding_rate_data,
                    open_interest_data,
                )

                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¾Œã®æ¤œè¨¼
                if basic_features is not None and not basic_features.empty:
                    # æœ€ä½é™å¿…è¦ãªç‰¹å¾´é‡ã®æ¤œè¨¼
                    required_features = ["open", "high", "low", "close", "volume"]
                    available_features = [
                        col
                        for col in required_features
                        if col in basic_features.columns
                    ]

                    if len(available_features) >= 3:  # æœ€ä½3ã¤ã®ä¾¡æ ¼åˆ—ãŒåˆ©ç”¨å¯èƒ½
                        logger.info(
                            f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ: {len(basic_features.columns)}å€‹ã®ç‰¹å¾´é‡"
                        )
                        return basic_features
                    else:
                        raise ValueError(
                            f"åŸºæœ¬ç‰¹å¾´é‡ã‚‚ä¸è¶³ã—ã¦ã„ã¾ã™: {available_features}"
                        )
                else:
                    raise ValueError("åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—çµæœãŒç©ºã§ã™")

            except Exception as fallback_error:
                logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {fallback_error}")

                # æœ€å¾Œã®æ‰‹æ®µï¼šå…ƒãƒ‡ãƒ¼ã‚¿ã«æœ€ä½ã®ç‰¹å¾´é‡ã®ã¿è¿½åŠ 
                logger.warning("ğŸ†˜ æœ€çµ‚æ‰‹æ®µ: å…ƒãƒ‡ãƒ¼ã‚¿ã«åŸºç¤ç‰¹å¾´é‡ã®ã¿è¿½åŠ ")
                result_df = ohlcv_data.copy()

                try:
                    # æœ€ä½ä¾¡æ ¼å¤‰å‹•ç‡ç‰¹å¾´é‡ã®ã¿è¨ˆç®—
                    if "close" in result_df.columns:
                        result_df["returns"] = result_df["close"].pct_change()
                        result_df["returns"] = result_df["returns"].fillna(0.0)

                    if "volume" in result_df.columns:
                        result_df["volume_change"] = result_df["volume"].pct_change()
                        result_df["volume_change"] = result_df["volume_change"].fillna(
                            0.0
                        )

                    logger.info(f"âœ… æœ€çµ‚æ‰‹æ®µæˆåŠŸ: {len(result_df.columns)}å€‹ã®åŸºæœ¬åˆ—")
                    return result_df

                except Exception as final_error:
                    logger.error(f"æœ€çµ‚æ‰‹æ®µã‚‚å¤±æ•—: {final_error}")
                    error_msg = (
                        f"ç‰¹å¾´é‡è¨ˆç®—ã«å®Œå…¨ã«å¤±æ•—ã—ã¾ã—ãŸ: "
                        f"{str(e)} -> {str(fallback_error)} -> {str(final_error)}"
                    )
                    raise DataError(error_msg)

    def _prepare_training_data(
        self, features_df: pd.DataFrame, **training_params
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """
        å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™

        æ–°ã—ã„ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‚’ä½¿ç”¨ã—ã€ãƒ—ãƒªã‚»ãƒƒãƒˆã¾ãŸã¯ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        æ—¢å­˜ã®target_columnãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
        æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

        Args:
            features_df: ç‰¹å¾´é‡DataFrameï¼ˆOHLCVãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                - target_column: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ åï¼ˆå¾Œæ–¹äº’æ›æ€§ç”¨ï¼‰
                - ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯data_preprocessorã«æ¸¡ã•ã‚Œã‚‹

        Returns:
            Tuple[pd.DataFrame, pd.Series]: ã‚¯ãƒªãƒ¼ãƒ³ãªç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®ã‚¿ãƒ—ãƒ«

        Raises:
            DataError: ãƒ©ãƒ™ãƒ«ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆ
            ValueError: ãƒ—ãƒªã‚»ãƒƒãƒˆåãŒå­˜åœ¨ã—ãªã„å ´åˆ

        Note:
            - ãƒ—ãƒªã‚»ãƒƒãƒˆä½¿ç”¨æ™‚: unified_config.ml.training.label_generation.use_preset=True
            - ã‚«ã‚¹ã‚¿ãƒ è¨­å®šä½¿ç”¨æ™‚: use_preset=Falseï¼ˆå€‹åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
            - å¾Œæ–¹äº’æ›æ€§: target_columnãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
        """
        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‚’å–å¾—
        label_config = unified_config.ml.training.label_generation

        # target_columnãŒæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        target_column = training_params.get("target_column")
        if target_column is not None and target_column in features_df.columns:
            logger.info(f"ğŸ“Œ å¾Œæ–¹äº’æ›æ€§ãƒ¢ãƒ¼ãƒ‰: target_column='{target_column}' ã‚’ä½¿ç”¨")

            # æ—¢å­˜ã®LabelGeneratorã‚’ä½¿ç”¨
            from ...utils.label_generation import LabelGenerator

            label_generator = LabelGenerator()

            # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’å§”è­²
            features_clean, labels_clean, threshold_info = (
                data_preprocessor.prepare_training_data(
                    features_df, label_generator, **training_params
                )
            )

            # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’ä¿å­˜
            self.feature_columns = features_clean.columns.tolist()

            return features_clean, labels_clean

        # æ–°ã—ã„ãƒ—ãƒªã‚»ãƒƒãƒˆ/ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’ä½¿ç”¨
        try:
            logger.info("ğŸ¯ æ–°ã—ã„ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‚’ä½¿ç”¨")

            # ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
            if label_config.use_preset:
                # ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
                try:
                    labels, preset_info = apply_preset_by_name(
                        features_df, label_config.default_preset
                    )
                    logger.info(f"âœ… ãƒ—ãƒªã‚»ãƒƒãƒˆä½¿ç”¨: {label_config.default_preset}")
                    logger.info(f"   è¨­å®š: {preset_info.get('description', 'N/A')}")
                except ValueError:
                    # ãƒ—ãƒªã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
                    available_presets = list(get_common_presets().keys())
                    logger.error(
                        f"âŒ ãƒ—ãƒªã‚»ãƒƒãƒˆ '{label_config.default_preset}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                        f"åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒªã‚»ãƒƒãƒˆ: {', '.join(sorted(available_presets[:5]))}..."
                    )
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’ä½¿ç”¨
                    logger.warning("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
                    labels = forward_classification_preset(
                        df=features_df,
                        timeframe=label_config.timeframe,
                        horizon_n=label_config.horizon_n,
                        threshold=label_config.threshold,
                        price_column=label_config.price_column,
                        threshold_method=label_config.get_threshold_method_enum(),
                    )
            else:
                # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’ä½¿ç”¨
                logger.info("ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‚’ä½¿ç”¨")
                labels = forward_classification_preset(
                    df=features_df,
                    timeframe=label_config.timeframe,
                    horizon_n=label_config.horizon_n,
                    threshold=label_config.threshold,
                    price_column=label_config.price_column,
                    threshold_method=label_config.get_threshold_method_enum(),
                )
                logger.info(
                    f"   è¨­å®š: {label_config.timeframe}, "
                    f"horizon={label_config.horizon_n}, "
                    f"threshold={label_config.threshold}, "
                    f"method={label_config.threshold_method}"
                )

            # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã‚’ãƒ­ã‚°å‡ºåŠ›
            label_counts = labels.value_counts()
            total_labels = len(labels.dropna())
            if total_labels > 0:
                logger.info("ğŸ“Š ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
                for label_value in ["UP", "RANGE", "DOWN"]:
                    if label_value in label_counts.index:
                        count = label_counts[label_value]
                        pct = (count / total_labels) * 100
                        logger.info(f"   {label_value}: {count}å€‹ ({pct:.1f}%)")

            # NaNã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            valid_idx = labels.notna()
            features_clean = features_df[valid_idx].copy()
            labels_clean = labels[valid_idx].copy()

            # æ–‡å­—åˆ—ãƒ©ãƒ™ãƒ«ã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
            # "DOWN" -> 0, "RANGE" -> 1, "UP" -> 2
            label_mapping = {"DOWN": 0, "RANGE": 1, "UP": 2}
            labels_numeric = labels_clean.map(label_mapping)

            # æ¬ æå€¤ãŒãªã„ã“ã¨ã‚’ç¢ºèª
            if labels_numeric.isna().any():
                logger.warning(
                    f"âš ï¸ {labels_numeric.isna().sum()}å€‹ã®ä¸æ˜ãªãƒ©ãƒ™ãƒ«ã‚’é™¤å¤–ã—ã¾ã™"
                )
                valid_numeric_idx = labels_numeric.notna()
                features_clean = features_clean[valid_numeric_idx]
                labels_numeric = labels_numeric[valid_numeric_idx]

            # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’ä¿å­˜
            self.feature_columns = features_clean.columns.tolist()

            logger.info(
                f"âœ… ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†: {len(features_clean)}ã‚µãƒ³ãƒ—ãƒ« "
                f"({len(features_df) - len(features_clean)}å€‹ã‚’é™¤å¤–)"
            )

            return features_clean, labels_numeric

        except Exception as e:
            logger.error(f"âŒ æ–°ã—ã„ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            logger.warning("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢å­˜ã®LabelGeneratorã‚’ä½¿ç”¨ã—ã¾ã™")

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢å­˜ã®LabelGeneratorã‚’ä½¿ç”¨
            try:
                from ...utils.label_generation import LabelGenerator

                label_generator = LabelGenerator()

                # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’å§”è­²
                features_clean, labels_clean, threshold_info = (
                    data_preprocessor.prepare_training_data(
                        features_df, label_generator, **training_params
                    )
                )

                # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’ä¿å­˜
                self.feature_columns = features_clean.columns.tolist()

                logger.info(
                    "âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ: æ—¢å­˜ã®LabelGeneratorã§ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†"
                )
                return features_clean, labels_clean

            except Exception as fallback_error:
                logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {fallback_error}")
                raise DataError(
                    f"ãƒ©ãƒ™ãƒ«ç”Ÿæˆã«å®Œå…¨ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)} -> {str(fallback_error)}"
                )

    def _split_data(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ™‚ç³»åˆ—åˆ†å‰²ï¼‰

        Args:
            X: ç‰¹å¾´é‡DataFrame
            y: ãƒ©ãƒ™ãƒ«Series
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                - use_time_series_split: æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è¨­å®šå€¤ã¾ãŸã¯Trueï¼‰
                - use_random_split: ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ã‚’ä½¿ç”¨ï¼ˆä¸‹ä½äº’æ›æ€§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Falseï¼‰
                - test_size: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.2ï¼‰
                - random_state: ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 42ï¼‰

        Returns:
            åˆ†å‰²ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ (X_train, X_test, y_train, y_test)

        Note:
            æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€å°†æ¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ã“ã¨ã‚’é˜²ããŸã‚ã€
            æ™‚é–“é †åºã‚’ä¿æŒã—ãŸåˆ†å‰²ã‚’è¡Œã„ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§TimeSeriesSplitã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
        """
        test_size = training_params.get("test_size", 0.2)
        random_state = training_params.get("random_state", 42)

        # ä¸‹ä½äº’æ›æ€§: use_random_split=TrueãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²
        use_random_split = training_params.get("use_random_split", False)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨ï¼ˆè¨­å®šã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        use_time_series_split = training_params.get(
            "use_time_series_split",
            (
                self.config.training.USE_TIME_SERIES_SPLIT
                if not use_random_split
                else False
            ),
        )

        if use_time_series_split and not use_random_split:
            # æ™‚ç³»åˆ—åˆ†å‰²ï¼šæ™‚é–“é †åºã‚’ä¿æŒã—ã¦åˆ†å‰²
            logger.info("ğŸ•’ æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰")

            # ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’å–å¾—
            n_samples = len(X)
            train_size = int(n_samples * (1 - test_size))

            # æ™‚é–“é †åºã‚’ä¿æŒã—ã¦åˆ†å‰²
            X_train = X.iloc[:train_size].copy()
            X_test = X.iloc[train_size:].copy()
            y_train = y.iloc[:train_size].copy()
            y_test = y.iloc[train_size:].copy()

            logger.info(
                f"æ™‚ç³»åˆ—åˆ†å‰²çµæœ: å­¦ç¿’={len(X_train)}ã‚µãƒ³ãƒ—ãƒ«, ãƒ†ã‚¹ãƒˆ={len(X_test)}ã‚µãƒ³ãƒ—ãƒ«"
            )
            logger.info(f"å­¦ç¿’æœŸé–“: {X_train.index[0]} ï½ {X_train.index[-1]}")
            logger.info(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {X_test.index[0]} ï½ {X_test.index[-1]}")

        else:
            # ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆä¸‹ä½äº’æ›æ€§ç¶­æŒï¼‰
            logger.info(
                "ğŸ”€ ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ã‚’ä½¿ç”¨ï¼ˆuse_random_split=True ã¾ãŸã¯ use_time_series_split=Falseï¼‰"
            )

            # å±¤åŒ–æŠ½å‡ºã¯ã€ãƒ©ãƒ™ãƒ«ãŒ2ç¨®é¡ä»¥ä¸Šã‚ã‚‹å ´åˆã«ã®ã¿æœ‰åŠ¹
            stratify_param = y if y.nunique() > 1 else None
            if stratify_param is None:
                logger.warning(
                    "ãƒ©ãƒ™ãƒ«ãŒ1ç¨®é¡ä»¥ä¸‹ã®ãŸã‚ã€å±¤åŒ–æŠ½å‡ºãªã—ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¾ã™ã€‚"
                )

            # train_test_splitã¯ãƒªã‚¹ãƒˆã‚’è¿”ã™ãŸã‚ã€ä¸€åº¦å¤‰æ•°ã«å—ã‘ã¦ã‹ã‚‰ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹
            splits = train_test_split(
                X,
                y,
                test_size=test_size,
                random_state=random_state,
                stratify=stratify_param,
            )

            # å‹ãƒã‚§ãƒƒã‚«ãƒ¼ã®ãŸã‚ã«æ˜ç¤ºçš„ã«ã‚­ãƒ£ã‚¹ãƒˆ
            X_train = cast(pd.DataFrame, splits[0])
            X_test = cast(pd.DataFrame, splits[1])
            y_train = cast(pd.Series, splits[2])
            y_test = cast(pd.Series, splits[3])

        # åˆ†å‰²å¾Œã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã‚’ç¢ºèª
        logger.info("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
        for label_value in sorted(y_train.unique()):
            count = (y_train == label_value).sum()
            percentage = count / len(y_train) * 100
            logger.info(f"  ãƒ©ãƒ™ãƒ« {label_value}: {count}ã‚µãƒ³ãƒ—ãƒ« ({percentage:.1f}%)")

        logger.info("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
        for label_value in sorted(y_test.unique()):
            count = (y_test == label_value).sum()
            percentage = count / len(y_test) * 100
            logger.info(f"  ãƒ©ãƒ™ãƒ« {label_value}: {count}ã‚µãƒ³ãƒ—ãƒ« ({percentage:.1f}%)")

        return X_train, X_test, y_train, y_test

    def _time_series_cross_validate(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Dict[str, Any]:
        """
        æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

        ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æ¤œè¨¼ã‚’è¡Œã„ã€ã‚ˆã‚Šå …ç‰¢ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚’æä¾›ã—ã¾ã™ã€‚

        Args:
            X: ç‰¹å¾´é‡DataFrame
            y: ãƒ©ãƒ™ãƒ«Series
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                - cv_splits: åˆ†å‰²æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ml_config.training.CROSS_VALIDATION_FOLDSï¼‰
                - max_train_size: æœ€å¤§å­¦ç¿’ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ml_config.training.MAX_TRAIN_SIZEï¼‰

        Returns:
            ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®è¾æ›¸
        """
        # ml_configã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’èª­ã¿è¾¼ã¿
        n_splits = training_params.get(
            "cv_splits", self.config.training.CROSS_VALIDATION_FOLDS
        )
        max_train_size = training_params.get(
            "max_train_size", self.config.training.MAX_TRAIN_SIZE
        )

        logger.info(f"ğŸ”„ æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ï¼ˆ{n_splits}åˆ†å‰²ï¼‰")

        # TimeSeriesSplitã‚’åˆæœŸåŒ–
        tscv = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size)

        cv_scores = []
        fold_results = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
            logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold}/{n_splits} ã‚’å®Ÿè¡Œä¸­...")

            # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
            X_train_cv = X.iloc[train_idx]
            X_test_cv = X.iloc[test_idx]
            y_train_cv = y.iloc[train_idx]
            y_test_cv = y.iloc[test_idx]

            # ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†
            X_train_scaled, X_test_scaled = self._preprocess_data(X_train_cv, X_test_cv)

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’ã‚’å®Ÿè¡Œ
            fold_result = self._train_fold_with_error_handling(
                fold,
                X_train_scaled,
                X_test_scaled,
                y_train_cv,
                y_test_cv,
                X_train_cv,
                X_test_cv,
                training_params,
            )

            cv_scores.append(fold_result.get("accuracy", 0.0))
            fold_results.append(fold_result)

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’é›†è¨ˆ
        cv_result = {
            "cv_scores": cv_scores,
            "cv_mean": np.mean(cv_scores),
            "cv_std": np.std(cv_scores),
            "cv_min": np.min(cv_scores),
            "cv_max": np.max(cv_scores),
            "fold_results": fold_results,
            "n_splits": n_splits,
        }

        logger.info("æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†:")
        logger.info(
            f"  å¹³å‡ç²¾åº¦: {cv_result['cv_mean']:.4f} Â± {cv_result['cv_std']:.4f}"
        )
        logger.info(f"  æœ€å°ç²¾åº¦: {cv_result['cv_min']:.4f}")
        logger.info(f"  æœ€å¤§ç²¾åº¦: {cv_result['cv_max']:.4f}")

        return cv_result

    def _preprocess_data(
        self, X_train: pd.DataFrame, X_test: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰"""
        # LightGBMãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦
        if hasattr(self, "model_type") and "LightGBM" in str(self.model_type):
            return X_train, X_test

        # ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ
        assert self.scaler is not None, "ScalerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        X_train_scaled = pd.DataFrame(
            self.scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index,
        )

        X_test_scaled = pd.DataFrame(
            self.scaler.transform(X_test), columns=X_test.columns, index=X_test.index
        )

        return X_train_scaled, X_test_scaled

    def save_model(
        self, model_name: str, metadata: Optional[Dict[str, Any]] = None
    ) -> Optional[str]:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        if not self.is_trained:
            raise MLModelError("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã¡ã‚‰ã«å§”è­²
        if hasattr(self, "_ensemble_trainer") and self._ensemble_trainer:
            logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã«ä¿å­˜ã‚’å§”è­²ã—ã¾ã™")
            return self._ensemble_trainer.save_model(model_name, metadata)

        # åŸºæœ¬çš„ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        final_metadata = {
            "model_type": self.__class__.__name__,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "is_trained": self.is_trained,
        }
        # æä¾›ã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°
        if metadata:
            final_metadata.update(metadata)

        # ç‰¹å¾´é‡é‡è¦åº¦ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        try:
            feature_importance = self.get_feature_importance(top_n=100)
            if feature_importance:
                final_metadata["feature_importance"] = feature_importance
                logger.info(
                    f"ç‰¹å¾´é‡é‡è¦åº¦ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ : {len(feature_importance)}å€‹"
                )
        except Exception as e:
            logger.warning(f"ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—ã«å¤±æ•—: {e}")

        # çµ±ä¸€ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚’ä½¿ç”¨
        model_path = model_manager.save_model(
            model=self,  # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼å…¨ä½“ã‚’ä¿å­˜
            model_name=model_name,
            metadata=final_metadata,
            scaler=self.scaler,
            feature_columns=self.feature_columns,
        )

        logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")
        return model_path

    def _format_training_result(
        self, training_result: Dict[str, Any], X: pd.DataFrame, y: pd.Series
    ) -> Dict[str, Any]:
        """å­¦ç¿’çµæœã‚’æ•´å½¢"""
        result = {
            "success": True,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "total_samples": len(X),
            **training_result,
        }

        return result

    def get_model_info(self) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—

        Returns:
            ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¾æ›¸
        """
        return {
            "model_type": self.__class__.__name__,
            "is_trained": self.is_trained,
            "trainer_type": getattr(self, "trainer_type", "unknown"),
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
        }

    def get_feature_importance(self, top_n: int = 10) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—

        Args:
            top_n: ä¸Šä½Nå€‹ã®ç‰¹å¾´é‡

        Returns:
            ç‰¹å¾´é‡é‡è¦åº¦ã®è¾æ›¸
        """
        if not self.is_trained:
            logger.warning("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã¡ã‚‰ã«å§”è­²
        if hasattr(self, "_ensemble_trainer") and self._ensemble_trainer:
            if hasattr(self._ensemble_trainer, "get_feature_importance"):
                try:
                    feature_importance = self._ensemble_trainer.get_feature_importance()
                    if feature_importance:
                        # ä¸Šä½Nå€‹ã‚’å–å¾—
                        sorted_importance = sorted(
                            feature_importance.items(), key=lambda x: x[1], reverse=True
                        )[:top_n]
                        logger.info(
                            f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‹ã‚‰ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—: {len(sorted_importance)}å€‹"
                        )
                        return dict(sorted_importance)
                except Exception as e:
                    logger.error(
                        f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‹ã‚‰ã®ç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"
                    )

        # ãƒ¢ãƒ‡ãƒ«ãŒç‰¹å¾´é‡é‡è¦åº¦ã‚’æä¾›ã™ã‚‹å ´åˆ
        if hasattr(self._model, "get_feature_importance"):
            try:
                feature_importance = self._model.get_feature_importance(top_n)
                if feature_importance:
                    logger.info(
                        f"ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—: {len(feature_importance)}å€‹"
                    )
                    return feature_importance
            except Exception as e:
                logger.error(f"ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®ç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        # LightGBMãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
        if hasattr(self._model, "feature_importance") and self.feature_columns:
            try:
                importance_scores = self._model.feature_importance(
                    importance_type="gain"
                )
                feature_importance = dict(zip(self.feature_columns, importance_scores))

                # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½Nå€‹ã‚’å–å¾—
                sorted_importance = sorted(
                    feature_importance.items(), key=lambda x: x[1], reverse=True
                )[:top_n]

                logger.info(
                    f"LightGBMã‹ã‚‰ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—: {len(sorted_importance)}å€‹"
                )
                return dict(sorted_importance)
            except Exception as e:
                logger.error(f"LightGBMç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                return {}

        logger.warning("ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç‰¹å¾´é‡é‡è¦åº¦ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“")
        return {}

    @safe_ml_operation(
        default_return=False, context="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )
    def load_model(self, model_path: str) -> bool:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿

        Args:
            model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            èª­ã¿è¾¼ã¿æˆåŠŸãƒ•ãƒ©ã‚°
        """
        model_data = model_manager.load_model(model_path)

        if model_data is None:
            return False

        # ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„è¦ç´ ã‚’å–å¾—
        self._model = model_data.get("model")
        self.scaler = model_data.get("scaler")
        self.feature_columns = model_data.get("feature_columns")

        if self._model is None:
            raise MLModelError("ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        self.is_trained = True
        return True

    def _cleanup_temporary_files(self, level: CleanupLevel):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # BaseMLTrainerã§ã¯ç‰¹ã«ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œæˆã—ãªã„ãŸã‚ã€ãƒ‘ã‚¹
        pass

    def _cleanup_cache(self, level: CleanupLevel):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            # AutoMLæ©Ÿèƒ½ã¯å‰Šé™¤æ¸ˆã¿ã®ãŸã‚ã€ç‰¹ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ãªã—
            if self.feature_service is not None:
                logger.debug("ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}")

    def _cleanup_models(self, level: CleanupLevel):
        """ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.feature_service is not None:
                if hasattr(self.feature_service, "cleanup_resources"):
                    self.feature_service.cleanup_resources()
                    logger.debug("ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")

            # ãƒ¢ãƒ‡ãƒ«ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
            self._model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False

        except Exception as e:
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¯ç¶šè¡Œ
            self._model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False

    @safe_ml_operation(
        default_return={
            "fold": 0,
            "error": "ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            "accuracy": 0.0,
        },
        context="ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’",
    )
    def _train_fold_with_error_handling(
        self,
        fold: int,
        X_train_scaled: pd.DataFrame,
        X_test_scaled: pd.DataFrame,
        y_train_cv: pd.Series,
        y_test_cv: pd.Series,
        X_train_cv: pd.DataFrame,
        X_test_cv: pd.DataFrame,
        training_params: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ããƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’

        Args:
            fold: ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ç•ªå·
            X_train_scaled: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¸ˆã¿å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test_scaled: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train_cv: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test_cv: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            X_train_cv: å…ƒã®å­¦ç¿’ç”¨ç‰¹å¾´é‡ï¼ˆæœŸé–“æƒ…å ±ç”¨ï¼‰
            X_test_cv: å…ƒã®ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡ï¼ˆæœŸé–“æƒ…å ±ç”¨ï¼‰
            training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’çµæœã®è¾æ›¸
        """
        # ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        fold_result = self._train_model_impl(
            X_train_scaled,
            X_test_scaled,
            y_train_cv,
            y_test_cv,
            **training_params,
        )

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’è¿½åŠ 
        fold_result.update(
            {
                "fold": fold,
                "train_samples": len(X_train_cv),
                "test_samples": len(X_test_cv),
                "train_period": f"{X_train_cv.index[0]} ï½ {X_train_cv.index[-1]}",
                "test_period": f"{X_test_cv.index[0]} ï½ {X_test_cv.index[-1]}",
            }
        )

        logger.info(
            f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold} å®Œäº†: ç²¾åº¦={fold_result.get('accuracy', 0.0):.4f}"
        )

        return fold_result

    def _prepare_combined_training_data(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
    ) -> pd.DataFrame:
        """
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆæº–å‚™ï¼ˆæ—§å®Ÿè£…ã¨ã®äº’æ›æ€§ç¶­æŒï¼‰

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«

        Returns:
            çµ±åˆã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        """
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆæ—§å®Ÿè£…ã¨ã®äº’æ›æ€§ç¶­æŒï¼‰
        X_combined = pd.concat([X_train, X_test])
        y_combined = pd.concat([y_train, y_test])
        training_data = X_combined.copy()
        training_data["target"] = y_combined
        return training_data

    def _execute_single_model_training(
        self, training_data: pd.DataFrame, **training_params
    ) -> Dict[str, Any]:
        """
        å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œ

        Args:
            training_data: çµ±åˆæ¸ˆã¿å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        # æ—§å®Ÿè£…ã¨ã®äº’æ›æ€§ã‚’ç¶­æŒã—ã¤ã¤ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
        from .single_model.single_model_trainer import SingleModelTrainer

        trainer = SingleModelTrainer(model_type=self.model_type)

        result = trainer.train_model(training_data, **training_params)

        # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        self._model = trainer.model

        return result

    def _execute_ensemble_model_training(
        self, training_data: pd.DataFrame, **training_params
    ) -> Dict[str, Any]:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œ

        Args:
            training_data: çµ±åˆæ¸ˆã¿å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        # æ—§å®Ÿè£…ã¨ã®äº’æ›æ€§ã‚’ç¶­æŒã—ã¤ã¤ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
        from .ensemble.ensemble_trainer import EnsembleTrainer

        trainer = EnsembleTrainer(ensemble_config=self.ensemble_config)

        result = trainer.train_model(training_data, **training_params)

        # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        self.models = trainer.models
        self._model = trainer  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è‡ªä½“ã‚’ä¿å­˜
        self._ensemble_trainer = trainer  # å‚ç…§ã‚’ä¿æŒ

        return result
