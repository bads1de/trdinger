"""
MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»å‰å‡¦ç†ãƒ»ä¿å­˜ã«é–¢ã‚ã‚‹å…±é€šãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã™ã‚‹æŠ½è±¡åŸºç›¤ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
å…·ä½“çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„æœ€é©åŒ–æ‰‹æ³•ã®è©³ç´°èª¬æ˜ã¯Docstringã«å«ã‚ã¾ã›ã‚“ã€‚
ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ãŒãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®å­¦ç¿’å‡¦ç†ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
"""

import logging
from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, Tuple, cast

import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.preprocessing import StandardScaler

from ...config.unified_config import unified_config
from ...utils.data_processing import data_processor as data_preprocessor
from ...utils.error_handler import (
    DataError,
    ml_operation_context,
    safe_ml_operation,
)
from ...utils.label_generation.presets import (
    apply_preset_by_name,
    forward_classification_preset,
    get_common_presets,
)
from ...utils.cross_validation import PurgedKFold

from .data_processing.sampling import ImbalanceSampler
from .common.base_resource_manager import BaseResourceManager, CleanupLevel
from .config import ml_config
from .exceptions import MLModelError
from .feature_engineering.feature_engineering_service import FeatureEngineeringService
from .ml_metadata import ModelMetadata
from .model_manager import model_manager

logger = logging.getLogger(__name__)


class BaseMLTrainer(BaseResourceManager, ABC):
    """
    MLå­¦ç¿’åŸºç›¤ã‚¯ãƒ©ã‚¹

    å…±é€šã®å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã—ã€å…·ä½“çš„ãªå®Ÿè£…ã¯ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§è¡Œã„ã¾ã™ã€‚
    å˜ä¸€è²¬ä»»åŸå‰‡ã«å¾“ã„ã€å­¦ç¿’ã«é–¢ã™ã‚‹è²¬ä»»ã®ã¿ã‚’æŒã¡ã¾ã™ã€‚
    """

    def __init__(
        self,
        trainer_config: Optional[Dict[str, Any]] = None,
        trainer_type: Optional[str] = None,
        model_type: Optional[str] = None,
    ):
        """
        åˆæœŸåŒ–

        Args:
            trainer_config: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®š
            trainer_type: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆäº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰
            model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆäº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰
        """
        # BaseResourceManagerã®åˆæœŸåŒ–
        super().__init__()

        self.config = ml_config

        self.feature_service = FeatureEngineeringService()
        logger.debug("ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

        self.trainer_config = trainer_config or {}
        
        # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®è¨­å®šï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ç¶­æŒï¼‰
        self.trainer_type = trainer_type or self.trainer_config.get("type", "single")
        self.model_type = model_type or self.trainer_config.get("model_type", "lightgbm")
        self.ensemble_config = self.trainer_config.get("ensemble_config", {})

        self.scaler = StandardScaler()
        self.feature_columns = None
        self.is_trained = False
        self._model = None
        self.last_training_results = None

    @safe_ml_operation(default_return={}, context="MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    def train_model(
        self,
        training_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
        save_model: bool = True,
        model_name: Optional[str] = None,
        **training_params,
    ) -> Dict[str, Any]:
        """
        MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

        Args:
            training_data: å­¦ç¿’ç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            save_model: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            **training_params: è¿½åŠ ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸
        """
        with ml_operation_context("MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’"):
            # 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            self._validate_training_data(training_data)

            # 2. ç‰¹å¾´é‡ã‚’è¨ˆç®—
            features_df = self._calculate_features(
                training_data, funding_rate_data, open_interest_data
            )

            # 3. å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            X, y = self._prepare_training_data(features_df, **training_params)

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if X is None or X.empty or y is None or y.empty:
                raise DataError("å‰å‡¦ç†å¾Œã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

            # 4. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            use_cross_validation = training_params.get("use_cross_validation", False)

            if use_cross_validation:
                # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
                cv_result = self._time_series_cross_validate(X, y, **training_params)

                # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
                logger.info("ğŸ¯ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ä¸­...")
                X_scaled = self._preprocess_data(X, X)[0]  # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

                # ãƒ€ãƒŸãƒ¼ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€å¾Œã®20%ï¼‰ã‚’ä½œæˆ
                test_size = training_params.get("test_size", 0.2)
                n_samples = len(X)
                train_size = int(n_samples * (1 - test_size))

                X_train_final = X_scaled.iloc[:train_size]
                X_test_final = X_scaled.iloc[train_size:]
                y_train_final = y.iloc[:train_size]
                y_test_final = y.iloc[train_size:]

                training_result = self._train_model_impl(
                    X_train_final,
                    X_test_final,
                    y_train_final,
                    y_test_final,
                    **training_params,
                )

                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¿½åŠ 
                training_result.update(cv_result)

            else:
                # é€šå¸¸ã®å˜ä¸€åˆ†å‰²å­¦ç¿’
                # 4. ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
                X_train, X_test, y_train, y_test = self._split_data(
                    X, y, **training_params
                )

                # 5. ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†
                X_train_scaled, X_test_scaled = self._preprocess_data(X_train, X_test)

                # 6. ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
                training_result = self._train_model_impl(
                    X_train_scaled, X_test_scaled, y_train, y_test, **training_params
                )

            # 7. å­¦ç¿’å®Œäº†ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
            self.is_trained = True

            # 8. ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            should_save_model = bool(save_model) if save_model is not None else True
            if should_save_model:
                model_metadata = ModelMetadata.from_training_result(
                    training_result=training_result,
                    training_params=training_params,
                    model_type=self.__class__.__name__,
                    feature_count=(
                        len(self.feature_columns) if self.feature_columns else 0
                    ),
                )

                model_metadata.log_summary()
                
                validation_result = model_metadata.validate()
                if not validation_result["is_valid"]:
                    logger.warning(f"ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å•é¡Œ: {validation_result['errors']}")

                model_path = self.save_model(
                    model_name or self.config.model.AUTO_STRATEGY_MODEL_NAME,
                    model_metadata.to_dict(),
                )
                training_result["model_path"] = model_path

            # 9. å­¦ç¿’çµæœã‚’æ•´å½¢
            result = self._format_training_result(training_result, X, y)

            logger.info("MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
            return result

    @safe_ml_operation(default_return={}, context="ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    def evaluate_model(
        self,
        test_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> Dict[str, Any]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

        Args:
            test_data: ãƒ†ã‚¹ãƒˆç”¨OHLCVãƒ‡ãƒ¼ã‚¿
            funding_rate_data: ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            open_interest_data: å»ºç‰æ®‹é«˜ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        if not self.is_trained:
            raise MLModelError("è©•ä¾¡å¯¾è±¡ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        # ç‰¹å¾´é‡ã‚’è¨ˆç®—
        features_df = self._calculate_features(
            test_data, funding_rate_data, open_interest_data
        )

        # äºˆæ¸¬ã‚’å®Ÿè¡Œ
        predictions = self.predict(features_df)

        # è©•ä¾¡çµæœã‚’ä½œæˆ
        evaluation_result = {
            "predictions": predictions,
            "test_samples": len(test_data),
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "model_status": "trained" if self.is_trained else "not_trained",
        }

        return evaluation_result

    @abstractmethod
    def predict(self, features_df: pd.DataFrame) -> np.ndarray:
        """
        äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        pass

    @abstractmethod
    def _train_model_impl(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè£…ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ãƒ©ãƒ™ãƒ«
            y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
            **training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å­¦ç¿’çµæœ
        """
        pass

    def _preprocess_features_for_prediction(
        self, features_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡å‰å‡¦ç†

        Args:
            features_df: ç‰¹å¾´é‡DataFrame

        Returns:
            å‰å‡¦ç†æ¸ˆã¿ç‰¹å¾´é‡
        """
        try:
            # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®é¸æŠ
            if self.feature_columns is not None:
                available_columns = [
                    col for col in self.feature_columns if col in features_df.columns
                ]
                processed_features = features_df[available_columns].copy()
            else:
                processed_features = features_df.copy()

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            if hasattr(self, "scaler") and self.scaler is not None:
                try:
                    processed_features = pd.DataFrame(
                        self.scaler.transform(processed_features),
                        columns=processed_features.columns,
                        index=processed_features.index,
                    )
                except Exception as e:
                    logger.warning(f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")

            return processed_features

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return features_df

    def _validate_training_data(self, training_data: pd.DataFrame) -> None:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        if training_data is None or training_data.empty:
            raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

        required_columns = ["open", "high", "low", "close", "volume"]
        missing_columns = [
            col for col in required_columns if col not in training_data.columns
        ]
        if missing_columns:
            raise DataError(f"å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")

        if len(training_data) < 100:
            raise DataError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½100è¡Œå¿…è¦ï¼‰")

    def _calculate_features(
        self,
        ohlcv_data: pd.DataFrame,
        funding_rate_data: Optional[pd.DataFrame] = None,
        open_interest_data: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        """ç‰¹å¾´é‡ã‚’è¨ˆç®—"""
        try:
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            if ohlcv_data is None or ohlcv_data.empty:
                raise ValueError("OHLCVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

            # è¨­å®šã‹ã‚‰profileã‚’å–å¾—
            profile = unified_config.ml.feature_engineering.profile
            logger.info(f"ğŸ“Š ç‰¹å¾´é‡è¨ˆç®—ã‚’å®Ÿè¡Œä¸­ï¼ˆprofile: {profile}ï¼‰...")

            # åŸºæœ¬ç‰¹å¾´é‡è¨ˆç®—
            basic_features = self.feature_service.calculate_advanced_features(
                ohlcv_data=ohlcv_data,
                funding_rate_data=funding_rate_data,
                open_interest_data=open_interest_data,
                profile=profile,
            )

            logger.info(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(basic_features.columns)}å€‹ã®ç‰¹å¾´é‡")
            return basic_features

        except Exception as e:
            logger.warning(f"ç‰¹å¾´é‡è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼ã€åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆç°¡ç•¥åŒ–ï¼‰
            return ohlcv_data.copy()

    def _prepare_training_data(
        self, features_df: pd.DataFrame, **training_params
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‚’å–å¾—
        label_config = unified_config.ml.training.label_generation

        # target_columnãŒæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        target_column = training_params.get("target_column")
        if target_column is not None and target_column in features_df.columns:
            logger.info(f"ğŸ“Œ å¾Œæ–¹äº’æ›æ€§ãƒ¢ãƒ¼ãƒ‰: target_column='{target_column}' ã‚’ä½¿ç”¨")
            
            from ...utils.label_generation import LabelGenerator
            label_generator = LabelGenerator()

            features_clean, labels_clean, threshold_info = (
                data_preprocessor.prepare_training_data(
                    features_df, label_generator, **training_params
                )
            )
            
            self.feature_columns = features_clean.columns.tolist()
            return features_clean, labels_clean

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒªã‚»ãƒƒãƒˆ/ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãƒ­ã‚¸ãƒƒã‚¯
        try:
            logger.info("ğŸ¯ æ–°ã—ã„ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‚’ä½¿ç”¨")
            
            if label_config.use_preset:
                try:
                    labels, preset_info = apply_preset_by_name(
                        features_df, label_config.default_preset
                    )
                except ValueError:
                    logger.warning("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
                    labels = forward_classification_preset(
                        df=features_df,
                        timeframe=label_config.timeframe,
                        horizon_n=label_config.horizon_n,
                        threshold=label_config.threshold,
                        price_column=label_config.price_column,
                        threshold_method=label_config.get_threshold_method_enum(),
                    )
            else:
                labels = forward_classification_preset(
                    df=features_df,
                    timeframe=label_config.timeframe,
                    horizon_n=label_config.horizon_n,
                    threshold=label_config.threshold,
                    price_column=label_config.price_column,
                    threshold_method=label_config.get_threshold_method_enum(),
                )

            # NaNã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            valid_idx = labels.notna()
            features_clean = features_df[valid_idx].copy()
            labels_clean = labels[valid_idx].copy()

            # æ–‡å­—åˆ—ãƒ©ãƒ™ãƒ«ã‚’æ•°å€¤ã«å¤‰æ›
            unique_labels = set(labels_clean.unique())
            if "TREND" in unique_labels:
                label_mapping = {"RANGE": 0, "TREND": 1}
            else:
                label_mapping = {"DOWN": 0, "RANGE": 1, "UP": 2}

            labels_numeric = labels_clean.map(label_mapping)

            if labels_numeric.isna().any():
                valid_numeric_idx = labels_numeric.notna()
                features_clean = features_clean[valid_numeric_idx]
                labels_numeric = labels_numeric[valid_numeric_idx]

            self.feature_columns = features_clean.columns.tolist()
            logger.info(f"âœ… ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†: {len(features_clean)}ã‚µãƒ³ãƒ—ãƒ«")

            return features_clean, labels_numeric

        except Exception as e:
            logger.error(f"âŒ æ–°ã—ã„ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆçœç•¥ï¼‰
            raise DataError(f"ãƒ©ãƒ™ãƒ«ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def _split_data(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²"""
        test_size = training_params.get("test_size", 0.2)
        random_state = training_params.get("random_state", 42)
        use_random_split = training_params.get("use_random_split", False)
        use_time_series_split = training_params.get(
            "use_time_series_split",
            (self.config.training.USE_TIME_SERIES_SPLIT if not use_random_split else False),
        )

        if use_time_series_split and not use_random_split:
            logger.info("ğŸ•’ æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨")
            n_samples = len(X)
            train_size = int(n_samples * (1 - test_size))
            
            X_train = X.iloc[:train_size].copy()
            X_test = X.iloc[train_size:].copy()
            y_train = y.iloc[:train_size].copy()
            y_test = y.iloc[train_size:].copy()
        else:
            logger.info("ğŸ”€ ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ã‚’ä½¿ç”¨")
            stratify_param = y if y.nunique() > 1 else None
            splits = train_test_split(
                X, y, test_size=test_size, random_state=random_state, stratify=stratify_param
            )
            X_train = cast(pd.DataFrame, splits[0])
            X_test = cast(pd.DataFrame, splits[1])
            y_train = cast(pd.Series, splits[2])
            y_test = cast(pd.Series, splits[3])

        return X_train, X_test, y_train, y_test

    def _time_series_cross_validate(
        self, X: pd.DataFrame, y: pd.Series, **training_params
    ) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        n_splits = training_params.get("cv_splits", self.config.training.CROSS_VALIDATION_FOLDS)
        max_train_size = training_params.get("max_train_size", self.config.training.MAX_TRAIN_SIZE)

        logger.info(f"ğŸ”„ æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ï¼ˆ{n_splits}åˆ†å‰²ï¼‰")

        if self.config.training.USE_PURGED_KFOLD:
            # ãƒ©ãƒ™ãƒ«ç”Ÿæˆè¨­å®šã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
            label_config = unified_config.ml.training.label_generation
            # ç°¡æ˜“çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼ˆè©³ç´°ã¯çœç•¥ï¼‰
            t1_horizon_n = self.config.training.PREDICTION_HORIZON
            t1_timeframe = "1h"
            
            t1 = self._get_t1_series(X.index, t1_horizon_n, t1_timeframe)
            splitter = PurgedKFold(n_splits=n_splits, t1=t1, pct_embargo=0.01)
        else:
            splitter = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size)

        cv_scores = []
        fold_results = []

        for fold, (train_idx, test_idx) in enumerate(splitter.split(X), 1):
            logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold}/{n_splits} ã‚’å®Ÿè¡Œä¸­...")
            X_train_cv = X.iloc[train_idx]
            X_test_cv = X.iloc[test_idx]
            y_train_cv = y.iloc[train_idx]
            y_test_cv = y.iloc[test_idx]

            # SMOTEå‡¦ç†ï¼ˆçœç•¥ï¼‰

            X_train_scaled, X_test_scaled = self._preprocess_data(X_train_cv, X_test_cv)

            fold_result = self._train_fold_with_error_handling(
                fold, X_train_scaled, X_test_scaled, y_train_cv, y_test_cv,
                X_train_cv, X_test_cv, training_params
            )

            cv_scores.append(fold_result.get("accuracy", 0.0))
            fold_results.append(fold_result)

        cv_result = {
            "cv_scores": cv_scores,
            "cv_mean": np.mean(cv_scores),
            "cv_std": np.std(cv_scores),
            "fold_results": fold_results,
        }
        return cv_result

    def _preprocess_data(
        self, X_train: pd.DataFrame, X_test: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰"""
        if self.scaler is None:
            self.scaler = StandardScaler()

        X_train_scaled = pd.DataFrame(
            self.scaler.fit_transform(X_train),
            columns=X_train.columns,
            index=X_train.index,
        )
        X_test_scaled = pd.DataFrame(
            self.scaler.transform(X_test), columns=X_test.columns, index=X_test.index
        )
        return X_train_scaled, X_test_scaled

    def save_model(
        self, model_name: str, metadata: Optional[Dict[str, Any]] = None
    ) -> Optional[str]:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        if not self.is_trained:
            raise MLModelError("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        final_metadata = {
            "model_type": self.__class__.__name__,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "is_trained": self.is_trained,
        }
        if metadata:
            final_metadata.update(metadata)

        try:
            feature_importance = self.get_feature_importance(top_n=100)
            if feature_importance:
                final_metadata["feature_importance"] = feature_importance
        except Exception as e:
            logger.warning(f"ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—ã«å¤±æ•—: {e}")

        # çµ±ä¸€ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚’ä½¿ç”¨
        model_path = model_manager.save_model(
            model=self,
            model_name=model_name,
            metadata=final_metadata,
            scaler=self.scaler,
            feature_columns=self.feature_columns,
        )

        logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")
        return model_path

    def _format_training_result(
        self, training_result: Dict[str, Any], X: pd.DataFrame, y: pd.Series
    ) -> Dict[str, Any]:
        """å­¦ç¿’çµæœã‚’æ•´å½¢"""
        result = {
            "success": True,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "total_samples": len(X),
            **training_result,
        }
        return result

    def get_feature_importance(self, top_n: int = 10) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè£…ï¼‰
        ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å¯èƒ½
        """
        if not self.is_trained:
            logger.warning("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}

        # LightGBM/XGBoostãƒ¢ãƒ‡ãƒ«ã®å ´åˆã®ä¸€èˆ¬çš„ãªå‡¦ç†
        if hasattr(self._model, "feature_importance") and self.feature_columns:
            try:
                importance_scores = self._model.feature_importance(importance_type="gain")
                feature_importance = dict(zip(self.feature_columns, importance_scores))
                sorted_importance = sorted(
                    feature_importance.items(), key=lambda x: x[1], reverse=True
                )[:top_n]
                return dict(sorted_importance)
            except Exception:
                pass
        
        # get_feature_importanceãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
        if hasattr(self._model, "get_feature_importance"):
            try:
                return self._model.get_feature_importance(top_n)
            except Exception:
                pass

        return {}

    @safe_ml_operation(
        default_return=False, context="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    )
    def load_model(self, model_path: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        model_data = model_manager.load_model(model_path)

        if model_data is None:
            return False

        self._model = model_data.get("model")
        self.scaler = model_data.get("scaler")
        self.feature_columns = model_data.get("feature_columns")

        if self._model is None:
            raise MLModelError("ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        self.is_trained = True
        return True

    def _cleanup_temporary_files(self, level: CleanupLevel):
        pass

    def _cleanup_cache(self, level: CleanupLevel):
        pass

    def _cleanup_models(self, level: CleanupLevel):
        try:
            if self.feature_service is not None:
                if hasattr(self.feature_service, "cleanup_resources"):
                    self.feature_service.cleanup_resources()

            self._model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False
        except Exception as e:
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}")
            self._model = None
            self.scaler = None
            self.feature_columns = None
            self.is_trained = False

    @safe_ml_operation(
        default_return={
            "fold": 0,
            "error": "ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            "accuracy": 0.0,
        },
        context="ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’",
    )
    def _train_fold_with_error_handling(
        self,
        fold: int,
        X_train_scaled: pd.DataFrame,
        X_test_scaled: pd.DataFrame,
        y_train_cv: pd.Series,
        y_test_cv: pd.Series,
        X_train_cv: pd.DataFrame,
        X_test_cv: pd.DataFrame,
        training_params: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ããƒ•ã‚©ãƒ¼ãƒ«ãƒ‰å­¦ç¿’"""
        fold_result = self._train_model_impl(
            X_train_scaled,
            X_test_scaled,
            y_train_cv,
            y_test_cv,
            **training_params,
        )

        fold_result.update(
            {
                "fold": fold,
                "train_samples": len(X_train_cv),
                "test_samples": len(X_test_cv),
                "train_period": f"{X_train_cv.index[0]} ï½ {X_train_cv.index[-1]}",
                "test_period": f"{X_test_cv.index[0]} ï½ {X_test_cv.index[-1]}",
            }
        )
        return fold_result

    def _prepare_combined_training_data(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
    ) -> pd.DataFrame:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆæº–å‚™ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰"""
        X_combined = pd.concat([X_train, X_test])
        y_combined = pd.concat([y_train, y_test])
        training_data = X_combined.copy()
        training_data["target"] = y_combined
        return training_data

    @staticmethod
    def _get_t1_series(
        indices: pd.DatetimeIndex, horizon_n: int, timeframe: str
    ) -> pd.Series:
        """PurgedKFoldç”¨t1è¨ˆç®—"""
        if timeframe == "1h":
            delta = pd.Timedelta(hours=horizon_n)
        elif timeframe == "4h":
            delta = pd.Timedelta(hours=4 * horizon_n)
        elif timeframe == "1d":
            delta = pd.Timedelta(days=horizon_n)
        elif timeframe == "15m":
            delta = pd.Timedelta(minutes=15 * horizon_n)
        else:
            delta = pd.Timedelta(hours=horizon_n)

        t1 = pd.Series(indices + delta, index=indices)
        return t1