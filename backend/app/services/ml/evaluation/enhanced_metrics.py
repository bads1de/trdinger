"""
æ‹¡å¼µè©•ä¾¡æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ 

åˆ†æå ±å‘Šæ›¸ã§ææ¡ˆã•ã‚ŒãŸåŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’å®Ÿè£…ã€‚
ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹é©åˆ‡ãªè©•ä¾¡æŒ‡æ¨™ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import logging
import numpy as np
from typing import Dict, Any, List, Optional
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    precision_recall_fscore_support,
    roc_auc_score,
    average_precision_score,
    confusion_matrix,
    classification_report,
)
from dataclasses import dataclass
import warnings

logger = logging.getLogger(__name__)


@dataclass
class MetricsConfig:
    """è©•ä¾¡æŒ‡æ¨™è¨­å®š"""

    include_balanced_accuracy: bool = True
    include_pr_auc: bool = True
    include_roc_auc: bool = True
    include_confusion_matrix: bool = True
    include_classification_report: bool = True
    average_method: str = "weighted"  # 'macro', 'micro', 'weighted'
    zero_division: int = 0


class EnhancedMetricsCalculator:
    """
    æ‹¡å¼µè©•ä¾¡æŒ‡æ¨™è¨ˆç®—å™¨

    ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸåŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’æä¾›ã—ã€
    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å¤šè§’çš„ã«è©•ä¾¡ã—ã¾ã™ã€‚
    """

    def __init__(self, config: MetricsConfig = None):
        """
        åˆæœŸåŒ–

        Args:
            config: è©•ä¾¡æŒ‡æ¨™è¨­å®š
        """
        self.config = config or MetricsConfig()

    def calculate_comprehensive_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—

        Args:
            y_true: çœŸã®ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            y_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        logger.info("ğŸ“Š åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ä¸­...")

        metrics = {}

        try:
            # åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™
            metrics.update(self._calculate_basic_metrics(y_true, y_pred))

            # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œæŒ‡æ¨™
            if self.config.include_balanced_accuracy:
                metrics.update(self._calculate_balanced_metrics(y_true, y_pred))

            # ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™
            if y_proba is not None:
                metrics.update(self._calculate_probability_metrics(y_true, y_proba))

            # æ··åŒè¡Œåˆ—
            if self.config.include_confusion_matrix:
                metrics.update(
                    self._calculate_confusion_matrix_metrics(
                        y_true, y_pred, class_names
                    )
                )

            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
            if self.config.include_classification_report:
                metrics.update(
                    self._calculate_classification_report(y_true, y_pred, class_names)
                )

            # ã‚¯ãƒ©ã‚¹åˆ¥è©³ç´°æŒ‡æ¨™
            metrics.update(
                self._calculate_per_class_metrics(y_true, y_pred, class_names)
            )

            # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒæƒ…å ±
            metrics.update(self._calculate_distribution_metrics(y_true, y_pred))

            logger.info("âœ… è©•ä¾¡æŒ‡æ¨™è¨ˆç®—å®Œäº†")

        except Exception as e:
            logger.error(f"è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            metrics["error"] = str(e)

        return metrics

    def _calculate_basic_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, float]:
        """åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # æ¨™æº–ç²¾åº¦
            metrics["accuracy"] = accuracy_score(y_true, y_pred)

            # ç²¾å¯†åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢
            precision, recall, f1, support = precision_recall_fscore_support(
                y_true,
                y_pred,
                average=self.config.average_method,
                zero_division=self.config.zero_division,
            )

            metrics["precision"] = precision
            metrics["recall"] = recall
            metrics["f1_score"] = f1

            # ãƒã‚¯ãƒ­å¹³å‡ã‚‚è¨ˆç®—
            precision_macro, recall_macro, f1_macro, _ = (
                precision_recall_fscore_support(
                    y_true,
                    y_pred,
                    average="macro",
                    zero_division=self.config.zero_division,
                )
            )

            metrics["precision_macro"] = precision_macro
            metrics["recall_macro"] = recall_macro
            metrics["f1_score_macro"] = f1_macro

        except Exception as e:
            logger.warning(f"åŸºæœ¬æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_balanced_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, float]:
        """ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œæŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦ï¼ˆåˆ†æå ±å‘Šæ›¸ã§æ¨å¥¨ï¼‰
            metrics["balanced_accuracy"] = balanced_accuracy_score(y_true, y_pred)

            # ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ãç²¾åº¦
            sample_weight = self._calculate_class_weights(y_true)
            metrics["weighted_accuracy"] = accuracy_score(
                y_true, y_pred, sample_weight=sample_weight
            )

        except Exception as e:
            logger.warning(f"ãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_probability_metrics(
        self, y_true: np.ndarray, y_proba: np.ndarray
    ) -> Dict[str, float]:
        """ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            n_classes = len(np.unique(y_true))

            if self.config.include_roc_auc:
                # ROC-AUC
                if n_classes == 2:
                    # äºŒå€¤åˆ†é¡
                    metrics["roc_auc"] = roc_auc_score(y_true, y_proba[:, 1])
                else:
                    # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        metrics["roc_auc_ovr"] = roc_auc_score(
                            y_true, y_proba, multi_class="ovr", average="weighted"
                        )
                        metrics["roc_auc_ovo"] = roc_auc_score(
                            y_true, y_proba, multi_class="ovo", average="weighted"
                        )

            if self.config.include_pr_auc:
                # PR-AUCï¼ˆåˆ†æå ±å‘Šæ›¸ã§æ¨å¥¨ï¼‰
                if n_classes == 2:
                    # äºŒå€¤åˆ†é¡
                    metrics["pr_auc"] = average_precision_score(y_true, y_proba[:, 1])
                else:
                    # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ï¼šå„ã‚¯ãƒ©ã‚¹ã®PR-AUCã‚’è¨ˆç®—
                    pr_aucs = []
                    for i in range(n_classes):
                        y_true_binary = (y_true == i).astype(int)
                        pr_auc = average_precision_score(y_true_binary, y_proba[:, i])
                        pr_aucs.append(pr_auc)
                        metrics[f"pr_auc_class_{i}"] = pr_auc

                    metrics["pr_auc_macro"] = np.mean(pr_aucs)
                    metrics["pr_auc_weighted"] = np.average(
                        pr_aucs, weights=np.bincount(y_true)
                    )

        except Exception as e:
            logger.warning(f"ç¢ºç‡æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_confusion_matrix_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """æ··åŒè¡Œåˆ—é–¢é€£æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # æ··åŒè¡Œåˆ—
            cm = confusion_matrix(y_true, y_pred)
            metrics["confusion_matrix"] = cm.tolist()

            # æ­£è¦åŒ–ã•ã‚ŒãŸæ··åŒè¡Œåˆ—
            cm_normalized = confusion_matrix(y_true, y_pred, normalize="true")
            metrics["confusion_matrix_normalized"] = cm_normalized.tolist()

            # ã‚¯ãƒ©ã‚¹åãŒã‚ã‚Œã°è¿½åŠ 
            if class_names:
                metrics["class_names"] = class_names

            # æ··åŒè¡Œåˆ—ã‹ã‚‰æ´¾ç”Ÿã™ã‚‹æŒ‡æ¨™
            if cm.shape[0] == 2:  # äºŒå€¤åˆ†é¡ã®å ´åˆ
                tn, fp, fn, tp = cm.ravel()
                metrics["true_negatives"] = int(tn)
                metrics["false_positives"] = int(fp)
                metrics["false_negatives"] = int(fn)
                metrics["true_positives"] = int(tp)

                # ç‰¹ç•°åº¦
                metrics["specificity"] = tn / (tn + fp) if (tn + fp) > 0 else 0.0
                # æ„Ÿåº¦ï¼ˆå†ç¾ç‡ã¨åŒã˜ï¼‰
                metrics["sensitivity"] = tp / (tp + fn) if (tp + fn) > 0 else 0.0

        except Exception as e:
            logger.warning(f"æ··åŒè¡Œåˆ—è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_classification_report(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆè¾æ›¸å½¢å¼ï¼‰
            report = classification_report(
                y_true,
                y_pred,
                target_names=class_names,
                output_dict=True,
                zero_division=self.config.zero_division,
            )
            metrics["classification_report"] = report

        except Exception as e:
            logger.warning(f"åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_per_class_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹åˆ¥è©³ç´°æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # ã‚¯ãƒ©ã‚¹åˆ¥ã®ç²¾å¯†åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢
            precision, recall, f1, support = precision_recall_fscore_support(
                y_true, y_pred, average=None, zero_division=self.config.zero_division
            )

            classes = np.unique(y_true)
            per_class_metrics = {}

            for i, class_label in enumerate(classes):
                class_name = (
                    class_names[i]
                    if class_names and i < len(class_names)
                    else f"class_{class_label}"
                )
                per_class_metrics[class_name] = {
                    "precision": float(precision[i]),
                    "recall": float(recall[i]),
                    "f1_score": float(f1[i]),
                    "support": int(support[i]),
                }

            metrics["per_class_metrics"] = per_class_metrics

        except Exception as e:
            logger.warning(f"ã‚¯ãƒ©ã‚¹åˆ¥æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_distribution_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒé–¢é€£æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # çœŸã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            true_counts = np.bincount(y_true)
            true_distribution = true_counts / len(y_true)
            metrics["true_label_distribution"] = true_distribution.tolist()

            # äºˆæ¸¬ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            pred_counts = np.bincount(y_pred, minlength=len(true_counts))
            pred_distribution = pred_counts / len(y_pred)
            metrics["predicted_label_distribution"] = pred_distribution.tolist()

            # ä¸å‡è¡¡æ¯”ç‡
            max_class_ratio = np.max(true_distribution) / np.min(
                true_distribution[true_distribution > 0]
            )
            metrics["class_imbalance_ratio"] = float(max_class_ratio)

            # ã‚µãƒ³ãƒ—ãƒ«æ•°
            metrics["total_samples"] = len(y_true)
            metrics["n_classes"] = len(true_counts)

        except Exception as e:
            logger.warning(f"åˆ†å¸ƒæŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_class_weights(self, y_true: np.ndarray) -> np.ndarray:
        """ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—"""
        classes, counts = np.unique(y_true, return_counts=True)
        total_samples = len(y_true)
        n_classes = len(classes)

        # ãƒãƒ©ãƒ³ã‚¹é‡ã¿è¨ˆç®—
        weights = total_samples / (n_classes * counts)

        # å„ã‚µãƒ³ãƒ—ãƒ«ã®é‡ã¿ã‚’è¨ˆç®—
        sample_weights = np.zeros(len(y_true))
        for i, class_label in enumerate(classes):
            mask = y_true == class_label
            sample_weights[mask] = weights[i]

        return sample_weights

    def generate_metrics_summary(self, metrics: Dict[str, Any]) -> str:
        """è©•ä¾¡æŒ‡æ¨™ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        summary_lines = []
        summary_lines.append("ğŸ“Š è©•ä¾¡æŒ‡æ¨™ã‚µãƒãƒªãƒ¼")
        summary_lines.append("=" * 50)

        # ä¸»è¦æŒ‡æ¨™
        if "accuracy" in metrics:
            summary_lines.append(f"ç²¾åº¦ (Accuracy): {metrics['accuracy']:.4f}")

        if "balanced_accuracy" in metrics:
            summary_lines.append(f"ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {metrics['balanced_accuracy']:.4f}")

        if "f1_score" in metrics:
            summary_lines.append(f"F1ã‚¹ã‚³ã‚¢: {metrics['f1_score']:.4f}")

        if "roc_auc" in metrics:
            summary_lines.append(f"ROC-AUC: {metrics['roc_auc']:.4f}")

        if "pr_auc" in metrics:
            summary_lines.append(f"PR-AUC: {metrics['pr_auc']:.4f}")

        # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æƒ…å ±
        if "class_imbalance_ratio" in metrics:
            summary_lines.append(
                f"ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯”ç‡: {metrics['class_imbalance_ratio']:.2f}"
            )

        return "\n".join(summary_lines)

    def save_metrics_report(self, metrics: Dict[str, Any], filepath: str):
        """è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            import json

            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {filepath}")
        except Exception as e:
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
enhanced_metrics_calculator = EnhancedMetricsCalculator()
