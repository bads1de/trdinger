"""
çµ±åˆè©•ä¾¡æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ 

åˆ†æå ±å‘Šæ›¸ã§ææ¡ˆã•ã‚ŒãŸåŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’å®Ÿè£…ã€‚
ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹é©åˆ‡ãªè©•ä¾¡æŒ‡æ¨™ã‚’æä¾›ã—ã€
ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»ç®¡ç†æ©Ÿèƒ½ã‚‚çµ±åˆã—ã¾ã™ã€‚
"""

import json
import logging
import threading
import time
import warnings
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Callable, Dict, List, Optional

import numpy as np
from sklearn.metrics import (
    accuracy_score,
    average_precision_score,
    balanced_accuracy_score,
    classification_report,
    confusion_matrix,
    precision_recall_fscore_support,
    roc_auc_score,
)

logger = logging.getLogger(__name__)


@dataclass
class MetricsConfig:
    """è©•ä¾¡æŒ‡æ¨™è¨­å®š"""

    include_balanced_accuracy: bool = True
    include_pr_auc: bool = True
    include_roc_auc: bool = True
    include_confusion_matrix: bool = True
    include_classification_report: bool = True
    average_method: str = "weighted"  # 'macro', 'micro', 'weighted'
    zero_division: int = 0


@dataclass
class MetricData:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿"""

    name: str
    value: float
    timestamp: datetime
    tags: Dict[str, str] = field(default_factory=dict)
    context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    operation: str
    duration_ms: float
    memory_mb: float
    cpu_percent: float
    success: bool
    timestamp: datetime
    error_message: Optional[str] = None


@dataclass
class ModelEvaluationMetrics:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    model_name: str
    model_type: str
    metrics: Dict[str, Any]
    timestamp: datetime
    dataset_info: Dict[str, Any] = field(default_factory=dict)
    training_params: Dict[str, Any] = field(default_factory=dict)


class EnhancedMetricsCalculator:
    """
    çµ±åˆè©•ä¾¡æŒ‡æ¨™è¨ˆç®—å™¨ãƒ»åé›†å™¨

    ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸåŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’æä¾›ã—ã€
    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å¤šè§’çš„ã«è©•ä¾¡ã—ã¾ã™ã€‚
    ã¾ãŸã€ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»ç®¡ç†æ©Ÿèƒ½ã‚‚çµ±åˆã—ã¦ã„ã¾ã™ã€‚
    """

    def __init__(self, config: MetricsConfig = None, max_history: int = 1000):
        """
        åˆæœŸåŒ–

        Args:
            config: è©•ä¾¡æŒ‡æ¨™è¨­å®š
            max_history: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã®æœ€å¤§ä¿æŒæ•°
        """
        self.config = config or MetricsConfig()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†æ©Ÿèƒ½ã®åˆæœŸåŒ–
        self.max_history = max_history
        self._metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=max_history))
        self._performance_metrics: deque = deque(maxlen=max_history)
        self._model_evaluation_metrics: deque = deque(maxlen=max_history)
        self._error_counts: Dict[str, int] = defaultdict(int)
        self._operation_counts: Dict[str, int] = defaultdict(int)
        self._lock = threading.Lock()

    def calculate_comprehensive_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—

        Args:
            y_true: çœŸã®ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            y_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        logger.info("ğŸ“Š åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ä¸­...")

        metrics = {}

        try:
            # åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™
            metrics.update(self._calculate_basic_metrics(y_true, y_pred))

            # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œæŒ‡æ¨™
            if self.config.include_balanced_accuracy:
                metrics.update(self._calculate_balanced_metrics(y_true, y_pred))

            # ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™
            if y_proba is not None:
                metrics.update(self._calculate_probability_metrics(y_true, y_proba))

            # æ··åŒè¡Œåˆ—
            if self.config.include_confusion_matrix:
                metrics.update(
                    self._calculate_confusion_matrix_metrics(
                        y_true, y_pred, class_names
                    )
                )

            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
            if self.config.include_classification_report:
                metrics.update(
                    self._calculate_classification_report(y_true, y_pred, class_names)
                )

            # ã‚¯ãƒ©ã‚¹åˆ¥è©³ç´°æŒ‡æ¨™
            metrics.update(
                self._calculate_per_class_metrics(y_true, y_pred, class_names)
            )

            # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒæƒ…å ±
            metrics.update(self._calculate_distribution_metrics(y_true, y_pred))

            logger.info("âœ… è©•ä¾¡æŒ‡æ¨™è¨ˆç®—å®Œäº†")

        except Exception as e:
            logger.error(f"è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            metrics["error"] = str(e)

        return metrics

    def record_metric(
        self,
        name: str,
        value: float,
        tags: Optional[Dict[str, str]] = None,
        context: Optional[Dict[str, Any]] = None,
    ):
        """
        ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²

        Args:
            name: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å
            value: å€¤
            tags: ã‚¿ã‚°
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        """
        with self._lock:
            metric = MetricData(
                name=name,
                value=value,
                timestamp=datetime.now(),
                tags=tags or {},
                context=context or {},
            )
            self._metrics[name].append(metric)

    def record_performance(
        self,
        operation: str,
        duration_ms: float,
        memory_mb: float = 0.0,
        cpu_percent: float = 0.0,
        success: bool = True,
        error_message: Optional[str] = None,
    ):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²

        Args:
            operation: æ“ä½œå
            duration_ms: å‡¦ç†æ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰
            memory_mb: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
            cpu_percent: CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰
            success: æˆåŠŸãƒ•ãƒ©ã‚°
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        with self._lock:
            perf_metric = PerformanceMetrics(
                operation=operation,
                duration_ms=duration_ms,
                memory_mb=memory_mb,
                cpu_percent=cpu_percent,
                success=success,
                timestamp=datetime.now(),
                error_message=error_message,
            )
            self._performance_metrics.append(perf_metric)

            # æ“ä½œã‚«ã‚¦ãƒ³ãƒˆ
            self._operation_counts[operation] += 1

            # ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆ
            if not success:
                self._error_counts[operation] += 1

    def record_error(self, operation: str, error_type: str, error_message: str):
        """
        ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²

        Args:
            operation: æ“ä½œå
            error_type: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        with self._lock:
            self._error_counts[f"{operation}_{error_type}"] += 1

        # ãƒ­ã‚°ã«ã‚‚å‡ºåŠ›
        logger.error(
            f"Operation: {operation}, Error: {error_type}, Message: {error_message}"
        )

    def record_model_evaluation_metrics(
        self,
        model_name: str,
        model_type: str,
        evaluation_metrics: Dict[str, Any],
        dataset_info: Optional[Dict[str, Any]] = None,
        training_params: Optional[Dict[str, Any]] = None,
    ):
        """
        ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—
            evaluation_metrics: è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            dataset_info: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
            training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        with self._lock:
            # å€‹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚‚è¨˜éŒ²
            for metric_name, value in evaluation_metrics.items():
                if isinstance(value, (int, float)):
                    self.record_metric(
                        name=metric_name,
                        value=float(value),
                        tags={
                            "model_name": model_name,
                            "model_type": model_type,
                            "metric_category": "model_evaluation",
                        },
                        context={
                            "dataset_info": dataset_info or {},
                            "training_params": training_params or {},
                        },
                    )

            # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¨ä½“ã‚’è¨˜éŒ²
            model_eval_metric = ModelEvaluationMetrics(
                model_name=model_name,
                model_type=model_type,
                metrics=evaluation_metrics,
                timestamp=datetime.now(),
                dataset_info=dataset_info or {},
                training_params=training_params or {},
            )
            self._model_evaluation_metrics.append(model_eval_metric)

    def evaluate_and_record_model(
        self,
        model_name: str,
        model_type: str,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None,
        class_names: Optional[List[str]] = None,
        dataset_info: Optional[Dict[str, Any]] = None,
        training_params: Optional[Dict[str, Any]] = None,
        training_time: Optional[float] = None,
        memory_usage: Optional[float] = None,
    ) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€çµæœã‚’è¨˜éŒ²

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—
            y_true: çœŸã®ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            y_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            dataset_info: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            training_time: å­¦ç¿’æ™‚é–“ï¼ˆç§’ï¼‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            memory_usage: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        try:
            logger.info(f"ğŸ“Š çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡é–‹å§‹: {model_name} ({model_type})")

            # åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚’å®Ÿè¡Œ
            evaluation_metrics = self.calculate_comprehensive_metrics(
                y_true=y_true,
                y_pred=y_pred,
                y_proba=y_proba,
                class_names=class_names,
            )

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’è¿½åŠ 
            if training_time is not None:
                evaluation_metrics["training_time"] = training_time
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã—ã¦ã‚‚è¨˜éŒ²
                self.record_performance(
                    operation=f"model_training_{model_type}",
                    duration_ms=training_time * 1000,  # ç§’ã‚’ãƒŸãƒªç§’ã«å¤‰æ›
                    memory_mb=memory_usage or 0.0,
                    success=True,
                )

            if memory_usage is not None:
                evaluation_metrics["memory_usage"] = memory_usage

            # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœã‚’è¨˜éŒ²
            self.record_model_evaluation_metrics(
                model_name=model_name,
                model_type=model_type,
                evaluation_metrics=evaluation_metrics,
                dataset_info=dataset_info,
                training_params=training_params,
            )

            # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°å‡ºåŠ›
            accuracy = evaluation_metrics.get("accuracy", 0.0)
            f1_score = evaluation_metrics.get("f1_score", 0.0)
            balanced_accuracy = evaluation_metrics.get("balanced_accuracy", 0.0)

            logger.info(
                f"âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Œäº†: {model_name} - "
                f"ç²¾åº¦={accuracy:.4f}, F1={f1_score:.4f}, ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦={balanced_accuracy:.4f}"
            )

            return evaluation_metrics

        except Exception as e:
            logger.error(f"âŒ çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {model_name} - {e}")
            # ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            self.record_error(
                operation=f"model_evaluation_{model_type}",
                error_type=type(e).__name__,
                error_message=str(e),
            )
            raise

    def _calculate_basic_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, float]:
        """åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # æ¨™æº–ç²¾åº¦
            metrics["accuracy"] = accuracy_score(y_true, y_pred)

            # ç²¾å¯†åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢
            precision, recall, f1, support = precision_recall_fscore_support(
                y_true,
                y_pred,
                average=self.config.average_method,
                zero_division=self.config.zero_division,
            )

            metrics["precision"] = precision
            metrics["recall"] = recall
            metrics["f1_score"] = f1

            # ãƒã‚¯ãƒ­å¹³å‡ã‚‚è¨ˆç®—
            precision_macro, recall_macro, f1_macro, _ = (
                precision_recall_fscore_support(
                    y_true,
                    y_pred,
                    average="macro",
                    zero_division=self.config.zero_division,
                )
            )

            metrics["precision_macro"] = precision_macro
            metrics["recall_macro"] = recall_macro
            metrics["f1_score_macro"] = f1_macro

        except Exception as e:
            logger.warning(f"åŸºæœ¬æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_balanced_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, float]:
        """ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œæŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦ï¼ˆåˆ†æå ±å‘Šæ›¸ã§æ¨å¥¨ï¼‰
            metrics["balanced_accuracy"] = balanced_accuracy_score(y_true, y_pred)

            # ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ãç²¾åº¦
            sample_weight = self._calculate_class_weights(y_true)
            metrics["weighted_accuracy"] = accuracy_score(
                y_true, y_pred, sample_weight=sample_weight
            )

        except Exception as e:
            logger.warning(f"ãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_probability_metrics(
        self, y_true: np.ndarray, y_proba: np.ndarray
    ) -> Dict[str, float]:
        """ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            n_classes = len(np.unique(y_true))

            if self.config.include_roc_auc:
                # ROC-AUC
                if n_classes == 2:
                    # äºŒå€¤åˆ†é¡
                    metrics["roc_auc"] = roc_auc_score(y_true, y_proba[:, 1])
                else:
                    # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        metrics["roc_auc_ovr"] = roc_auc_score(
                            y_true, y_proba, multi_class="ovr", average="weighted"
                        )
                        metrics["roc_auc_ovo"] = roc_auc_score(
                            y_true, y_proba, multi_class="ovo", average="weighted"
                        )

            if self.config.include_pr_auc:
                # PR-AUCï¼ˆåˆ†æå ±å‘Šæ›¸ã§æ¨å¥¨ï¼‰
                if n_classes == 2:
                    # äºŒå€¤åˆ†é¡
                    metrics["pr_auc"] = average_precision_score(y_true, y_proba[:, 1])
                else:
                    # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ï¼šå„ã‚¯ãƒ©ã‚¹ã®PR-AUCã‚’è¨ˆç®—
                    pr_aucs = []
                    for i in range(n_classes):
                        y_true_binary = (y_true == i).astype(int)
                        pr_auc = average_precision_score(y_true_binary, y_proba[:, i])
                        pr_aucs.append(pr_auc)
                        metrics[f"pr_auc_class_{i}"] = pr_auc

                    metrics["pr_auc_macro"] = np.mean(pr_aucs)
                    metrics["pr_auc_weighted"] = np.average(
                        pr_aucs, weights=np.bincount(y_true)
                    )

        except Exception as e:
            logger.warning(f"ç¢ºç‡æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_confusion_matrix_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """æ··åŒè¡Œåˆ—é–¢é€£æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # æ··åŒè¡Œåˆ—
            cm = confusion_matrix(y_true, y_pred)
            metrics["confusion_matrix"] = cm.tolist()

            # æ­£è¦åŒ–ã•ã‚ŒãŸæ··åŒè¡Œåˆ—
            cm_normalized = confusion_matrix(y_true, y_pred, normalize="true")
            metrics["confusion_matrix_normalized"] = cm_normalized.tolist()

            # ã‚¯ãƒ©ã‚¹åãŒã‚ã‚Œã°è¿½åŠ 
            if class_names:
                metrics["class_names"] = class_names

            # æ··åŒè¡Œåˆ—ã‹ã‚‰æ´¾ç”Ÿã™ã‚‹æŒ‡æ¨™
            if cm.shape[0] == 2:  # äºŒå€¤åˆ†é¡ã®å ´åˆ
                tn, fp, fn, tp = cm.ravel()
                metrics["true_negatives"] = int(tn)
                metrics["false_positives"] = int(fp)
                metrics["false_negatives"] = int(fn)
                metrics["true_positives"] = int(tp)

                # ç‰¹ç•°åº¦
                metrics["specificity"] = tn / (tn + fp) if (tn + fp) > 0 else 0.0
                # æ„Ÿåº¦ï¼ˆå†ç¾ç‡ã¨åŒã˜ï¼‰
                metrics["sensitivity"] = tp / (tp + fn) if (tp + fn) > 0 else 0.0

        except Exception as e:
            logger.warning(f"æ··åŒè¡Œåˆ—è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_classification_report(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆè¾æ›¸å½¢å¼ï¼‰
            report = classification_report(
                y_true,
                y_pred,
                target_names=class_names,
                output_dict=True,
                zero_division=self.config.zero_division,
            )
            metrics["classification_report"] = report

        except Exception as e:
            logger.warning(f"åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_per_class_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹åˆ¥è©³ç´°æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # ã‚¯ãƒ©ã‚¹åˆ¥ã®ç²¾å¯†åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢
            precision, recall, f1, support = precision_recall_fscore_support(
                y_true, y_pred, average=None, zero_division=self.config.zero_division
            )

            classes = np.unique(y_true)
            per_class_metrics = {}

            for i, class_label in enumerate(classes):
                class_name = (
                    class_names[i]
                    if class_names and i < len(class_names)
                    else f"class_{class_label}"
                )
                per_class_metrics[class_name] = {
                    "precision": float(precision[i]),
                    "recall": float(recall[i]),
                    "f1_score": float(f1[i]),
                    "support": int(support[i]),
                }

            metrics["per_class_metrics"] = per_class_metrics

        except Exception as e:
            logger.warning(f"ã‚¯ãƒ©ã‚¹åˆ¥æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_distribution_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒé–¢é€£æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # çœŸã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            true_counts = np.bincount(y_true)
            true_distribution = true_counts / len(y_true)
            metrics["true_label_distribution"] = true_distribution.tolist()

            # äºˆæ¸¬ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            pred_counts = np.bincount(y_pred, minlength=len(true_counts))
            pred_distribution = pred_counts / len(y_pred)
            metrics["predicted_label_distribution"] = pred_distribution.tolist()

            # ä¸å‡è¡¡æ¯”ç‡
            max_class_ratio = np.max(true_distribution) / np.min(
                true_distribution[true_distribution > 0]
            )
            metrics["class_imbalance_ratio"] = float(max_class_ratio)

            # ã‚µãƒ³ãƒ—ãƒ«æ•°
            metrics["total_samples"] = len(y_true)
            metrics["n_classes"] = len(true_counts)

        except Exception as e:
            logger.warning(f"åˆ†å¸ƒæŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_class_weights(self, y_true: np.ndarray) -> np.ndarray:
        """ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—"""
        classes, counts = np.unique(y_true, return_counts=True)
        total_samples = len(y_true)
        n_classes = len(classes)

        # ãƒãƒ©ãƒ³ã‚¹é‡ã¿è¨ˆç®—
        weights = total_samples / (n_classes * counts)

        # å„ã‚µãƒ³ãƒ—ãƒ«ã®é‡ã¿ã‚’è¨ˆç®—
        sample_weights = np.zeros(len(y_true))
        for i, class_label in enumerate(classes):
            mask = y_true == class_label
            sample_weights[mask] = weights[i]

        return sample_weights

    def generate_metrics_summary(self, metrics: Dict[str, Any]) -> str:
        """è©•ä¾¡æŒ‡æ¨™ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        summary_lines = []
        summary_lines.append("ğŸ“Š è©•ä¾¡æŒ‡æ¨™ã‚µãƒãƒªãƒ¼")
        summary_lines.append("=" * 50)

        # ä¸»è¦æŒ‡æ¨™
        if "accuracy" in metrics:
            summary_lines.append(f"ç²¾åº¦ (Accuracy): {metrics['accuracy']:.4f}")

        if "balanced_accuracy" in metrics:
            summary_lines.append(f"ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦: {metrics['balanced_accuracy']:.4f}")

        if "f1_score" in metrics:
            summary_lines.append(f"F1ã‚¹ã‚³ã‚¢: {metrics['f1_score']:.4f}")

        if "roc_auc" in metrics:
            summary_lines.append(f"ROC-AUC: {metrics['roc_auc']:.4f}")

        if "pr_auc" in metrics:
            summary_lines.append(f"PR-AUC: {metrics['pr_auc']:.4f}")

        # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æƒ…å ±
        if "class_imbalance_ratio" in metrics:
            summary_lines.append(
                f"ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯”ç‡: {metrics['class_imbalance_ratio']:.2f}"
            )

        return "\n".join(summary_lines)

    def save_metrics_report(self, metrics: Dict[str, Any], filepath: str):
        """è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            import json

            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {filepath}")
        except Exception as e:
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆçµ±åˆã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—å™¨ãƒ»åé›†å™¨ï¼‰
enhanced_metrics_calculator = EnhancedMetricsCalculator()


# ä¾¿åˆ©ãªé–¢æ•°ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
def record_metric(name: str, value: float, **kwargs):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
    enhanced_metrics_calculator.record_metric(name, value, **kwargs)


def record_performance(operation: str, duration_ms: float, **kwargs):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²"""
    enhanced_metrics_calculator.record_performance(operation, duration_ms, **kwargs)


def record_error(operation: str, error_type: str, error_message: str):
    """ã‚¨ãƒ©ãƒ¼è¨˜éŒ²"""
    enhanced_metrics_calculator.record_error(operation, error_type, error_message)


def record_model_evaluation_metrics(
    model_name: str,
    model_type: str,
    evaluation_metrics: Dict[str, Any],
    dataset_info: Optional[Dict[str, Any]] = None,
    training_params: Optional[Dict[str, Any]] = None,
):
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
    enhanced_metrics_calculator.record_model_evaluation_metrics(
        model_name, model_type, evaluation_metrics, dataset_info, training_params
    )


def evaluate_and_record_model(
    model_name: str,
    model_type: str,
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_proba: Optional[np.ndarray] = None,
    **kwargs,
) -> Dict[str, Any]:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨è¨˜éŒ²ã®ä¾¿åˆ©é–¢æ•°"""
    return enhanced_metrics_calculator.evaluate_and_record_model(
        model_name, model_type, y_true, y_pred, y_proba, **kwargs
    )


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
MLMetricsCollector = EnhancedMetricsCalculator
metrics_collector = enhanced_metrics_calculator
