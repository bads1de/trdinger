"""
çµ±åˆè©•ä¾¡æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ 

åˆ†æå ±å‘Šæ›¸ã§ææ¡ˆã•ã‚ŒãŸåŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’å®Ÿè£…ã€‚
ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹é©åˆ‡ãªè©•ä¾¡æŒ‡æ¨™ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import logging
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

import numpy as np
from sklearn.metrics import (
    accuracy_score,
    average_precision_score,
    balanced_accuracy_score,
    brier_score_loss,
    classification_report,
    cohen_kappa_score,
    confusion_matrix,
    log_loss,
    matthews_corrcoef,
    multilabel_confusion_matrix,
    precision_recall_fscore_support,
    roc_auc_score,
)

from app.utils.error_handler import safe_operation

logger = logging.getLogger(__name__)


@dataclass
class MetricsConfig:
    """è©•ä¾¡æŒ‡æ¨™è¨­å®š"""

    include_balanced_accuracy: bool = True
    include_pr_auc: bool = True
    include_roc_auc: bool = True
    include_confusion_matrix: bool = True
    include_classification_report: bool = True
    average_method: str = "weighted"  # 'macro', 'micro', 'weighted'
    zero_division: str = "warn"


class MetricsCalculator:
    """
    çµ±åˆè©•ä¾¡æŒ‡æ¨™è¨ˆç®—å™¨

    ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸåŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’æä¾›ã—ã€
    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å¤šè§’çš„ã«è©•ä¾¡ã—ã¾ã™ã€‚
    """

    def __init__(self, config: MetricsConfig | None = None):
        """
        åˆæœŸåŒ–

        Args:
            config: è©•ä¾¡æŒ‡æ¨™è¨­å®š
        """
        self.config = config or MetricsConfig()

    @safe_operation(
        context="åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™è¨ˆç®—", is_api_call=False, default_return={}
    )
    def calculate_comprehensive_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None,
        class_names: Optional[List[str]] = None,
        level: str = "full",
    ) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—

        Args:
            y_true: çœŸã®ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            y_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ
            level: è¨ˆç®—ãƒ¬ãƒ™ãƒ« ('full' ã¾ãŸã¯ 'basic')
                   - 'basic': åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™ã®ã¿ï¼ˆé«˜é€Ÿï¼‰
                   - 'full': å…¨ã¦ã®æŒ‡æ¨™ï¼ˆAUCã€åˆ†å¸ƒãªã©å«ã‚€ï¼‰

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        if level == "full":
            logger.info("ğŸ“Š åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ä¸­(Full)...")

        metrics = {}

        # åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™ï¼ˆå¸¸ã«è¨ˆç®—ï¼‰
        metrics.update(self._calculate_basic_metrics(y_true, y_pred))

        # Basicãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ã“ã“ã§çµ‚äº†
        if level == "basic":
            # ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦ã¯é‡è¦ã‹ã¤è»½é‡ãªã®ã§è¨ˆç®—
            if self.config.include_balanced_accuracy:
                metrics.update(self._calculate_balanced_metrics(y_true, y_pred))
            return metrics

        # ä»¥ä¸‹ã¯Fullãƒ¢ãƒ¼ãƒ‰ã®ã¿å®Ÿè¡Œ

        # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œæŒ‡æ¨™
        if self.config.include_balanced_accuracy:
            metrics.update(self._calculate_balanced_metrics(y_true, y_pred))

        # ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™
        if y_proba is not None:
            metrics.update(self._calculate_probability_metrics(y_true, y_proba))

        # æ··åŒè¡Œåˆ—
        if self.config.include_confusion_matrix:
            metrics.update(
                self._calculate_confusion_matrix_metrics(y_true, y_pred, class_names)
            )

        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
        if self.config.include_classification_report:
            metrics.update(
                self._calculate_classification_report(y_true, y_pred, class_names)
            )

        # ã‚¯ãƒ©ã‚¹åˆ¥è©³ç´°æŒ‡æ¨™
        metrics.update(self._calculate_per_class_metrics(y_true, y_pred, class_names))

        # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒæƒ…å ±
        metrics.update(self._calculate_distribution_metrics(y_true, y_pred))

        if level == "full":
            logger.info("âœ… è©•ä¾¡æŒ‡æ¨™è¨ˆç®—å®Œäº†")

        return metrics

    def _calculate_basic_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, float]:
        """åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™ã‚’è¨ˆç®—"""
        try:
            # æ¨™æº–ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            p, r, f, _ = precision_recall_fscore_support(
                y_true, y_pred, average=self.config.average_method, zero_division=self.config.zero_division
            )
            pm, rm, fm, _ = precision_recall_fscore_support(
                y_true, y_pred, average="macro", zero_division=self.config.zero_division
            )
            
            return {
                "accuracy": accuracy_score(y_true, y_pred),
                "precision": float(p), "recall": float(r), "f1_score": float(f),
                "precision_macro": float(pm), "recall_macro": float(rm), "f1_score_macro": float(fm),
                "matthews_corrcoef": matthews_corrcoef(y_true, y_pred),
                "cohen_kappa": cohen_kappa_score(y_true, y_pred)
            }
        except Exception as e:
            logger.warning(f"åŸºæœ¬æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _calculate_balanced_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, float]:
        """ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œæŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦ï¼ˆåˆ†æå ±å‘Šæ›¸ã§æ¨å¥¨ï¼‰
            metrics["balanced_accuracy"] = balanced_accuracy_score(y_true, y_pred)

            # ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ãç²¾åº¦
            sample_weight = self._calculate_class_weights(y_true)
            metrics["weighted_accuracy"] = accuracy_score(
                y_true, y_pred, sample_weight=sample_weight
            )

        except Exception as e:
            logger.warning(f"ãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_probability_metrics(
        self, y_true: np.ndarray, y_proba: np.ndarray
    ) -> Dict[str, float]:
        """ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}
        try:
            n_classes = len(np.unique(y_true))
            y_pos = y_proba[:, 1] if y_proba.ndim > 1 and y_proba.shape[1] > 1 else y_proba.ravel()

            if self.config.include_roc_auc:
                if n_classes == 2:
                    metrics["roc_auc"] = roc_auc_score(y_true, y_pos)
                else:
                    metrics["roc_auc"] = roc_auc_score(y_true, y_proba, multi_class="ovr", average="weighted")

            if self.config.include_pr_auc:
                if n_classes == 2:
                    metrics["pr_auc"] = average_precision_score(y_true, y_pos)
                else:
                    pr_aucs = [average_precision_score((y_true == i).astype(int), y_proba[:, i]) 
                               for i in range(n_classes) if np.sum(y_true == i) > 0]
                    metrics["pr_auc"] = np.mean(pr_aucs) if pr_aucs else 0.0

            metrics["log_loss"] = log_loss(y_true, y_proba)
            if n_classes == 2:
                metrics["brier_score"] = brier_score_loss(y_true, y_pos)
            else:
                metrics["brier_score"] = np.mean([brier_score_loss((y_true == i).astype(int), y_proba[:, i]) 
                                                 for i in range(n_classes)])
        except Exception as e:
            logger.warning(f"ç¢ºç‡æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            metrics.update({"log_loss": 0.0, "brier_score": 0.0})
        return metrics

    def _calculate_confusion_matrix_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray, class_names: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """æ··åŒè¡Œåˆ—é–¢é€£æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}
        try:
            cm = confusion_matrix(y_true, y_pred)
            metrics["confusion_matrix"] = cm.tolist()
            metrics["confusion_matrix_normalized"] = confusion_matrix(y_true, y_pred, normalize="true").tolist()
            if class_names:
                metrics["class_names"] = class_names

            labels = np.unique(y_true)
            mcm = multilabel_confusion_matrix(y_true, y_pred, labels=labels)
            tn, fp, fn, tp = mcm[:, 0, 0], mcm[:, 0, 1], mcm[:, 1, 0], mcm[:, 1, 1]
            
            eps = 1e-12
            spec, sens, npv, ppv = tn / (tn + fp + eps), tp / (tp + fn + eps), tn / (tn + fn + eps), tp / (tp + fp + eps)

            if len(labels) == 2:
                metrics.update({"true_negatives": int(tn[1]), "false_positives": int(fp[1]), 
                                "false_negatives": int(fn[1]), "true_positives": int(tp[1]),
                                "specificity": float(spec[1]), "sensitivity": float(sens[1]),
                                "npv": float(npv[1]), "ppv": float(ppv[1])})
            else:
                counts = np.bincount(y_true)
                weights = counts / (counts.sum() + eps)
                metrics.update({"specificity": float(np.average(spec, weights=weights)),
                                "sensitivity": float(np.average(sens, weights=weights)),
                                "npv": float(np.average(npv, weights=weights)),
                                "ppv": float(np.average(ppv, weights=weights))})
        except Exception as e:
            logger.warning(f"æ··åŒè¡Œåˆ—è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return metrics

    def _calculate_classification_report(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆè¾æ›¸å½¢å¼ï¼‰
            report = classification_report(
                y_true,
                y_pred,
                target_names=class_names,
                output_dict=True,
                zero_division=self.config.zero_division,
            )
            metrics["classification_report"] = report

        except Exception as e:
            logger.warning(f"åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_per_class_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹åˆ¥è©³ç´°æŒ‡æ¨™ã‚’è¨ˆç®—"""
        try:
            p, r, f, s = precision_recall_fscore_support(
                y_true, y_pred, average=None, zero_division=self.config.zero_division
            )
            labels = np.unique(y_true)
            
            return {"per_class_metrics": {
                (class_names[i] if class_names and i < len(class_names) else f"class_{lab}"): {
                    "precision": float(p[i]), "recall": float(r[i]), 
                    "f1_score": float(f[i]), "support": int(s[i])
                } for i, lab in enumerate(labels)
            }}
        except Exception as e:
            logger.warning(f"ã‚¯ãƒ©ã‚¹åˆ¥æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _calculate_distribution_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒé–¢é€£æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # çœŸã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            true_counts = np.bincount(y_true)
            true_distribution = true_counts / len(y_true)
            metrics["true_label_distribution"] = true_distribution.tolist()

            # äºˆæ¸¬ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            pred_counts = np.bincount(y_pred, minlength=len(true_counts))
            pred_distribution = pred_counts / len(y_pred)
            metrics["predicted_label_distribution"] = pred_distribution.tolist()

            # ä¸å‡è¡¡æ¯”ç‡
            max_class_ratio = np.max(true_distribution) / np.min(
                true_distribution[true_distribution > 0]
            )
            metrics["class_imbalance_ratio"] = float(max_class_ratio)

            # ã‚µãƒ³ãƒ—ãƒ«æ•°
            metrics["total_samples"] = len(y_true)
            metrics["n_classes"] = len(true_counts)

        except Exception as e:
            logger.warning(f"åˆ†å¸ƒæŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_class_weights(self, y_true: np.ndarray) -> np.ndarray:
        """ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—"""
        classes, counts = np.unique(y_true, return_counts=True)
        total_samples = len(y_true)
        n_classes = len(classes)

        # ãƒãƒ©ãƒ³ã‚¹é‡ã¿è¨ˆç®—
        weights = total_samples / (n_classes * counts)

        # å„ã‚µãƒ³ãƒ—ãƒ«ã®é‡ã¿ã‚’è¨ˆç®—
        sample_weights = np.zeros(len(y_true))
        for i, class_label in enumerate(classes):
            mask = y_true == class_label
            sample_weights[mask] = weights[i]

        return sample_weights


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
metrics_collector = MetricsCalculator()