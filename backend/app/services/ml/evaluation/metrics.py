"""
çµ±åˆè©•ä¾¡æŒ‡æ¨™ã‚·ã‚¹ãƒ†ãƒ 

åˆ†æå ±å‘Šæ›¸ã§ææ¡ˆã•ã‚ŒãŸåŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’å®Ÿè£…ã€‚
ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹é©åˆ‡ãªè©•ä¾¡æŒ‡æ¨™ã‚’æä¾›ã—ã€
ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»ç®¡ç†æ©Ÿèƒ½ã‚‚çµ±åˆã—ã¾ã™ã€‚
"""

import logging
import threading
import warnings
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional

import numpy as np
from sklearn.metrics import (
    accuracy_score,
    average_precision_score,
    balanced_accuracy_score,
    brier_score_loss,
    classification_report,
    cohen_kappa_score,
    confusion_matrix,
    log_loss,
    matthews_corrcoef,
    multilabel_confusion_matrix,
    precision_recall_fscore_support,
    roc_auc_score,
)

from app.utils.error_handler import safe_operation

logger = logging.getLogger(__name__)


@dataclass
class MetricsConfig:
    """è©•ä¾¡æŒ‡æ¨™è¨­å®š"""

    include_balanced_accuracy: bool = True
    include_pr_auc: bool = True
    include_roc_auc: bool = True
    include_confusion_matrix: bool = True
    include_classification_report: bool = True
    average_method: str = "weighted"  # 'macro', 'micro', 'weighted'
    zero_division: str = "warn"


@dataclass
class MetricData:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿"""

    name: str
    value: float
    timestamp: datetime
    tags: Dict[str, str] = field(default_factory=dict)
    context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    operation: str
    duration_ms: float
    memory_mb: float
    cpu_percent: float
    success: bool
    timestamp: datetime
    error_message: Optional[str] = None


@dataclass
class ModelEvaluationMetrics:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    model_name: str
    model_type: str
    metrics: Dict[str, Any]
    timestamp: datetime
    dataset_info: Dict[str, Any] = field(default_factory=dict)
    training_params: Dict[str, Any] = field(default_factory=dict)


class MetricsCalculator:
    """
    çµ±åˆè©•ä¾¡æŒ‡æ¨™è¨ˆç®—å™¨ãƒ»åé›†å™¨

    ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸåŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’æä¾›ã—ã€
    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å¤šè§’çš„ã«è©•ä¾¡ã—ã¾ã™ã€‚
    ã¾ãŸã€ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»ç®¡ç†æ©Ÿèƒ½ã‚‚çµ±åˆã—ã¦ã„ã¾ã™ã€‚
    """

    def __init__(self, config: MetricsConfig | None = None, max_history: int = 1000):
        """
        åˆæœŸåŒ–

        Args:
            config: è©•ä¾¡æŒ‡æ¨™è¨­å®š
            max_history: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã®æœ€å¤§ä¿æŒæ•°
        """
        self.config = config or MetricsConfig()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†æ©Ÿèƒ½ã®åˆæœŸåŒ–
        self.max_history = max_history
        self._metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=max_history))
        self._performance_metrics: deque = deque(maxlen=max_history)
        self._model_evaluation_metrics: deque = deque(maxlen=max_history)
        self._error_counts: Dict[str, int] = defaultdict(int)
        self._operation_counts: Dict[str, int] = defaultdict(int)
        self._lock = threading.Lock()

    @safe_operation(
        context="åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™è¨ˆç®—", is_api_call=False, default_return={}
    )
    def calculate_comprehensive_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—

        Args:
            y_true: çœŸã®ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            y_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆ

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        logger.info("ğŸ“Š åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ä¸­...")

        metrics = {}

        # åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™
        metrics.update(self._calculate_basic_metrics(y_true, y_pred))

        # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œæŒ‡æ¨™
        if self.config.include_balanced_accuracy:
            metrics.update(self._calculate_balanced_metrics(y_true, y_pred))

        # ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™
        if y_proba is not None:
            metrics.update(self._calculate_probability_metrics(y_true, y_proba))

        # æ··åŒè¡Œåˆ—
        if self.config.include_confusion_matrix:
            metrics.update(
                self._calculate_confusion_matrix_metrics(y_true, y_pred, class_names)
            )

        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
        if self.config.include_classification_report:
            metrics.update(
                self._calculate_classification_report(y_true, y_pred, class_names)
            )

        # ã‚¯ãƒ©ã‚¹åˆ¥è©³ç´°æŒ‡æ¨™
        metrics.update(self._calculate_per_class_metrics(y_true, y_pred, class_names))

        # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒæƒ…å ±
        metrics.update(self._calculate_distribution_metrics(y_true, y_pred))

        logger.info("âœ… è©•ä¾¡æŒ‡æ¨™è¨ˆç®—å®Œäº†")

        return metrics

    def record_metric(
        self,
        name: str,
        value: float,
        tags: Optional[Dict[str, str]] = None,
        context: Optional[Dict[str, Any]] = None,
    ):
        """
        ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²

        Args:
            name: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å
            value: å€¤
            tags: ã‚¿ã‚°
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        """
        with self._lock:
            metric = MetricData(
                name=name,
                value=value,
                timestamp=datetime.now(),
                tags=tags or {},
                context=context or {},
            )
            self._metrics[name].append(metric)

    def record_performance(
        self,
        operation: str,
        duration_ms: float,
        memory_mb: float = 0.0,
        cpu_percent: float = 0.0,
        success: bool = True,
        error_message: Optional[str] = None,
    ):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²

        Args:
            operation: æ“ä½œå
            duration_ms: å‡¦ç†æ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰
            memory_mb: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
            cpu_percent: CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰
            success: æˆåŠŸãƒ•ãƒ©ã‚°
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        with self._lock:
            perf_metric = PerformanceMetrics(
                operation=operation,
                duration_ms=duration_ms,
                memory_mb=memory_mb,
                cpu_percent=cpu_percent,
                success=success,
                timestamp=datetime.now(),
                error_message=error_message,
            )
            self._performance_metrics.append(perf_metric)

            # æ“ä½œã‚«ã‚¦ãƒ³ãƒˆ
            self._operation_counts[operation] += 1

            # ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆ
            if not success:
                self._error_counts[operation] += 1

    def record_error(self, operation: str, error_type: str, error_message: str):
        """
        ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²

        Args:
            operation: æ“ä½œå
            error_type: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        with self._lock:
            self._error_counts[f"{operation}_{error_type}"] += 1

        # ãƒ­ã‚°ã«ã‚‚å‡ºåŠ›
        logger.error(
            f"Operation: {operation}, Error: {error_type}, Message: {error_message}"
        )

    def record_model_evaluation_metrics(
        self,
        model_name: str,
        model_type: str,
        evaluation_metrics: Dict[str, Any],
        dataset_info: Optional[Dict[str, Any]] = None,
        training_params: Optional[Dict[str, Any]] = None,
    ):
        """
        ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—
            evaluation_metrics: è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            dataset_info: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
            training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        with self._lock:
            # å€‹åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚‚è¨˜éŒ²
            for metric_name, value in evaluation_metrics.items():
                if isinstance(value, (int, float)):
                    self.record_metric(
                        name=metric_name,
                        value=float(value),
                        tags={
                            "model_name": model_name,
                            "model_type": model_type,
                            "metric_category": "model_evaluation",
                        },
                        context={
                            "dataset_info": dataset_info or {},
                            "training_params": training_params or {},
                        },
                    )

            # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¨ä½“ã‚’è¨˜éŒ²
            model_eval_metric = ModelEvaluationMetrics(
                model_name=model_name,
                model_type=model_type,
                metrics=evaluation_metrics,
                timestamp=datetime.now(),
                dataset_info=dataset_info or {},
                training_params=training_params or {},
            )
            self._model_evaluation_metrics.append(model_eval_metric)

    @safe_operation(context="çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡", is_api_call=False, default_return={})
    def evaluate_and_record_model(
        self,
        model_name: str,
        model_type: str,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None,
        class_names: Optional[List[str]] = None,
        dataset_info: Optional[Dict[str, Any]] = None,
        training_params: Optional[Dict[str, Any]] = None,
        training_time: Optional[float] = None,
        memory_usage: Optional[float] = None,
    ) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€çµæœã‚’è¨˜éŒ²

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—
            y_true: çœŸã®ãƒ©ãƒ™ãƒ«
            y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
            y_proba: äºˆæ¸¬ç¢ºç‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            class_names: ã‚¯ãƒ©ã‚¹åã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            dataset_info: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            training_params: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            training_time: å­¦ç¿’æ™‚é–“ï¼ˆç§’ï¼‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            memory_usage: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        logger.info(f"ğŸ“Š çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡é–‹å§‹: {model_name} ({model_type})")

        # åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚’å®Ÿè¡Œ
        evaluation_metrics = self.calculate_comprehensive_metrics(
            y_true=y_true,
            y_pred=y_pred,
            y_proba=y_proba,
            class_names=class_names,
        )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’è¿½åŠ 
        if training_time is not None:
            evaluation_metrics["training_time"] = training_time
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã—ã¦ã‚‚è¨˜éŒ²
            self.record_performance(
                operation=f"model_training_{model_type}",
                duration_ms=training_time * 1000,  # ç§’ã‚’ãƒŸãƒªç§’ã«å¤‰æ›
                memory_mb=memory_usage or 0.0,
                success=True,
            )

        if memory_usage is not None:
            evaluation_metrics["memory_usage"] = memory_usage

        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœã‚’è¨˜éŒ²
        self.record_model_evaluation_metrics(
            model_name=model_name,
            model_type=model_type,
            evaluation_metrics=evaluation_metrics,
            dataset_info=dataset_info,
            training_params=training_params,
        )

        # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°å‡ºåŠ›
        accuracy = evaluation_metrics.get("accuracy", 0.0)
        f1_score = evaluation_metrics.get("f1_score", 0.0)
        balanced_accuracy = evaluation_metrics.get("balanced_accuracy", 0.0)

        logger.info(
            f"âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Œäº†: {model_name} - "
            f"ç²¾åº¦={accuracy:.4f}, F1={f1_score:.4f}, ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦={balanced_accuracy:.4f}"
        )

        return evaluation_metrics

    def _calculate_basic_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, float]:
        """åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # æ¨™æº–ç²¾åº¦
            metrics["accuracy"] = accuracy_score(y_true, y_pred)

            # ç²¾å¯†åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢
            precision, recall, f1, support = precision_recall_fscore_support(
                y_true,
                y_pred,
                average=self.config.average_method,
                zero_division=self.config.zero_division,
            )

            metrics["precision"] = precision
            metrics["recall"] = recall
            metrics["f1_score"] = f1

            # ãƒã‚¯ãƒ­å¹³å‡ã‚‚è¨ˆç®—
            precision_macro, recall_macro, f1_macro, _ = (
                precision_recall_fscore_support(
                    y_true,
                    y_pred,
                    average="macro",
                    zero_division=self.config.zero_division,
                )
            )

            metrics["precision_macro"] = precision_macro
            metrics["recall_macro"] = recall_macro
            metrics["f1_score_macro"] = f1_macro

            # è¿½åŠ ã®åŸºæœ¬æŒ‡æ¨™
            metrics["matthews_corrcoef"] = matthews_corrcoef(y_true, y_pred)
            metrics["cohen_kappa"] = cohen_kappa_score(y_true, y_pred)

        except Exception as e:
            logger.warning(f"åŸºæœ¬æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_balanced_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, float]:
        """ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œæŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦ï¼ˆåˆ†æå ±å‘Šæ›¸ã§æ¨å¥¨ï¼‰
            metrics["balanced_accuracy"] = balanced_accuracy_score(y_true, y_pred)

            # ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ãç²¾åº¦
            sample_weight = self._calculate_class_weights(y_true)
            metrics["weighted_accuracy"] = accuracy_score(
                y_true, y_pred, sample_weight=sample_weight
            )

        except Exception as e:
            logger.warning(f"ãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_probability_metrics(
        self, y_true: np.ndarray, y_proba: np.ndarray
    ) -> Dict[str, float]:
        """ç¢ºç‡ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            n_classes = len(np.unique(y_true))

            if self.config.include_roc_auc:
                # ROC-AUC
                if n_classes == 2:
                    # äºŒå€¤åˆ†é¡
                    try:
                        y_prob_positive = (
                            y_proba[:, 1]
                            if y_proba.ndim > 1 and y_proba.shape[1] > 1
                            else y_proba.ravel()
                        )
                        metrics["roc_auc"] = roc_auc_score(y_true, y_prob_positive)
                    except ValueError as e:
                        logger.warning(f"äºŒå€¤åˆ†é¡ROC-AUCè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                        metrics["roc_auc"] = 0.0
                else:
                    # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        try:
                            metrics["roc_auc"] = roc_auc_score(
                                y_true, y_proba, multi_class="ovr", average="weighted"
                            )
                            # è¿½åŠ ã®è©³ç´°æŒ‡æ¨™
                            metrics["roc_auc_ovr"] = metrics["roc_auc"]
                            metrics["roc_auc_ovo"] = roc_auc_score(
                                y_true, y_proba, multi_class="ovo", average="weighted"
                            )
                        except ValueError as e:
                            logger.warning(f"å¤šã‚¯ãƒ©ã‚¹ROC-AUCè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                            metrics["roc_auc"] = 0.0
                            metrics["roc_auc_ovr"] = 0.0
                            metrics["roc_auc_ovo"] = 0.0

            if self.config.include_pr_auc:
                # PR-AUCï¼ˆåˆ†æå ±å‘Šæ›¸ã§æ¨å¥¨ï¼‰
                if n_classes == 2:
                    # äºŒå€¤åˆ†é¡
                    try:
                        y_prob_positive = (
                            y_proba[:, 1]
                            if y_proba.ndim > 1 and y_proba.shape[1] > 1
                            else y_proba.ravel()
                        )
                        metrics["pr_auc"] = average_precision_score(
                            y_true, y_prob_positive
                        )
                    except ValueError as e:
                        logger.warning(f"äºŒå€¤åˆ†é¡PR-AUCè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                        metrics["pr_auc"] = 0.0
                else:
                    # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ï¼šå„ã‚¯ãƒ©ã‚¹ã®PR-AUCã‚’è¨ˆç®—
                    try:
                        pr_aucs = []
                        for i in range(n_classes):
                            y_true_binary = (y_true == i).astype(int)
                            if (
                                np.sum(y_true_binary) > 0
                            ):  # ã‚¯ãƒ©ã‚¹ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¨ˆç®—
                                pr_auc = average_precision_score(
                                    y_true_binary, y_proba[:, i]
                                )
                                pr_aucs.append(pr_auc)
                                metrics[f"pr_auc_class_{i}"] = pr_auc
                            else:
                                metrics[f"pr_auc_class_{i}"] = 0.0

                        if pr_aucs:
                            metrics["pr_auc"] = np.mean(pr_aucs)  # ãƒ¡ã‚¤ãƒ³æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨
                            metrics["pr_auc_macro"] = np.mean(pr_aucs)
                            metrics["pr_auc_weighted"] = np.average(
                                pr_aucs,
                                weights=np.bincount(y_true)[np.bincount(y_true) > 0],
                            )
                        else:
                            metrics["pr_auc"] = 0.0
                            metrics["pr_auc_macro"] = 0.0
                            metrics["pr_auc_weighted"] = 0.0
                    except Exception as e:
                        logger.warning(f"å¤šã‚¯ãƒ©ã‚¹PR-AUCè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                        metrics["pr_auc"] = 0.0
                        metrics["pr_auc_macro"] = 0.0
                        metrics["pr_auc_weighted"] = 0.0

            # Log Loss
            try:
                metrics["log_loss"] = log_loss(y_true, y_proba)
            except ValueError as e:
                logger.warning(f"Log Lossè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                metrics["log_loss"] = 0.0

            # Brier Score
            if n_classes == 2:
                # äºŒå€¤åˆ†é¡
                try:
                    y_prob_positive = (
                        y_proba[:, 1]
                        if y_proba.ndim > 1 and y_proba.shape[1] > 1
                        else y_proba.ravel()
                    )
                    metrics["brier_score"] = brier_score_loss(y_true, y_prob_positive)
                except ValueError as e:
                    logger.warning(f"äºŒå€¤åˆ†é¡Brier Scoreè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                    metrics["brier_score"] = 0.0
            else:
                # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã§ã¯å„ã‚¯ãƒ©ã‚¹ã®Brier Scoreã‚’è¨ˆç®—ã—ã¦å¹³å‡
                try:
                    brier_scores = []
                    for i in range(n_classes):
                        y_true_binary = (y_true == i).astype(int)
                        brier_score = brier_score_loss(y_true_binary, y_proba[:, i])
                        brier_scores.append(brier_score)
                    metrics["brier_score"] = np.mean(brier_scores)
                except Exception as e:
                    logger.warning(f"å¤šã‚¯ãƒ©ã‚¹Brier Scoreè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                    metrics["brier_score"] = 0.0

        except Exception as e:
            logger.warning(f"ç¢ºç‡æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
            metrics.update(
                {
                    "log_loss": 0.0,
                    "brier_score": 0.0,
                }
            )

        return metrics

    def _calculate_confusion_matrix_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """æ··åŒè¡Œåˆ—é–¢é€£æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # æ··åŒè¡Œåˆ—
            cm = confusion_matrix(y_true, y_pred)
            metrics["confusion_matrix"] = cm.tolist()

            # æ­£è¦åŒ–ã•ã‚ŒãŸæ··åŒè¡Œåˆ—
            cm_normalized = confusion_matrix(y_true, y_pred, normalize="true")
            metrics["confusion_matrix_normalized"] = cm_normalized.tolist()

            # ã‚¯ãƒ©ã‚¹åãŒã‚ã‚Œã°è¿½åŠ 
            if class_names:
                metrics["class_names"] = class_names

            # æ··åŒè¡Œåˆ—ã‹ã‚‰æ´¾ç”Ÿã™ã‚‹æŒ‡æ¨™
            if cm.shape[0] == 2:  # äºŒå€¤åˆ†é¡ã®å ´åˆ
                tn, fp, fn, tp = cm.ravel()
                metrics["true_negatives"] = int(tn)
                metrics["false_positives"] = int(fp)
                metrics["false_negatives"] = int(fn)
                metrics["true_positives"] = int(tp)

                # ç‰¹ç•°åº¦
                metrics["specificity"] = tn / (tn + fp) if (tn + fp) > 0 else 0.0
                # æ„Ÿåº¦ï¼ˆå†ç¾ç‡ã¨åŒã˜ï¼‰
                metrics["sensitivity"] = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                # NPV, PPV
                metrics["npv"] = tn / (tn + fn) if (tn + fn) > 0 else 0.0
                metrics["ppv"] = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            else:  # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã®å ´åˆï¼ˆmultilabel_confusion_matrixã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
                labels = np.unique(y_true)
                mcm = multilabel_confusion_matrix(y_true, y_pred, labels=labels)
                tn = mcm[:, 0, 0].astype(float)
                fp = mcm[:, 0, 1].astype(float)
                fn = mcm[:, 1, 0].astype(float)
                tp = mcm[:, 1, 1].astype(float)

                eps = 1e-12
                specificity_vec = tn / (tn + fp + eps)
                sensitivity_vec = tp / (tp + fn + eps)
                npv_vec = tn / (tn + fn + eps)
                ppv_vec = tp / (tp + fp + eps)

                # é‡ã¿ä»˜ãå¹³å‡ï¼ˆã‚¯ãƒ©ã‚¹é »åº¦ã§é‡ã¿ä»˜ã‘ï¼‰
                counts = np.array(
                    [(y_true == lab).sum() for lab in labels], dtype=float
                )
                weights = counts / (counts.sum() + eps)

                metrics["specificity"] = float(
                    np.average(specificity_vec, weights=weights)
                )
                metrics["sensitivity"] = float(
                    np.average(sensitivity_vec, weights=weights)
                )
                metrics["npv"] = float(np.average(npv_vec, weights=weights))
                metrics["ppv"] = float(np.average(ppv_vec, weights=weights))

        except Exception as e:
            logger.warning(f"æ··åŒè¡Œåˆ—è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_classification_report(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆè¾æ›¸å½¢å¼ï¼‰
            report = classification_report(
                y_true,
                y_pred,
                target_names=class_names,
                output_dict=True,
                zero_division=self.config.zero_division,
            )
            metrics["classification_report"] = report

        except Exception as e:
            logger.warning(f"åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_per_class_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        class_names: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹åˆ¥è©³ç´°æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # ã‚¯ãƒ©ã‚¹åˆ¥ã®ç²¾å¯†åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢
            # typing.castã‚’ä½¿ã£ã¦å‹ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã€å‹ãƒã‚§ãƒƒã‚«ãƒ¼ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±º
            from typing import cast

            precision, recall, f1_score, support = cast(
                tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],
                precision_recall_fscore_support(
                    y_true,
                    y_pred,
                    average=None,
                    zero_division=self.config.zero_division,
                ),
            )

            # å„å¤‰æ•°ã‚’æ˜ç¤ºçš„ã«numpyé…åˆ—ã¨ã—ã¦æ‰±ã†
            precision_array: np.ndarray = precision
            recall_array: np.ndarray = recall
            f1_array: np.ndarray = f1_score  # f1 -> f1_score ã§ã‚¹ãƒšãƒ«ãƒã‚§ãƒƒã‚¯å•é¡Œã‚’è§£æ±º
            support_array: np.ndarray = support

            classes = np.unique(y_true)
            per_class_metrics = {}

            for i, class_label in enumerate(classes):
                # class_namesã®å‹ã‚’æ˜ç¤ºçš„ã«ãƒã‚§ãƒƒã‚¯
                if class_names is not None and i < len(class_names):
                    class_name = class_names[i]
                else:
                    class_name = f"class_{class_label}"

                per_class_metrics[class_name] = {
                    "precision": float(precision_array[i]),
                    "recall": float(recall_array[i]),
                    "f1_score": float(f1_array[i]),  # f1 -> f1_array ã«ä¿®æ­£
                    "support": int(support_array[i]),
                }

            metrics["per_class_metrics"] = per_class_metrics

        except Exception as e:
            logger.warning(f"ã‚¯ãƒ©ã‚¹åˆ¥æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_distribution_metrics(
        self, y_true: np.ndarray, y_pred: np.ndarray
    ) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒé–¢é€£æŒ‡æ¨™ã‚’è¨ˆç®—"""
        metrics = {}

        try:
            # çœŸã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            true_counts = np.bincount(y_true)
            true_distribution = true_counts / len(y_true)
            metrics["true_label_distribution"] = true_distribution.tolist()

            # äºˆæ¸¬ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
            pred_counts = np.bincount(y_pred, minlength=len(true_counts))
            pred_distribution = pred_counts / len(y_pred)
            metrics["predicted_label_distribution"] = pred_distribution.tolist()

            # ä¸å‡è¡¡æ¯”ç‡
            max_class_ratio = np.max(true_distribution) / np.min(
                true_distribution[true_distribution > 0]
            )
            metrics["class_imbalance_ratio"] = float(max_class_ratio)

            # ã‚µãƒ³ãƒ—ãƒ«æ•°
            metrics["total_samples"] = len(y_true)
            metrics["n_classes"] = len(true_counts)

        except Exception as e:
            logger.warning(f"åˆ†å¸ƒæŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return metrics

    def _calculate_class_weights(self, y_true: np.ndarray) -> np.ndarray:
        """ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—"""
        classes, counts = np.unique(y_true, return_counts=True)
        total_samples = len(y_true)
        n_classes = len(classes)

        # ãƒãƒ©ãƒ³ã‚¹é‡ã¿è¨ˆç®—
        weights = total_samples / (n_classes * counts)

        # å„ã‚µãƒ³ãƒ—ãƒ«ã®é‡ã¿ã‚’è¨ˆç®—
        sample_weights = np.zeros(len(y_true))
        for i, class_label in enumerate(classes):
            mask = y_true == class_label
            sample_weights[mask] = weights[i]

        return sample_weights


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆçµ±åˆã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—å™¨ãƒ»åé›†å™¨ï¼‰
metrics_collector = MetricsCalculator()
