"""
MLç®¡ç† ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚µãƒ¼ãƒ“ã‚¹
"""

import logging
from typing import List, Dict, Any
import os
from urllib.parse import unquote
from datetime import datetime
from fastapi import HTTPException

from app.services.ml.model_manager import model_manager
from app.services.auto_strategy.services.ml_orchestrator import MLOrchestrator

from app.services.ml.feature_engineering.automl_feature_analyzer import (
    AutoMLFeatureAnalyzer,
)

from app.services.ml.config.ml_config_manager import ml_config_manager


logger = logging.getLogger(__name__)


class MLManagementOrchestrationService:
    """
    MLç®¡ç†æ©Ÿèƒ½ã®ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã‚’é›†ç´„ã—ãŸã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹
    """

    def __init__(self):
        self.ml_orchestrator = MLOrchestrator()

    async def get_formatted_models(self) -> Dict[str, List[Dict[str, Any]]]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§ã‚’å–å¾—ã—ã€ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰è¡¨ç¤ºç”¨ã«æ•´å½¢ã™ã‚‹
        """
        models = model_manager.list_models("*")

        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’æ•´å½¢
        formatted_models = []
        for model in models:
            # åŸºæœ¬æƒ…å ±
            model_info = {
                "id": model["name"],
                "name": model["name"],
                "path": model["path"],
                "size_mb": model["size_mb"],
                "modified_at": model["modified_at"].isoformat(),
                "directory": model["directory"],
                "is_active": self._is_active_model(model),  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®š
            }

            # ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
            try:
                model_data = model_manager.load_model(model["path"])
                if model_data and "metadata" in model_data:
                    metadata = model_data["metadata"]

                    # æ€§èƒ½æŒ‡æ¨™ã‚’è¿½åŠ 
                    model_info.update(
                        {
                            "accuracy": metadata.get("accuracy", 0.0),
                            "precision": metadata.get("precision", 0.0),
                            "recall": metadata.get("recall", 0.0),
                            "f1_score": metadata.get("f1_score", 0.0),
                            "feature_count": metadata.get("feature_count", 0),
                            "model_type": metadata.get("model_type", "LightGBM"),
                            "training_samples": metadata.get("training_samples", 0),
                        }
                    )

                    # classification_reportã‹ã‚‰è©³ç´°æŒ‡æ¨™ã‚’æŠ½å‡º
                    if "classification_report" in metadata:
                        report = metadata["classification_report"]
                        if isinstance(report, dict) and "macro avg" in report:
                            macro_avg = report["macro avg"]
                            model_info.update(
                                {
                                    "precision": macro_avg.get(
                                        "precision", model_info.get("precision", 0.0)
                                    ),
                                    "recall": macro_avg.get(
                                        "recall", model_info.get("recall", 0.0)
                                    ),
                                    "f1_score": macro_avg.get(
                                        "f1-score", model_info.get("f1_score", 0.0)
                                    ),
                                }
                            )

                    logger.info(
                        f"âœ… ãƒ¢ãƒ‡ãƒ«è©³ç´°æƒ…å ±ã‚’å–å¾—: {model['name']} - ç²¾åº¦: {model_info.get('accuracy', 0.0):.3f}, F1: {model_info.get('f1_score', 0.0):.3f}, ç‰¹å¾´é‡: {model_info.get('feature_count', 0)}å€‹"
                    )

            except Exception as e:
                logger.warning(f"ãƒ¢ãƒ‡ãƒ«è©³ç´°æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ {model['name']}: {e}")
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                model_info.update(
                    {
                        "accuracy": 0.0,
                        "precision": 0.0,
                        "recall": 0.0,
                        "f1_score": 0.0,
                        "feature_count": 0,
                        "model_type": "Unknown",
                        "training_samples": 0,
                    }
                )

            formatted_models.append(model_info)

        return {"models": formatted_models}

    async def delete_model(self, model_id: str) -> Dict[str, str]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤
        """
        logger.info(f"ãƒ¢ãƒ‡ãƒ«å‰Šé™¤è¦æ±‚: {model_id}")

        decoded_model_id = unquote(model_id)

        models = model_manager.list_models("*")
        target_model = None

        for model in models:
            if model["name"] == decoded_model_id or model["name"] == model_id:
                target_model = model
                break

        if not target_model:
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {decoded_model_id}")
            logger.info(f"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {[m['name'] for m in models]}")
            raise HTTPException(
                status_code=404, detail=f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {decoded_model_id}"
            )

        if not os.path.exists(target_model["path"]):
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {target_model['path']}")
            raise HTTPException(status_code=404, detail="ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

        try:
            os.remove(target_model["path"])
            logger.info(f"ãƒ¢ãƒ‡ãƒ«å‰Šé™¤å®Œäº†: {decoded_model_id} -> {target_model['path']}")
            return {"message": "ãƒ¢ãƒ‡ãƒ«ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ"}
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            raise HTTPException(
                status_code=500, detail="ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ"
            )

    async def get_ml_status(self) -> Dict[str, Any]:
        """
        MLãƒ¢ãƒ‡ãƒ«ã®ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—
        """
        status = self.ml_orchestrator.get_model_status()

        latest_model = model_manager.get_latest_model("*")

        if latest_model and os.path.exists(latest_model):
            try:
                model_data = model_manager.load_model(latest_model)
                if model_data and "metadata" in model_data:
                    metadata = model_data["metadata"]
                    model_info = {
                        "accuracy": metadata.get("accuracy", 0.0),
                        "precision": metadata.get("precision", 0.0),
                        "recall": metadata.get("recall", 0.0),
                        "f1_score": metadata.get("f1_score", 0.0),
                        "auc_score": metadata.get("auc_score", 0.0),
                        "auc_roc": metadata.get("auc_roc", 0.0),
                        "auc_pr": metadata.get("auc_pr", 0.0),
                        "balanced_accuracy": metadata.get("balanced_accuracy", 0.0),
                        "matthews_corrcoef": metadata.get("matthews_corrcoef", 0.0),
                        "cohen_kappa": metadata.get("cohen_kappa", 0.0),
                        "specificity": metadata.get("specificity", 0.0),
                        "sensitivity": metadata.get("sensitivity", 0.0),
                        "npv": metadata.get("npv", 0.0),
                        "ppv": metadata.get("ppv", 0.0),
                        "log_loss": metadata.get("log_loss", 0.0),
                        "brier_score": metadata.get("brier_score", 0.0),
                        "model_type": metadata.get("model_type", "LightGBM"),
                        "last_updated": datetime.fromtimestamp(
                            os.path.getmtime(latest_model)
                        ).isoformat(),
                        "training_samples": metadata.get("training_samples", 0),
                        "test_samples": metadata.get("test_samples", 0),
                        "file_size_mb": os.path.getsize(latest_model) / (1024 * 1024),
                        "feature_count": metadata.get("feature_count", 0),
                        "num_classes": metadata.get("num_classes", 2),
                        "best_iteration": metadata.get("best_iteration", 0),
                        "train_test_split": metadata.get("train_test_split", 0.8),
                        "random_state": metadata.get("random_state", 42),
                        "feature_importance": metadata.get("feature_importance", {}),
                        "classification_report": metadata.get(
                            "classification_report", {}
                        ),
                    }

                    if "classification_report" in metadata:
                        report = metadata["classification_report"]
                        if isinstance(report, dict) and "macro avg" in report:
                            macro_avg = report["macro avg"]
                            if model_info["precision"] == 0.0:
                                model_info["precision"] = macro_avg.get(
                                    "precision", 0.0
                                )
                            if model_info["recall"] == 0.0:
                                model_info["recall"] = macro_avg.get("recall", 0.0)
                            if model_info["f1_score"] == 0.0:
                                model_info["f1_score"] = macro_avg.get("f1-score", 0.0)

                    logger.info(
                        f"ğŸ“Š ML Status API - ãƒ¢ãƒ‡ãƒ«è©³ç´°æƒ…å ±ã‚’å–å¾—: ç²¾åº¦={model_info['accuracy']:.4f}, F1={model_info['f1_score']:.4f}, ç‰¹å¾´é‡={model_info['feature_count']}å€‹"
                    )
                else:
                    model_info = {
                        "accuracy": 0.0,
                        "model_type": "LightGBM",
                        "last_updated": datetime.fromtimestamp(
                            os.path.getmtime(latest_model)
                        ).isoformat(),
                        "training_samples": 0,
                        "file_size_mb": os.path.getsize(latest_model) / (1024 * 1024),
                        "feature_count": 0,
                    }
                status["model_info"] = model_info

                # ModelManagerã‹ã‚‰ç›´æ¥ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                model_data = model_manager.load_model(latest_model)
                if model_data and "metadata" in model_data:
                    metadata = model_data["metadata"]
                    # æ–°ã—ã„å½¢å¼ã®æ€§èƒ½æŒ‡æ¨™ã‚’æŠ½å‡ºï¼ˆå…¨ã¦ã®è©•ä¾¡æŒ‡æ¨™ã‚’å«ã‚€ï¼‰
                    performance_metrics = {
                        # åŸºæœ¬æŒ‡æ¨™
                        "accuracy": metadata.get("accuracy", 0.0),
                        "precision": metadata.get("precision", 0.0),
                        "recall": metadata.get("recall", 0.0),
                        "f1_score": metadata.get("f1_score", 0.0),
                        # AUCæŒ‡æ¨™
                        "auc_roc": metadata.get("auc_roc", 0.0),
                        "auc_pr": metadata.get("auc_pr", 0.0),
                        # é«˜åº¦ãªæŒ‡æ¨™
                        "balanced_accuracy": metadata.get("balanced_accuracy", 0.0),
                        "matthews_corrcoef": metadata.get("matthews_corrcoef", 0.0),
                        "cohen_kappa": metadata.get("cohen_kappa", 0.0),
                        # å°‚é–€æŒ‡æ¨™
                        "specificity": metadata.get("specificity", 0.0),
                        "sensitivity": metadata.get("sensitivity", 0.0),
                        "npv": metadata.get("npv", 0.0),
                        "ppv": metadata.get("ppv", 0.0),
                        # ç¢ºç‡æŒ‡æ¨™
                        "log_loss": metadata.get("log_loss", 0.0),
                        "brier_score": metadata.get("brier_score", 0.0),
                        # ãã®ä»–
                        "loss": metadata.get("loss", 0.0),
                        "val_accuracy": metadata.get("val_accuracy", 0.0),
                        "val_loss": metadata.get("val_loss", 0.0),
                        "training_time": metadata.get("training_time", 0.0),
                    }
                    status["performance_metrics"] = performance_metrics
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                    status["performance_metrics"] = {
                        "accuracy": 0.0,
                        "precision": 0.0,
                        "recall": 0.0,
                        "f1_score": 0.0,
                        "auc_roc": 0.0,
                        "auc_pr": 0.0,
                        "balanced_accuracy": 0.0,
                        "matthews_corrcoef": 0.0,
                        "cohen_kappa": 0.0,
                    }
            except Exception as e:
                logger.warning(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                status["model_info"] = {
                    "accuracy": 0.0,
                    "model_type": "Unknown",
                    "last_updated": datetime.fromtimestamp(
                        os.path.getmtime(latest_model)
                    ).isoformat(),
                    "training_samples": 0,
                    "file_size_mb": os.path.getsize(latest_model) / (1024 * 1024),
                    "feature_count": 0,
                }
                status["performance_metrics"] = {
                    "accuracy": 0.0,
                    "precision": 0.0,
                    "recall": 0.0,
                    "f1_score": 0.0,
                    "auc_score": 0.0,
                    "auc_roc": 0.0,
                    "auc_pr": 0.0,
                    "balanced_accuracy": 0.0,
                    "matthews_corrcoef": 0.0,
                    "cohen_kappa": 0.0,
                    "specificity": 0.0,
                    "sensitivity": 0.0,
                    "npv": 0.0,
                    "ppv": 0.0,
                    "log_loss": 0.0,
                    "brier_score": 0.0,
                    "loss": 0.0,
                    "val_accuracy": 0.0,
                    "val_loss": 0.0,
                    "training_time": 0.0,
                }

        return status

    async def get_feature_importance(self, top_n: int = 10) -> Dict[str, Any]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
        """
        feature_importance = self.ml_orchestrator.get_feature_importance(top_n)
        return {"feature_importance": feature_importance}

    async def get_automl_feature_analysis(self, top_n: int = 20) -> Dict[str, Any]:
        """
        AutoMLç‰¹å¾´é‡åˆ†æçµæœã‚’å–å¾—
        """
        feature_importance = self.ml_orchestrator.get_feature_importance(100)
        if not feature_importance:
            return {"error": "ç‰¹å¾´é‡é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}

        analyzer = AutoMLFeatureAnalyzer()
        analysis_result = analyzer.analyze_feature_importance(feature_importance, top_n)
        return analysis_result

    async def cleanup_old_models(self) -> Dict[str, str]:
        """
        å¤ã„ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        """
        model_manager.cleanup_expired_models()
        return {"message": "å¤ã„ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ"}

    def get_ml_config_dict(self) -> Dict[str, Any]:
        """
        MLè¨­å®šã‚’è¾æ›¸å½¢å¼ã§å–å¾—

        Returns:
            MLè¨­å®šè¾æ›¸
        """
        return ml_config_manager.get_config_dict()

    async def update_ml_config(self, config_updates: Dict[str, Any]) -> Dict[str, Any]:
        """
        MLè¨­å®šã‚’æ›´æ–°

        Args:
            config_updates: æ›´æ–°ã™ã‚‹è¨­å®šé …ç›®

        Returns:
            æ›´æ–°çµæœ
        """
        try:
            success = ml_config_manager.update_config(config_updates)

            if success:
                return {
                    "success": True,
                    "message": "MLè¨­å®šãŒæ­£å¸¸ã«æ›´æ–°ã•ã‚Œã¾ã—ãŸ",
                    "updated_config": ml_config_manager.get_config_dict(),
                }
            else:
                return {"success": False, "message": "MLè¨­å®šã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ"}

        except Exception as e:
            logger.error(f"MLè¨­å®šæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "message": f"MLè¨­å®šã®æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
            }

    async def reset_ml_config(self) -> Dict[str, Any]:
        """
        MLè¨­å®šã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«ãƒªã‚»ãƒƒãƒˆ

        Returns:
            ãƒªã‚»ãƒƒãƒˆçµæœ
        """
        try:
            success = ml_config_manager.reset_config()

            if success:
                return {
                    "success": True,
                    "message": "MLè¨­å®šãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã—ãŸ",
                    "config": ml_config_manager.get_config_dict(),
                }
            else:
                return {"success": False, "message": "MLè¨­å®šã®ãƒªã‚»ãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ"}

        except Exception as e:
            logger.error(f"MLè¨­å®šãƒªã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "message": f"MLè¨­å®šã®ãƒªã‚»ãƒƒãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
            }

    def _is_active_model(self, model: Dict[str, Any]) -> bool:
        """
        ãƒ¢ãƒ‡ãƒ«ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‹ã©ã†ã‹ã‚’åˆ¤å®š

        Args:
            model: ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¾æ›¸

        Returns:
            ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã®å ´åˆã¯Trueã€ãã†ã§ãªã„å ´åˆã¯False
        """
        try:
            # æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ¯”è¼ƒã—ã¦ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‹åˆ¤å®š
            latest_model = model_manager.get_latest_model("*")
            if latest_model:
                # ãƒ‘ã‚¹ãŒä¸€è‡´ã™ã‚‹å ´åˆã¯ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã¨åˆ¤å®š
                return model["path"] == latest_model

            # ãƒ¢ãƒ‡ãƒ«ãŒ1ã¤ã ã‘ã®å ´åˆã¯ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã¨åˆ¤å®š
            all_models = model_manager.list_models("*")
            return len(all_models) == 1 and all_models[0]["path"] == model["path"]

        except Exception as e:
            logger.warning(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
