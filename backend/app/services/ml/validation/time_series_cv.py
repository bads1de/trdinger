"""
æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

åˆ†æå ±å‘Šæ›¸ã§ææ¡ˆã•ã‚ŒãŸæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•ã‚’å®Ÿè£…ã€‚
ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ãã€å®Ÿéš›ã®å–å¼•ç’°å¢ƒã«è¿‘ã„è©•ä¾¡ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import logging
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    roc_auc_score,
)
from sklearn.model_selection import TimeSeriesSplit

logger = logging.getLogger(__name__)


class CVStrategy(Enum):
    """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥"""

    TIME_SERIES_SPLIT = "time_series_split"
    WALK_FORWARD = "walk_forward"
    PURGED_CV = "purged_cv"
    EXPANDING_WINDOW = "expanding_window"


@dataclass
class CVConfig:
    """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š"""

    strategy: CVStrategy = CVStrategy.TIME_SERIES_SPLIT
    n_splits: int = 5
    max_train_size: Optional[int] = None
    test_size: Optional[int] = None
    gap: int = 0  # Purged CVã§ã®ã‚®ãƒ£ãƒƒãƒ—
    min_train_size: int = 100  # æœ€å°å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°


class TimeSeriesCrossValidator:
    """
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼

    ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ãã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸ
    å …ç‰¢ãªã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚
    """

    def __init__(self, config: CVConfig = None):
        """
        åˆæœŸåŒ–

        Args:
            config: ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
        """
        self.config = config or CVConfig()
        self.cv_results = []

    def cross_validate(
        self,
        model,
        X: pd.DataFrame,
        y: pd.Series,
        scoring: List[str] = None,
        **fit_params,
    ) -> Dict[str, Any]:
        """
        æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ

        Args:
            model: å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
            X: ç‰¹å¾´é‡DataFrame
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆSeries
            scoring: è©•ä¾¡æŒ‡æ¨™ã®ãƒªã‚¹ãƒˆ
            **fit_params: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
        """
        if scoring is None:
            scoring = ["accuracy", "precision", "recall", "f1", "roc_auc"]

        logger.info(f"ğŸ”„ æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹: {self.config.strategy.value}")
        logger.info(f"åˆ†å‰²æ•°: {self.config.n_splits}, ãƒ‡ãƒ¼ã‚¿æ•°: {len(X)}")

        # ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        self._validate_data(X, y)

        # åˆ†å‰²æˆ¦ç•¥ã«å¿œã˜ã¦ã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼ã‚’ä½œæˆ
        splitter = self._create_splitter()

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        fold_results = []
        scores = {metric: [] for metric in scoring}

        for fold, (train_idx, test_idx) in enumerate(splitter.split(X), 1):
            logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold}/{self.config.n_splits} ã‚’å®Ÿè¡Œä¸­...")

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train = X.iloc[train_idx]
            X_test = X.iloc[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]

            # åˆ†å‰²çµæœã®æ¤œè¨¼
            if len(X_train) < self.config.min_train_size:
                logger.warning(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold}: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ ({len(X_train)})")
                continue

            if len(y_train.unique()) < 2:
                logger.warning(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold}: ãƒ©ãƒ™ãƒ«ã®ç¨®é¡ãŒä¸è¶³")
                continue

            try:
                # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                if hasattr(model, "fit"):
                    model.fit(X_train, y_train, **fit_params)
                else:
                    # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                    model.train(X_train, y_train, **fit_params)

                # äºˆæ¸¬
                if hasattr(model, "predict"):
                    y_pred = model.predict(X_test)
                    if hasattr(model, "predict_proba"):
                        y_proba = model.predict_proba(X_test)
                    else:
                        y_proba = None
                else:
                    # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                    predictions = model.predict(X_test)
                    if isinstance(predictions, dict):
                        y_pred = predictions.get("predictions")
                        y_proba = predictions.get("probabilities")
                    else:
                        y_pred = predictions
                        y_proba = None

                # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
                fold_scores = self._calculate_scores(y_test, y_pred, y_proba, scoring)

                # çµæœè¨˜éŒ²
                fold_result = {
                    "fold": fold,
                    "train_size": len(X_train),
                    "test_size": len(X_test),
                    "train_period": f"{X_train.index[0]} ï½ {X_train.index[-1]}",
                    "test_period": f"{X_test.index[0]} ï½ {X_test.index[-1]}",
                    **fold_scores,
                }

                fold_results.append(fold_result)

                # ã‚¹ã‚³ã‚¢é›†è¨ˆ
                for metric in scoring:
                    if metric in fold_scores:
                        scores[metric].append(fold_scores[metric])

                logger.info(
                    f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold} å®Œäº†: ç²¾åº¦={fold_scores.get('accuracy', 0.0):.4f}"
                )

            except Exception as e:
                logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # çµæœé›†è¨ˆ
        cv_result = self._aggregate_results(scores, fold_results)
        self.cv_results = fold_results

        logger.info("æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†:")
        for metric, values in scores.items():
            if values:
                mean_score = np.mean(values)
                std_score = np.std(values)
                logger.info(f"  {metric}: {mean_score:.4f} Â± {std_score:.4f}")

        return cv_result

    def _create_splitter(self):
        """åˆ†å‰²æˆ¦ç•¥ã«å¿œã˜ã¦ã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼ã‚’ä½œæˆ"""
        if self.config.strategy == CVStrategy.TIME_SERIES_SPLIT:
            return TimeSeriesSplit(
                n_splits=self.config.n_splits,
                max_train_size=self.config.max_train_size,
                test_size=self.config.test_size,
            )
        elif self.config.strategy == CVStrategy.WALK_FORWARD:
            return self._walk_forward_splitter()
        elif self.config.strategy == CVStrategy.PURGED_CV:
            return self._purged_cv_splitter()
        elif self.config.strategy == CVStrategy.EXPANDING_WINDOW:
            return self._expanding_window_splitter()
        else:
            raise ValueError(f"æœªå¯¾å¿œã®æˆ¦ç•¥: {self.config.strategy}")

    def _walk_forward_splitter(self):
        """ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰åˆ†æç”¨ã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼"""

        class WalkForwardSplitter:
            def __init__(self, n_splits, test_size):
                self.n_splits = n_splits
                self.test_size = test_size

            def split(self, X):
                n_samples = len(X)
                test_size = self.test_size or n_samples // (self.n_splits + 1)

                for i in range(self.n_splits):
                    test_start = n_samples - (self.n_splits - i) * test_size
                    test_end = test_start + test_size
                    train_end = test_start

                    if train_end < 100:  # æœ€å°å­¦ç¿’ã‚µã‚¤ã‚º
                        continue

                    train_idx = np.arange(0, train_end)
                    test_idx = np.arange(test_start, min(test_end, n_samples))

                    yield train_idx, test_idx

        return WalkForwardSplitter(self.config.n_splits, self.config.test_size)

    def _purged_cv_splitter(self):
        """ãƒ‘ãƒ¼ã‚¸ãƒ‰ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼"""

        class PurgedCVSplitter:
            def __init__(self, n_splits, gap):
                self.n_splits = n_splits
                self.gap = gap

            def split(self, X):
                n_samples = len(X)
                test_size = n_samples // self.n_splits

                for i in range(self.n_splits):
                    test_start = i * test_size
                    test_end = min((i + 1) * test_size, n_samples)

                    # ã‚®ãƒ£ãƒƒãƒ—ã‚’è€ƒæ…®ã—ãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿
                    train_idx = np.concatenate(
                        [
                            np.arange(0, max(0, test_start - self.gap)),
                            np.arange(min(test_end + self.gap, n_samples), n_samples),
                        ]
                    )
                    test_idx = np.arange(test_start, test_end)

                    if len(train_idx) < 100:  # æœ€å°å­¦ç¿’ã‚µã‚¤ã‚º
                        continue

                    yield train_idx, test_idx

        return PurgedCVSplitter(self.config.n_splits, self.config.gap)

    def _expanding_window_splitter(self):
        """æ‹¡å¼µã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç”¨ã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼"""

        class ExpandingWindowSplitter:
            def __init__(self, n_splits, test_size):
                self.n_splits = n_splits
                self.test_size = test_size

            def split(self, X):
                n_samples = len(X)
                test_size = self.test_size or n_samples // (self.n_splits + 1)

                for i in range(1, self.n_splits + 1):
                    test_start = n_samples - (self.n_splits - i + 1) * test_size
                    test_end = test_start + test_size
                    train_end = test_start

                    train_idx = np.arange(0, train_end)
                    test_idx = np.arange(test_start, min(test_end, n_samples))

                    if len(train_idx) < 100:  # æœ€å°å­¦ç¿’ã‚µã‚¤ã‚º
                        continue

                    yield train_idx, test_idx

        return ExpandingWindowSplitter(self.config.n_splits, self.config.test_size)

    def _validate_data(self, X: pd.DataFrame, y: pd.Series):
        """ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        if len(X) != len(y):
            raise ValueError("ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é•·ã•ãŒä¸€è‡´ã—ã¾ã›ã‚“")

        if len(X) < self.config.n_splits * self.config.min_train_size:
            raise ValueError(
                f"ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {len(X)} < {self.config.n_splits * self.config.min_train_size}"
            )

        if not isinstance(X.index, pd.DatetimeIndex):
            logger.warning("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒDatetimeIndexã§ã¯ã‚ã‚Šã¾ã›ã‚“")

    def _calculate_scores(
        self,
        y_true: pd.Series,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray],
        scoring: List[str],
    ) -> Dict[str, float]:
        """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
        scores = {}

        try:
            if "accuracy" in scoring:
                scores["accuracy"] = accuracy_score(y_true, y_pred)

            if any(metric in scoring for metric in ["precision", "recall", "f1"]):
                precision, recall, f1, _ = precision_recall_fscore_support(
                    y_true, y_pred, average="weighted", zero_division=0
                )
                if "precision" in scoring:
                    scores["precision"] = precision
                if "recall" in scoring:
                    scores["recall"] = recall
                if "f1" in scoring:
                    scores["f1"] = f1

            if "roc_auc" in scoring and y_proba is not None:
                try:
                    if y_proba.ndim == 2 and y_proba.shape[1] > 2:
                        # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã®å ´åˆ
                        scores["roc_auc"] = roc_auc_score(
                            y_true, y_proba, multi_class="ovr", average="weighted"
                        )
                    else:
                        # äºŒå€¤åˆ†é¡ã®å ´åˆ
                        scores["roc_auc"] = roc_auc_score(
                            y_true, y_proba[:, 1] if y_proba.ndim == 2 else y_proba
                        )
                except Exception as e:
                    logger.warning(f"ROC-AUCè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                    scores["roc_auc"] = 0.0

        except Exception as e:
            logger.error(f"è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        return scores

    def _aggregate_results(
        self, scores: Dict[str, List[float]], fold_results: List[Dict]
    ) -> Dict[str, Any]:
        """çµæœã‚’é›†è¨ˆ"""
        aggregated = {
            "fold_results": fold_results,
            "n_splits": len(fold_results),
            "strategy": self.config.strategy.value,
        }

        for metric, values in scores.items():
            if values:
                aggregated[f"{metric}_scores"] = values
                aggregated[f"{metric}_mean"] = np.mean(values)
                aggregated[f"{metric}_std"] = np.std(values)
                aggregated[f"{metric}_min"] = np.min(values)
                aggregated[f"{metric}_max"] = np.max(values)

        return aggregated

    def get_best_fold(self, metric: str = "accuracy") -> Optional[Dict[str, Any]]:
        """æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—"""
        if not self.cv_results:
            return None

        best_fold = max(self.cv_results, key=lambda x: x.get(metric, 0))
        return best_fold

    def plot_cv_scores(self, metric: str = "accuracy"):
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå®Ÿè£…ã¯çœç•¥ï¼‰"""
        # å®Ÿè£…ã¯å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
