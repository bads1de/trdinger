"""
Ridgeãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼

ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã§ä½¿ç”¨ã™ã‚‹RidgeClassifierãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
scikit-learnã®RidgeClassifierã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å°‚ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, List
from sklearn.linear_model import RidgeClassifier
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    f1_score,
    matthews_corrcoef,
)

from ....utils.unified_error_handler import UnifiedModelError

logger = logging.getLogger(__name__)


class RidgeModel:
    """
    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å†…ã§ä½¿ç”¨ã™ã‚‹RidgeClassifierãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼

    scikit-learnã®RidgeClassifierã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å°‚ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
    æ³¨æ„: RidgeClassifierã¯predict_probaãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŒãŸãªã„ãŸã‚ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ã¯åˆ¶é™ã‚ã‚Š
    """

    def __init__(self, automl_config: Optional[Dict[str, Any]] = None):
        """
        åˆæœŸåŒ–

        Args:
            automl_config: AutoMLè¨­å®šï¼ˆç¾åœ¨ã¯æœªä½¿ç”¨ï¼‰
        """
        self.model = None
        self.is_trained = False
        self.feature_columns = None
        self.automl_config = automl_config

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.default_params = {
            "alpha": 1.0,
            "class_weight": "balanced",
            "random_state": 42,
            "max_iter": 1000,
        }

    def _train_model_impl(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        Ridgeãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            y_test: ãƒ†ã‚¹ãƒˆç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸
        """
        try:
            logger.info("ğŸ“ Ridgeãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹")

            # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’ä¿å­˜
            self.feature_columns = list(X_train.columns)

            # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            self.model = RidgeClassifier(**self.default_params)

            # å­¦ç¿’å®Ÿè¡Œ
            self.model.fit(X_train, y_train)

            # äºˆæ¸¬
            y_pred_train = self.model.predict(X_train)
            y_pred_test = self.model.predict(X_test)

            # åŸºæœ¬è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            train_accuracy = accuracy_score(y_train, y_pred_train)
            test_accuracy = accuracy_score(y_test, y_pred_test)
            train_balanced_acc = balanced_accuracy_score(y_train, y_pred_train)
            test_balanced_acc = balanced_accuracy_score(y_test, y_pred_test)
            train_f1 = f1_score(y_train, y_pred_train, average="weighted")
            test_f1 = f1_score(y_test, y_pred_test, average="weighted")

            # è¿½åŠ è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ï¼ˆAUCæŒ‡æ¨™ã¯é™¤å¤–ï¼šRidgeClassifierã¯predict_probaã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ï¼‰
            train_mcc = matthews_corrcoef(y_train, y_pred_train)
            test_mcc = matthews_corrcoef(y_test, y_pred_test)

            # ã‚¯ãƒ©ã‚¹æ•°ã‚’å–å¾—
            n_classes = len(np.unique(y_train))

            # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¿‚æ•°ã®çµ¶å¯¾å€¤ï¼‰
            if hasattr(self.model, "coef_"):
                if len(self.model.coef_.shape) > 1:
                    # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã®å ´åˆ
                    feature_importance = dict(
                        zip(
                            self.feature_columns,
                            np.mean(np.abs(self.model.coef_), axis=0),
                        )
                    )
                else:
                    # äºŒå€¤åˆ†é¡ã®å ´åˆ
                    feature_importance = dict(
                        zip(self.feature_columns, np.abs(self.model.coef_))
                    )
            else:
                feature_importance = {}

            self.is_trained = True

            results = {
                "algorithm": "ridge",
                # åŸºæœ¬æŒ‡æ¨™
                "train_accuracy": train_accuracy,
                "test_accuracy": test_accuracy,
                "accuracy": test_accuracy,  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã®çµ±ä¸€ã‚­ãƒ¼
                "train_balanced_accuracy": train_balanced_acc,
                "test_balanced_accuracy": test_balanced_acc,
                "balanced_accuracy": test_balanced_acc,  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã®çµ±ä¸€ã‚­ãƒ¼
                "train_f1_score": train_f1,
                "test_f1_score": test_f1,
                "f1_score": test_f1,  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã®çµ±ä¸€ã‚­ãƒ¼
                # è¿½åŠ æŒ‡æ¨™ï¼ˆAUCæŒ‡æ¨™ã¯é™¤å¤–ï¼‰
                "train_mcc": train_mcc,
                "test_mcc": test_mcc,
                "matthews_corrcoef": test_mcc,  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã®çµ±ä¸€ã‚­ãƒ¼
                # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
                "feature_importance": feature_importance,
                "alpha": self.model.alpha,
                "max_iter": self.model.max_iter,
                "feature_count": len(self.feature_columns),
                "train_samples": len(X_train),
                "test_samples": len(X_test),
                "num_classes": n_classes,
                "has_predict_proba": False,  # RidgeClassifierã¯ç¢ºç‡äºˆæ¸¬ãªã—
            }

            logger.info(f"âœ… Ridgeå­¦ç¿’å®Œäº† - ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.4f}")
            return results

        except Exception as e:
            logger.error(f"âŒ Ridgeå­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            raise UnifiedModelError(f"Ridgeå­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        äºˆæ¸¬ã‚’å®Ÿè¡Œ

        Args:
            X: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬çµæœ
        """
        if not self.is_trained or self.model is None:
            raise UnifiedModelError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # ç‰¹å¾´é‡ã®é †åºã‚’ç¢ºèª
            if self.feature_columns:
                X = X[self.feature_columns]

            predictions = self.model.predict(X)
            return predictions

        except Exception as e:
            logger.error(f"Ridgeäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            raise UnifiedModelError(f"äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—ï¼ˆRidgeClassifierã¯å¯¾å¿œã—ã¦ã„ãªã„ãŸã‚ä¾‹å¤–ã‚’ç™ºç”Ÿï¼‰

        Args:
            X: ç‰¹å¾´é‡DataFrame

        Returns:
            äºˆæ¸¬ç¢ºç‡ã®é…åˆ—

        Raises:
            UnifiedModelError: RidgeClassifierã¯ç¢ºç‡äºˆæ¸¬ã«å¯¾å¿œã—ã¦ã„ãªã„
        """
        raise UnifiedModelError(
            "RidgeClassifierã¯ç¢ºç‡äºˆæ¸¬ï¼ˆpredict_probaï¼‰ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“"
        )

    @property
    def feature_columns(self) -> List[str]:
        """ç‰¹å¾´é‡ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self._feature_columns

    @feature_columns.setter
    def feature_columns(self, columns: List[str]):
        """ç‰¹å¾´é‡ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆã‚’è¨­å®š"""
        self._feature_columns = columns

    def get_feature_importance(self) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ï¼ˆä¿‚æ•°ã®çµ¶å¯¾å€¤ï¼‰

        Returns:
            ç‰¹å¾´é‡é‡è¦åº¦ã®è¾æ›¸
        """
        if not self.is_trained or self.model is None:
            raise UnifiedModelError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        if not self.feature_columns:
            raise UnifiedModelError("ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        if hasattr(self.model, "coef_"):
            if len(self.model.coef_.shape) > 1:
                # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã®å ´åˆ
                importance = np.mean(np.abs(self.model.coef_), axis=0)
            else:
                # äºŒå€¤åˆ†é¡ã®å ´åˆ
                importance = np.abs(self.model.coef_)

            return dict(zip(self.feature_columns, importance))
        else:
            return {}

    def get_model_info(self) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—

        Returns:
            ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¾æ›¸
        """
        if not self.is_trained or self.model is None:
            return {"status": "not_trained"}

        return {
            "algorithm": "ridge",
            "alpha": self.model.alpha,
            "max_iter": self.model.max_iter,
            "class_weight": "balanced",
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "has_predict_proba": False,
            "status": "trained",
        }
