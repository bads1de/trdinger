"""
K-Nearest Neighborsãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼

ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã§ä½¿ç”¨ã™ã‚‹KNNãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
scikit-learnã®KNeighborsClassifierã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å°‚ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
"""

import logging
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors


from ....utils.unified_error_handler import UnifiedModelError

logger = logging.getLogger(__name__)


class KNNModel:
    """
    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å†…ã§ä½¿ç”¨ã™ã‚‹KNNãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼

    scikit-learnã®KNeighborsClassifierã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å°‚ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
    """

    # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åï¼ˆAlgorithmRegistryã‹ã‚‰å–å¾—ï¼‰
    ALGORITHM_NAME = "knn"

    def __init__(
        self,
        automl_config: Optional[Dict[str, Any]] = None,
        n_neighbors: int = 5,
        weights: str = "distance",
        algorithm: str = "auto",
        metric: str = "minkowski",
        p: int = 2,
        n_jobs: int = -1,
        **kwargs,
    ):
        """
        åˆæœŸåŒ–

        Args:
            automl_config: AutoMLè¨­å®šï¼ˆç¾åœ¨ã¯æœªä½¿ç”¨ï¼‰
            n_neighbors: è¿‘å‚æ•°
            weights: é‡ã¿ä»˜ã‘æ–¹æ³•
            algorithm: ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
            metric: è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯
            p: ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼è·é›¢ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            n_jobs: ä¸¦åˆ—å‡¦ç†æ•°
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        self.model = None
        self.is_trained = False
        self.feature_columns = None
        self.automl_config = automl_config
        self.classes_ = None  # sklearnäº’æ›æ€§ã®ãŸã‚

        # sklearnäº’æ›æ€§ã®ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.n_neighbors = n_neighbors
        self.weights = weights
        self.algorithm = algorithm
        self.metric = metric
        self.p = p
        self.n_jobs = n_jobs

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šï¼‰
        self.default_params = {
            "n_neighbors": self.n_neighbors,
            "weights": self.weights,
            "algorithm": self.algorithm,
            "metric": self.metric,
            "p": self.p,
            "n_jobs": self.n_jobs,
            "leaf_size": 30,  # åŠ¹ç‡çš„ãªæ¤œç´¢ã®ãŸã‚ã®ãƒªãƒ¼ãƒ•ã‚µã‚¤ã‚º
        }

        # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        for key, value in kwargs.items():
            setattr(self, key, value)

    def fit(self, X, y) -> "KNNModel":
        """
        sklearnäº’æ›ã®fitãƒ¡ã‚½ãƒƒãƒ‰

        Args:
            X: å­¦ç¿’ç”¨ç‰¹å¾´é‡ï¼ˆDataFrame or numpy arrayï¼‰
            y: å­¦ç¿’ç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆSeries or numpy arrayï¼‰

        Returns:
            self: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        """
        try:
            # numpyé…åˆ—ã‚’DataFrameã«å¤‰æ›
            if not isinstance(X, pd.DataFrame):
                if hasattr(self, "feature_columns") and self.feature_columns:
                    X = pd.DataFrame(X, columns=self.feature_columns)
                else:
                    X = pd.DataFrame(
                        X, columns=[f"feature_{i}" for i in range(X.shape[1])]
                    )

            if not isinstance(y, pd.Series):
                y = pd.Series(y)

            # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’ä¿å­˜
            self.feature_columns = list(X.columns)

            # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            self.model = KNeighborsClassifier(**self.default_params)

            # å­¦ç¿’å®Ÿè¡Œï¼ˆKNNã¯é…å»¶å­¦ç¿’ãªã®ã§å®Ÿéš›ã¯ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ã ã‘ï¼‰
            self.model.fit(X, y)

            # classes_å±æ€§ã‚’è¨­å®šï¼ˆsklearnäº’æ›æ€§ã®ãŸã‚ï¼‰
            self.classes_ = np.unique(y)
            self.is_trained = True

            return self

        except Exception as e:
            logger.error(f"sklearnäº’æ›fitå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise UnifiedModelError(f"KNNãƒ¢ãƒ‡ãƒ«ã®fitå®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def _train_model_impl(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        **training_params,
    ) -> Dict[str, Any]:
        """
        KNNãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

        Args:
            X_train: å­¦ç¿’ç”¨ç‰¹å¾´é‡
            X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
            y_train: å­¦ç¿’ç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            y_test: ãƒ†ã‚¹ãƒˆç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ

        Returns:
            å­¦ç¿’çµæœã®è¾æ›¸
        """
        try:
            logger.info("ğŸ” KNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹")

            # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’ä¿å­˜
            self.feature_columns = list(X_train.columns)

            # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            self.model = KNeighborsClassifier(**self.default_params)

            # å­¦ç¿’å®Ÿè¡Œï¼ˆKNNã¯é…å»¶å­¦ç¿’ãªã®ã§å®Ÿéš›ã¯ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ã ã‘ï¼‰
            self.model.fit(X_train, y_train)

            # äºˆæ¸¬
            y_pred_train = self.model.predict(X_train)
            y_pred_test = self.model.predict(X_test)

            # ç¢ºç‡äºˆæ¸¬ï¼ˆKNNã¯ç¢ºç‡äºˆæ¸¬ã‚’ã‚µãƒãƒ¼ãƒˆï¼‰
            y_pred_proba_train = self.model.predict_proba(X_train)
            y_pred_proba_test = self.model.predict_proba(X_test)

            # çµ±ä¸€ã•ã‚ŒãŸè©•ä¾¡æŒ‡æ¨™è¨ˆç®—å™¨ã‚’ä½¿ç”¨
            from ..evaluation.enhanced_metrics import (
                EnhancedMetricsCalculator,
                MetricsConfig,
            )

            config = MetricsConfig(
                include_balanced_accuracy=True,
                include_pr_auc=True,
                include_roc_auc=True,
                include_confusion_matrix=True,
                include_classification_report=True,
                average_method="weighted",
                zero_division=0,
            )

            metrics_calculator = EnhancedMetricsCalculator(config)

            # åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
            test_metrics = metrics_calculator.calculate_comprehensive_metrics(
                y_test, y_pred_test, y_pred_proba_test
            )

            # åŒ…æ‹¬çš„ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼‰
            train_metrics = metrics_calculator.calculate_comprehensive_metrics(
                y_train, y_pred_train, y_pred_proba_train
            )

            # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆKNNã§ã¯ç›´æ¥çš„ãªé‡è¦åº¦ãªã—ï¼‰
            # å„ç‰¹å¾´é‡ã®åˆ†æ•£ã‚’é‡è¦åº¦ã¨ã—ã¦è¿‘ä¼¼
            feature_variance = X_train.var()
            total_variance = feature_variance.sum()
            feature_importance = {}
            if total_variance > 0:
                feature_importance = dict(
                    zip(self.feature_columns, feature_variance / total_variance)
                )

            self.is_trained = True

            results = {
                "algorithm": self.ALGORITHM_NAME,
                "n_neighbors": self.model.n_neighbors,
                "weights": self.model.weights,
                "algorithm_type": self.model.algorithm,
                "metric": self.model.metric,
                "feature_count": len(self.feature_columns),
                "train_samples": len(X_train),
                "test_samples": len(X_test),
                "num_classes": (
                    len(self.model.classes_) if hasattr(self.model, "classes_") else 0
                ),
                "feature_importance": feature_importance,
            }

            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¿½åŠ ï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãï¼‰
            for key, value in test_metrics.items():
                if key not in ["error"]:  # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã¯é™¤å¤–
                    results[f"test_{key}"] = value
                    # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã®çµ±ä¸€ã‚­ãƒ¼ï¼ˆtest_ãªã—ã®ã‚­ãƒ¼ï¼‰
                    if key in [
                        "accuracy",
                        "balanced_accuracy",
                        "f1_score",
                        "matthews_corrcoef",
                    ]:
                        results[key] = value
                    elif key == "roc_auc" or key == "roc_auc_ovr":
                        results["auc_roc"] = value
                    elif key == "pr_auc" or key == "pr_auc_macro":
                        results["auc_pr"] = value

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¿½åŠ ï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãï¼‰
            for key, value in train_metrics.items():
                if key not in ["error"]:  # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã¯é™¤å¤–
                    results[f"train_{key}"] = value

            logger.info(
                f"âœ… KNNå­¦ç¿’å®Œäº† - ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_metrics.get('accuracy', 0.0):.4f}"
            )
            return results

        except Exception as e:
            logger.error(f"âŒ KNNå­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            raise UnifiedModelError(f"KNNå­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def predict(self, X) -> np.ndarray:
        """
        sklearnäº’æ›ã®äºˆæ¸¬ãƒ¡ã‚½ãƒƒãƒ‰

        Args:
            X: ç‰¹å¾´é‡ï¼ˆDataFrame or numpy arrayï¼‰

        Returns:
            äºˆæ¸¬çµæœ
        """
        if not self.is_trained or self.model is None:
            raise UnifiedModelError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # numpyé…åˆ—ã‚’DataFrameã«å¤‰æ›
            if not isinstance(X, pd.DataFrame):
                if hasattr(self, "feature_columns") and self.feature_columns:
                    X = pd.DataFrame(X, columns=self.feature_columns)
                else:
                    X = pd.DataFrame(
                        X, columns=[f"feature_{i}" for i in range(X.shape[1])]
                    )

            # ç‰¹å¾´é‡ã®é †åºã‚’ç¢ºèª
            if self.feature_columns:
                X = X[self.feature_columns]

            predictions = self.model.predict(X)
            return predictions

        except Exception as e:
            logger.error(f"KNNäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            raise UnifiedModelError(f"äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def predict_proba(self, X) -> np.ndarray:
        """
        sklearnäº’æ›ã®äºˆæ¸¬ç¢ºç‡ãƒ¡ã‚½ãƒƒãƒ‰

        Args:
            X: ç‰¹å¾´é‡ï¼ˆDataFrame or numpy arrayï¼‰

        Returns:
            äºˆæ¸¬ç¢ºç‡ã®é…åˆ—
        """
        if not self.is_trained or self.model is None:
            raise UnifiedModelError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # numpyé…åˆ—ã‚’DataFrameã«å¤‰æ›
            if not isinstance(X, pd.DataFrame):
                if hasattr(self, "feature_columns") and self.feature_columns:
                    X = pd.DataFrame(X, columns=self.feature_columns)
                else:
                    X = pd.DataFrame(
                        X, columns=[f"feature_{i}" for i in range(X.shape[1])]
                    )

            # ç‰¹å¾´é‡ã®é †åºã‚’ç¢ºèª
            if self.feature_columns:
                X = X[self.feature_columns]

            probabilities = self.model.predict_proba(X)
            return probabilities

        except Exception as e:
            logger.error(f"KNNç¢ºç‡äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            raise UnifiedModelError(f"ç¢ºç‡äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def get_params(self, deep: bool = True) -> Dict[str, Any]:
        """
        sklearnäº’æ›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—ãƒ¡ã‚½ãƒƒãƒ‰

        Args:
            deep: æ·±ã„ã‚³ãƒ”ãƒ¼ã‚’è¡Œã†ã‹ã©ã†ã‹ï¼ˆæœªä½¿ç”¨ï¼‰

        Returns:
            ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        _ = deep  # æœªä½¿ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {
            "automl_config": self.automl_config,
            "n_neighbors": self.n_neighbors,
            "weights": self.weights,
            "algorithm": self.algorithm,
            "metric": self.metric,
            "p": self.p,
            "n_jobs": self.n_jobs,
        }

        # å‹•çš„ã«è¿½åŠ ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
        for attr_name in dir(self):
            if (
                not attr_name.startswith("_")
                and attr_name not in params
                and attr_name
                not in [
                    "model",
                    "is_trained",
                    "feature_columns",
                    "classes_",
                    "default_params",
                    "fit",
                    "predict",
                    "predict_proba",
                    "get_params",
                    "set_params",
                ]
            ):
                try:
                    attr_value = getattr(self, attr_name)
                    if not callable(attr_value):
                        params[attr_name] = attr_value
                except Exception:
                    # å±æ€§å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ç„¡è¦–
                    pass

        return params

    def set_params(self, **params) -> "KNNModel":
        """
        sklearnäº’æ›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šãƒ¡ã‚½ãƒƒãƒ‰

        Args:
            **params: è¨­å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            self: è¨­å®šå¾Œã®ãƒ¢ãƒ‡ãƒ«
        """
        for param, value in params.items():
            if hasattr(self, param):
                setattr(self, param, value)
                # default_paramsã‚‚æ›´æ–°
                if param in self.default_params:
                    self.default_params[param] = value
            else:
                logger.warning(f"æœªçŸ¥ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {param}")
        return self

    @property
    def feature_columns(self) -> List[str]:
        """ç‰¹å¾´é‡ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self._feature_columns

    @feature_columns.setter
    def feature_columns(self, columns: List[str]):
        """ç‰¹å¾´é‡ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆã‚’è¨­å®š"""
        self._feature_columns = columns

    def get_feature_importance(self) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ï¼ˆè·é›¢ãƒ™ãƒ¼ã‚¹ã®é‡è¦åº¦è¨ˆç®—ï¼‰

        Returns:
            ç‰¹å¾´é‡é‡è¦åº¦ã®è¾æ›¸
        """
        if not self.is_trained or self.model is None:
            raise UnifiedModelError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        if not self.feature_columns:
            raise UnifiedModelError("ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è·é›¢ãƒ™ãƒ¼ã‚¹ã®é‡è¦åº¦ã‚’è¨ˆç®—
        if hasattr(self.model, "_fit_X"):
            X_train = pd.DataFrame(self.model._fit_X, columns=self.feature_columns)

            # å„ç‰¹å¾´é‡ã®åˆ†æ•£ã‚’é‡è¦åº¦ã¨ã—ã¦ä½¿ç”¨ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            feature_variance = X_train.var()

            # ç‰¹å¾´é‡é–“ã®ç›¸é–¢ã‚’è€ƒæ…®ã—ãŸé‡è¦åº¦è¨ˆç®—
            try:
                # å„ç‰¹å¾´é‡ã®è¿‘å‚è·é›¢ã¸ã®å¯„ä¸åº¦ã‚’è¨ˆç®—
                nn = NearestNeighbors(
                    n_neighbors=min(5, len(X_train)), metric=self.metric, p=self.p
                )
                nn.fit(X_train)
                distances, _ = nn.kneighbors(X_train)

                # å„ç‰¹å¾´é‡ã®è·é›¢ã¸ã®å¯„ä¸åº¦ã‚’è¨ˆç®—
                feature_importance = {}
                for i, col in enumerate(self.feature_columns):
                    # ç‰¹å¾´é‡ã‚’é™¤ã„ãŸå ´åˆã®è·é›¢å¤‰åŒ–ã‚’è¨ˆç®—
                    X_without_feature = X_train.drop(columns=[col])
                    if X_without_feature.shape[1] > 0:
                        nn_without = NearestNeighbors(
                            n_neighbors=min(5, len(X_train)),
                            metric=self.metric,
                            p=self.p,
                        )
                        nn_without.fit(X_without_feature)
                        distances_without, _ = nn_without.kneighbors(X_without_feature)

                        # è·é›¢ã®å¤‰åŒ–ã‚’é‡è¦åº¦ã¨ã—ã¦ä½¿ç”¨
                        importance = np.mean(
                            np.abs(
                                distances.mean(axis=1) - distances_without.mean(axis=1)
                            )
                        )
                        feature_importance[col] = importance
                    else:
                        feature_importance[col] = feature_variance.iloc[i]

                # æ­£è¦åŒ–
                total_importance = sum(feature_importance.values())
                if total_importance > 0:
                    return {
                        k: v / total_importance for k, v in feature_importance.items()
                    }

            except Exception as e:
                logger.warning(f"è·é›¢ãƒ™ãƒ¼ã‚¹é‡è¦åº¦è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåˆ†æ•£ãƒ™ãƒ¼ã‚¹ã®é‡è¦åº¦
                total_variance = feature_variance.sum()
                if total_variance > 0:
                    return dict(
                        zip(self.feature_columns, feature_variance / total_variance)
                    )

        return {}

    def get_model_info(self) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—

        Returns:
            ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¾æ›¸
        """
        if not self.is_trained or self.model is None:
            return {"status": "not_trained"}

        return {
            "algorithm": self.ALGORITHM_NAME,
            "n_neighbors": self.model.n_neighbors,
            "weights": self.model.weights,
            "algorithm_type": self.model.algorithm,
            "metric": self.model.metric,
            "p": self.model.p,
            "feature_count": len(self.feature_columns) if self.feature_columns else 0,
            "status": "trained",
        }
