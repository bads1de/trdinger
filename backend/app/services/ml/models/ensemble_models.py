"""
ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ¢ãƒ‡ãƒ«

è¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ„ã¿åˆã‚ã›ã¦äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚
RandomForestã®40.55%ã‹ã‚‰XGBoost+ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§55%ä»¥ä¸Šã‚’ç›®æŒ‡ã—ã¾ã™ã€‚
"""

import logging
import warnings
from typing import Any, Dict, Optional

import lightgbm as lgb
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.ensemble import (
    AdaBoostClassifier,
    ExtraTreesClassifier,
    GradientBoostingClassifier,
    RandomForestClassifier,
    StackingClassifier,
    VotingClassifier,
)
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

logger = logging.getLogger(__name__)


class EnsembleModelManager:
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.models = {}
        self.ensemble_model = None
        self.feature_importance = None

    def create_base_models(self) -> Dict[str, Any]:
        """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
        logger.info("ğŸ¤– ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...")

        models = {}

        # 1. Random Forestï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        models["rf"] = RandomForestClassifier(
            n_estimators=200,
            max_depth=12,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features="sqrt",
            class_weight="balanced",
            random_state=42,
            n_jobs=-1,
        )

        # 2. XGBoost
        models["xgb"] = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            eval_metric="mlogloss",
            use_label_encoder=False,
        )

        # 3. LightGBM
        models["lgb"] = lgb.LGBMClassifier(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            class_weight="balanced",
            random_state=42,
            verbose=-1,
        )

        # 4. Logistic Regression
        models["lr"] = LogisticRegression(
            C=1.0, class_weight="balanced", random_state=42, max_iter=1000
        )

        # 5. SVMï¼ˆå°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ï¼‰
        models["svm"] = SVC(
            C=1.0,
            kernel="rbf",
            class_weight="balanced",
            probability=True,
            random_state=42,
        )

        # 6. Extra Treesï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ€§å¼·åŒ–ï¼‰
        models["extra_trees"] = ExtraTreesClassifier(
            n_estimators=200,
            max_depth=12,
            min_samples_split=5,
            min_samples_leaf=2,
            class_weight="balanced",
            random_state=42,
            n_jobs=-1,
        )

        # 7. Gradient Boosting
        models["gb"] = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            random_state=42,
        )

        # 8. AdaBoost
        models["ada"] = AdaBoostClassifier(
            estimator=DecisionTreeClassifier(max_depth=3, class_weight="balanced"),
            n_estimators=100,
            learning_rate=1.0,
            random_state=42,
        )

        # 9. Ridge Classifierï¼ˆç·šå½¢ï¼‰
        models["ridge"] = RidgeClassifier(
            alpha=1.0, class_weight="balanced", random_state=42
        )

        # 10. Naive Bayes
        models["nb"] = GaussianNB()

        # 11. K-Nearest Neighbors
        models["knn"] = KNeighborsClassifier(
            n_neighbors=5, weights="distance", metric="minkowski"
        )

        logger.info(f"âœ… {len(models)}å€‹ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ")
        return models

    def create_voting_ensemble(self, base_models: Dict[str, Any]) -> VotingClassifier:
        """æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ä½œæˆ"""
        logger.info("ğŸ—³ï¸ æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ä½œæˆä¸­...")

        # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        estimators = [(name, model) for name, model in base_models.items()]

        # ã‚½ãƒ•ãƒˆæŠ•ç¥¨ï¼ˆç¢ºç‡ãƒ™ãƒ¼ã‚¹ï¼‰
        voting_ensemble = VotingClassifier(estimators=estimators, voting="soft")

        return voting_ensemble

    def create_stacking_ensemble(
        self, base_models: Dict[str, Any]
    ) -> StackingClassifier:
        """ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ä½œæˆ"""
        logger.info("ğŸ“š ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ä½œæˆä¸­...")

        # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        estimators = [(name, model) for name, model in base_models.items()]

        # ãƒ¡ã‚¿å­¦ç¿’å™¨ï¼ˆLogistic Regressionï¼‰
        meta_learner = LogisticRegression(
            C=1.0, class_weight="balanced", random_state=42
        )

        # ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨
        stacking_ensemble = StackingClassifier(
            estimators=estimators,
            final_estimator=meta_learner,
            cv=3,  # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãªã®ã§å°‘ãªã‚
            stack_method="predict_proba",
        )

        return stacking_ensemble

    def train_and_evaluate_models(
        self,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
    ) -> Dict[str, Dict[str, float]]:
        """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ»è©•ä¾¡"""
        logger.info("ğŸ‹ï¸ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»è©•ä¾¡ã‚’é–‹å§‹...")

        results = {}

        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        base_models = self.create_base_models()

        # 1. å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
        for name, model in base_models.items():
            logger.info(f"ğŸ“Š {name}ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")

            try:
                # å­¦ç¿’
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    model.fit(X_train, y_train)

                # äºˆæ¸¬
                y_pred = model.predict(X_test)
                # è©•ä¾¡
                results[name] = {
                    "accuracy": accuracy_score(y_test, y_pred),
                    "balanced_accuracy": balanced_accuracy_score(y_test, y_pred),
                    "f1_score": f1_score(y_test, y_pred, average="weighted"),
                }

                logger.info(
                    f"  {name}: ç²¾åº¦={results[name]['accuracy']:.4f}, "
                    f"ãƒãƒ©ãƒ³ã‚¹ç²¾åº¦={results[name]['balanced_accuracy']:.4f}"
                )

            except Exception as e:
                logger.error(f"{name}ãƒ¢ãƒ‡ãƒ«ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # 2. æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®è©•ä¾¡
        try:
            logger.info("ğŸ—³ï¸ æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’å­¦ç¿’ä¸­...")
            voting_ensemble = self.create_voting_ensemble(base_models)

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                voting_ensemble.fit(X_train, y_train)

            y_pred_voting = voting_ensemble.predict(X_test)

            results["voting_ensemble"] = {
                "accuracy": accuracy_score(y_test, y_pred_voting),
                "balanced_accuracy": balanced_accuracy_score(y_test, y_pred_voting),
                "f1_score": f1_score(y_test, y_pred_voting, average="weighted"),
            }

            logger.info(
                f"  æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: ç²¾åº¦={results['voting_ensemble']['accuracy']:.4f}"
            )

        except Exception as e:
            logger.error(f"æŠ•ç¥¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ã‚¨ãƒ©ãƒ¼: {e}")

        # 3. ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®è©•ä¾¡
        try:
            logger.info("ğŸ“š ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’å­¦ç¿’ä¸­...")
            stacking_ensemble = self.create_stacking_ensemble(base_models)

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                stacking_ensemble.fit(X_train, y_train)

            y_pred_stacking = stacking_ensemble.predict(X_test)

            results["stacking_ensemble"] = {
                "accuracy": accuracy_score(y_test, y_pred_stacking),
                "balanced_accuracy": balanced_accuracy_score(y_test, y_pred_stacking),
                "f1_score": f1_score(y_test, y_pred_stacking, average="weighted"),
            }

            logger.info(
                f"  ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: ç²¾åº¦={results['stacking_ensemble']['accuracy']:.4f}"
            )

            # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜
            self.ensemble_model = stacking_ensemble

        except Exception as e:
            logger.error(f"ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ã‚¨ãƒ©ãƒ¼: {e}")

        return results

    def get_feature_importance(self, X: pd.DataFrame) -> Optional[pd.Series]:
        """ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—"""
        if self.ensemble_model is None:
            return None

        try:
            # ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®å ´åˆã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é‡è¦åº¦ã‚’å¹³å‡
            if hasattr(self.ensemble_model, "estimators_"):
                importances = []

                for name, model in self.ensemble_model.estimators_:
                    if hasattr(model, "feature_importances_"):
                        importances.append(model.feature_importances_)
                    elif hasattr(model, "coef_"):
                        # ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ä¿‚æ•°ã®çµ¶å¯¾å€¤
                        importances.append(np.abs(model.coef_[0]))

                if importances:
                    avg_importance = np.mean(importances, axis=0)
                    return pd.Series(avg_importance, index=X.columns).sort_values(
                        ascending=False
                    )

        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        return None

    def optimize_hyperparameters(
        self, X_train: pd.DataFrame, y_train: pd.Series, model_type: str = "xgb"
    ) -> Dict[str, Any]:
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        logger.info(f"ğŸ”§ {model_type}ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä¸­...")

        if model_type == "xgb":
            # XGBoostã®ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆç°¡æ˜“ç‰ˆï¼‰
            param_grid = {
                "n_estimators": [100, 200],
                "max_depth": [6, 8, 10],
                "learning_rate": [0.05, 0.1, 0.15],
            }

            best_score = 0
            best_params = {}

            for n_est in param_grid["n_estimators"]:
                for depth in param_grid["max_depth"]:
                    for lr in param_grid["learning_rate"]:
                        model = xgb.XGBClassifier(
                            n_estimators=n_est,
                            max_depth=depth,
                            learning_rate=lr,
                            random_state=42,
                            eval_metric="mlogloss",
                            use_label_encoder=False,
                        )

                        # 3-fold CV
                        scores = cross_val_score(
                            model, X_train, y_train, cv=3, scoring="balanced_accuracy"
                        )
                        avg_score = scores.mean()

                        if avg_score > best_score:
                            best_score = avg_score
                            best_params = {
                                "n_estimators": n_est,
                                "max_depth": depth,
                                "learning_rate": lr,
                            }

            logger.info(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}, ã‚¹ã‚³ã‚¢: {best_score:.4f}")
            return best_params

        return {}

    def create_optimized_ensemble(
        self, X_train: pd.DataFrame, y_train: pd.Series, optimize: bool = True
    ) -> Any:
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
        logger.info("ğŸš€ æœ€é©åŒ–ã•ã‚ŒãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...")

        models = {}

        # 1. æœ€é©åŒ–ã•ã‚ŒãŸXGBoost
        if optimize:
            best_params = self.optimize_hyperparameters(X_train, y_train, "xgb")
            models["xgb_optimized"] = xgb.XGBClassifier(
                **best_params,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                random_state=42,
                eval_metric="mlogloss",
                use_label_encoder=False,
            )
        else:
            models["xgb"] = xgb.XGBClassifier(
                n_estimators=200,
                max_depth=8,
                learning_rate=0.1,
                random_state=42,
                eval_metric="mlogloss",
                use_label_encoder=False,
            )

        # 2. æ”¹è‰¯ã•ã‚ŒãŸRandom Forest
        models["rf_improved"] = RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=3,
            min_samples_leaf=1,
            max_features="sqrt",
            class_weight="balanced",
            random_state=42,
            n_jobs=-1,
        )

        # 3. LightGBM
        models["lgb"] = lgb.LGBMClassifier(
            n_estimators=200,
            max_depth=10,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            class_weight="balanced",
            random_state=42,
            verbose=-1,
        )

        # ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        estimators = [(name, model) for name, model in models.items()]

        meta_learner = LogisticRegression(
            C=1.0, class_weight="balanced", random_state=42, max_iter=1000
        )

        optimized_ensemble = StackingClassifier(
            estimators=estimators,
            final_estimator=meta_learner,
            cv=3,
            stack_method="predict_proba",
        )

        logger.info(f"âœ… æœ€é©åŒ–ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆå®Œäº†: {len(models)}å€‹ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«")

        return optimized_ensemble


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
ensemble_manager = EnsembleModelManager()
