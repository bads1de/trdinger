"""
Êã°Âºµ„É¢„Éá„É´ÁÆ°ÁêÜ„Ç∑„Çπ„ÉÜ„É†

ÂàÜÊûêÂ†±ÂëäÊõ∏„ÅßÊèêÊ°à„Åï„Çå„Åü„É¢„Éá„É´ÁÆ°ÁêÜ„Ç∑„Çπ„ÉÜ„É†„ÅÆÊîπÂñÑ„ÇíÂÆüË£Ö„ÄÇ
„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ„ÄÅ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁõ£Ë¶ñ„ÄÅËá™Âãï„Éá„Éó„É≠„Ç§„É°„É≥„Éà„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ
"""

import hashlib
import json
import logging
import pickle
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd

logger = logging.getLogger(__name__)


class ModelStatus(Enum):
    """„É¢„Éá„É´„Çπ„ÉÜ„Éº„Çø„Çπ"""

    TRAINING = "training"
    TRAINED = "trained"
    VALIDATED = "validated"
    DEPLOYED = "deployed"
    DEPRECATED = "deprecated"
    FAILED = "failed"


class PerformanceMetric(Enum):
    """„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô"""

    ACCURACY = "accuracy"
    BALANCED_ACCURACY = "balanced_accuracy"
    F1_SCORE = "f1_score"
    ROC_AUC = "roc_auc"
    PR_AUC = "pr_auc"
    PRECISION = "precision"
    RECALL = "recall"


@dataclass
class ModelMetadata:
    """„É¢„Éá„É´„É°„Çø„Éá„Éº„Çø"""

    model_id: str
    version: str
    name: str
    algorithm: str
    created_at: datetime
    status: ModelStatus

    # Â≠¶ÁøíÊÉÖÂ†±
    training_data_hash: str
    feature_count: int
    sample_count: int
    training_duration: float

    # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô
    performance_metrics: Dict[str, float]
    validation_metrics: Dict[str, float]

    # Ë®≠ÂÆöÊÉÖÂ†±
    hyperparameters: Dict[str, Any]
    feature_selection_config: Dict[str, Any]
    preprocessing_config: Dict[str, Any]

    # „Éï„Ç°„Ç§„É´„Éë„Çπ
    model_path: str
    metadata_path: str

    # „Åù„ÅÆ‰ªñ
    tags: List[str]
    description: str
    author: str


@dataclass
class PerformanceMonitoringConfig:
    """„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁõ£Ë¶ñË®≠ÂÆö"""

    enable_monitoring: bool = True
    alert_threshold: float = 0.05  # „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ‰Ωé‰∏ã„ÅÆÈñæÂÄ§
    monitoring_window: int = 100  # Áõ£Ë¶ñ„Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫
    auto_retrain_threshold: float = 0.10  # Ëá™ÂãïÂÜçÂ≠¶Áøí„ÅÆÈñæÂÄ§
    max_performance_history: int = 1000


class EnhancedModelManager:
    """
    Êã°Âºµ„É¢„Éá„É´ÁÆ°ÁêÜ„Ç∑„Çπ„ÉÜ„É†

    „É¢„Éá„É´„ÅÆ„É©„Ç§„Éï„Çµ„Ç§„ÇØ„É´ÂÖ®‰Ωì„ÇíÁÆ°ÁêÜ„Åó„ÄÅ
    „Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ„Å®„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁõ£Ë¶ñ„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ
    """

    def __init__(
        self,
        base_path: str = "models",
        monitoring_config: PerformanceMonitoringConfig = None,
    ):
        """
        ÂàùÊúüÂåñ

        Args:
            base_path: „É¢„Éá„É´‰øùÂ≠ò„Éô„Éº„Çπ„Éë„Çπ
            monitoring_config: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁõ£Ë¶ñË®≠ÂÆö
        """
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)

        self.monitoring_config = monitoring_config or PerformanceMonitoringConfig()

        # „É°„Çø„Éá„Éº„Çø„Çπ„Éà„É¨„Éº„Ç∏
        self.metadata_file = self.base_path / "model_registry.json"
        self.performance_history_file = self.base_path / "performance_history.json"

        # „É¢„Éá„É´„É¨„Ç∏„Çπ„Éà„É™„ÅÆÂàùÊúüÂåñ
        self.model_registry = self._load_model_registry()
        self.performance_history = self._load_performance_history()

    def register_model(
        self,
        model: Any,
        model_name: str,
        algorithm: str,
        training_data: pd.DataFrame,
        performance_metrics: Dict[str, float],
        validation_metrics: Dict[str, float] = None,
        hyperparameters: Dict[str, Any] = None,
        feature_selection_config: Dict[str, Any] = None,
        preprocessing_config: Dict[str, Any] = None,
        tags: List[str] = None,
        description: str = "",
        author: str = "system",
    ) -> str:
        """
        „É¢„Éá„É´„ÇíÁôªÈå≤

        Args:
            model: Â≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´
            model_name: „É¢„Éá„É´Âêç
            algorithm: „Ç¢„É´„Ç¥„É™„Ç∫„É†Âêç
            training_data: Â≠¶Áøí„Éá„Éº„Çø
            performance_metrics: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô
            validation_metrics: Ê§úË®ºÊåáÊ®ô
            hyperparameters: „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø
            feature_selection_config: ÁâπÂæ¥ÈáèÈÅ∏ÊäûË®≠ÂÆö
            preprocessing_config: ÂâçÂá¶ÁêÜË®≠ÂÆö
            tags: „Çø„Ç∞
            description: Ë™¨Êòé
            author: ‰ΩúÊàêËÄÖ

        Returns:
            „É¢„Éá„É´ID
        """
        logger.info(f"üîÑ „É¢„Éá„É´ÁôªÈå≤ÈñãÂßã: {model_name}")

        # „É¢„Éá„É´ID„Å®„Éê„Éº„Ç∏„Éß„É≥„ÇíÁîüÊàê
        model_id = self._generate_model_id(model_name, algorithm)
        version = self._generate_version(model_id)

        # „Éá„Éº„Çø„Éè„ÉÉ„Ç∑„É•„ÇíË®àÁÆó
        training_data_hash = self._calculate_data_hash(training_data)

        # „É¢„Éá„É´„Éï„Ç°„Ç§„É´„Éë„Çπ„ÇíÁîüÊàê
        model_dir = self.base_path / model_id / version
        model_dir.mkdir(parents=True, exist_ok=True)

        model_path = model_dir / "model.pkl"
        metadata_path = model_dir / "metadata.json"

        # „É¢„Éá„É´„Çí‰øùÂ≠ò
        try:
            with open(model_path, "wb") as f:
                pickle.dump(model, f)
            logger.info(f"„É¢„Éá„É´„Éï„Ç°„Ç§„É´‰øùÂ≠ò: {model_path}")
        except Exception as e:
            logger.error(f"„É¢„Éá„É´‰øùÂ≠ò„Ç®„É©„Éº: {e}")
            raise

        # „É°„Çø„Éá„Éº„Çø„Çí‰ΩúÊàê
        metadata = ModelMetadata(
            model_id=model_id,
            version=version,
            name=model_name,
            algorithm=algorithm,
            created_at=datetime.now(timezone.utc),
            status=ModelStatus.TRAINED,
            training_data_hash=training_data_hash,
            feature_count=training_data.shape[1],
            sample_count=training_data.shape[0],
            training_duration=0.0,  # ÂÆüÈöõ„ÅÆÂ≠¶ÁøíÊôÇÈñì„ÅØÂ§ñÈÉ®„Åã„ÇâË®≠ÂÆö
            performance_metrics=performance_metrics,
            validation_metrics=validation_metrics or {},
            hyperparameters=hyperparameters or {},
            feature_selection_config=feature_selection_config or {},
            preprocessing_config=preprocessing_config or {},
            model_path=str(model_path),
            metadata_path=str(metadata_path),
            tags=tags or [],
            description=description,
            author=author,
        )

        # „É°„Çø„Éá„Éº„Çø„Çí‰øùÂ≠ò
        self._save_metadata(metadata)

        # „É¨„Ç∏„Çπ„Éà„É™„Å´ËøΩÂä†
        self.model_registry[f"{model_id}:{version}"] = asdict(metadata)
        self._save_model_registry()

        logger.info(f"‚úÖ „É¢„Éá„É´ÁôªÈå≤ÂÆå‰∫Ü: {model_id}:{version}")
        return f"{model_id}:{version}"

    def load_model(self, model_key: str) -> Tuple[Any, ModelMetadata]:
        """
        „É¢„Éá„É´„Çí„É≠„Éº„Éâ

        Args:
            model_key: „É¢„Éá„É´„Ç≠„Éº (model_id:version)

        Returns:
            „É¢„Éá„É´„Å®„É°„Çø„Éá„Éº„Çø„ÅÆ„Çø„Éó„É´
        """
        if model_key not in self.model_registry:
            raise ValueError(f"„É¢„Éá„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {model_key}")

        metadata_dict = self.model_registry[model_key]
        metadata = ModelMetadata(**metadata_dict)

        try:
            with open(metadata.model_path, "rb") as f:
                model = pickle.load(f)

            logger.info(f"„É¢„Éá„É´„É≠„Éº„ÉâÂÆå‰∫Ü: {model_key}")
            return model, metadata

        except Exception as e:
            logger.error(f"„É¢„Éá„É´„É≠„Éº„Éâ„Ç®„É©„Éº: {e}")
            raise

    def get_best_model(
        self,
        metric: PerformanceMetric = PerformanceMetric.BALANCED_ACCURACY,
        algorithm: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -> Optional[Tuple[str, ModelMetadata]]:
        """
        ÊúÄÈ´òÊÄßËÉΩ„ÅÆ„É¢„Éá„É´„ÇíÂèñÂæó

        Args:
            metric: Ë©ï‰æ°ÊåáÊ®ô
            algorithm: „Ç¢„É´„Ç¥„É™„Ç∫„É†Âêç„Åß„Éï„Ç£„É´„Çø
            tags: „Çø„Ç∞„Åß„Éï„Ç£„É´„Çø

        Returns:
            „É¢„Éá„É´„Ç≠„Éº„Å®„É°„Çø„Éá„Éº„Çø„ÅÆ„Çø„Éó„É´
        """
        candidates = []

        for model_key, metadata_dict in self.model_registry.items():
            metadata = ModelMetadata(**metadata_dict)

            # „Éï„Ç£„É´„ÇøÊù°‰ª∂„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            if algorithm and metadata.algorithm != algorithm:
                continue

            if tags and not any(tag in metadata.tags for tag in tags):
                continue

            if metadata.status not in [
                ModelStatus.TRAINED,
                ModelStatus.VALIDATED,
                ModelStatus.DEPLOYED,
            ]:
                continue

            # ÊåáÊ®ôÂÄ§„ÇíÂèñÂæó
            metric_value = metadata.performance_metrics.get(metric.value, 0.0)
            candidates.append((model_key, metadata, metric_value))

        if not candidates:
            return None

        # ÊúÄÈ´ò„Çπ„Ç≥„Ç¢„ÅÆ„É¢„Éá„É´„ÇíÈÅ∏Êäû
        best_model_key, best_metadata, _ = max(candidates, key=lambda x: x[2])
        return best_model_key, best_metadata

    def update_model_status(self, model_key: str, status: ModelStatus):
        """„É¢„Éá„É´„Çπ„ÉÜ„Éº„Çø„Çπ„ÇíÊõ¥Êñ∞"""
        if model_key not in self.model_registry:
            raise ValueError(f"„É¢„Éá„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {model_key}")

        self.model_registry[model_key]["status"] = status.value
        self._save_model_registry()

        logger.info(f"„É¢„Éá„É´„Çπ„ÉÜ„Éº„Çø„ÇπÊõ¥Êñ∞: {model_key} -> {status.value}")

    def record_performance(
        self,
        model_key: str,
        metrics: Dict[str, float],
        timestamp: Optional[datetime] = None,
    ):
        """
        „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂ±•Ê≠¥„ÇíË®òÈå≤

        Args:
            model_key: „É¢„Éá„É´„Ç≠„Éº
            metrics: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊåáÊ®ô
            timestamp: „Çø„Ç§„É†„Çπ„Çø„É≥„Éó
        """
        if not self.monitoring_config.enable_monitoring:
            return

        timestamp = timestamp or datetime.now(timezone.utc)

        if model_key not in self.performance_history:
            self.performance_history[model_key] = []

        # Â±•Ê≠¥„Å´ËøΩÂä†
        record = {"timestamp": timestamp.isoformat(), "metrics": metrics}

        self.performance_history[model_key].append(record)

        # Â±•Ê≠¥„Çµ„Ç§„Ç∫Âà∂Èôê
        max_history = self.monitoring_config.max_performance_history
        if len(self.performance_history[model_key]) > max_history:
            self.performance_history[model_key] = self.performance_history[model_key][
                -max_history:
            ]

        self._save_performance_history()

        # „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ‰Ωé‰∏ã„ÅÆÊ§úÂá∫
        self._check_performance_degradation(model_key, metrics)

    def _check_performance_degradation(
        self, model_key: str, current_metrics: Dict[str, float]
    ):
        """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ‰Ωé‰∏ã„ÇíÊ§úÂá∫"""
        if model_key not in self.model_registry:
            return

        baseline_metrics = self.model_registry[model_key]["performance_metrics"]

        for metric_name, current_value in current_metrics.items():
            if metric_name in baseline_metrics:
                baseline_value = baseline_metrics[metric_name]
                degradation = baseline_value - current_value

                if degradation > self.monitoring_config.alert_threshold:
                    logger.warning(
                        f"‚ö†Ô∏è „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ‰Ωé‰∏ãÊ§úÂá∫: {model_key} "
                        f"{metric_name}: {baseline_value:.4f} -> {current_value:.4f} "
                        f"(‰Ωé‰∏ã: {degradation:.4f})"
                    )

                    # Ëá™ÂãïÂÜçÂ≠¶Áøí„ÅÆÈñæÂÄ§„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                    if degradation > self.monitoring_config.auto_retrain_threshold:
                        logger.warning(f"üîÑ Ëá™ÂãïÂÜçÂ≠¶Áøí„ÅåÊé®Â•®„Åï„Çå„Åæ„Åô: {model_key}")

    def get_model_list(
        self, status: Optional[ModelStatus] = None, algorithm: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """„É¢„Éá„É´‰∏ÄË¶ß„ÇíÂèñÂæó"""
        models = []

        for model_key, metadata_dict in self.model_registry.items():
            if status and metadata_dict["status"] != status.value:
                continue

            if algorithm and metadata_dict["algorithm"] != algorithm:
                continue

            models.append({"model_key": model_key, **metadata_dict})

        # ‰ΩúÊàêÊó•ÊôÇ„Åß„ÇΩ„Éº„Éà
        models.sort(key=lambda x: x["created_at"], reverse=True)
        return models

    def delete_model(self, model_key: str):
        """„É¢„Éá„É´„ÇíÂâäÈô§"""
        if model_key not in self.model_registry:
            raise ValueError(f"„É¢„Éá„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {model_key}")

        metadata_dict = self.model_registry[model_key]

        # „Éï„Ç°„Ç§„É´„ÇíÂâäÈô§
        try:
            model_path = Path(metadata_dict["model_path"])
            if model_path.exists():
                model_path.unlink()

            metadata_path = Path(metadata_dict["metadata_path"])
            if metadata_path.exists():
                metadata_path.unlink()

            # „Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÁ©∫„Å™„ÇâÂâäÈô§
            model_dir = model_path.parent
            if model_dir.exists() and not any(model_dir.iterdir()):
                model_dir.rmdir()

        except Exception as e:
            logger.warning(f"„Éï„Ç°„Ç§„É´ÂâäÈô§„Ç®„É©„Éº: {e}")

        # „É¨„Ç∏„Çπ„Éà„É™„Åã„ÇâÂâäÈô§
        del self.model_registry[model_key]

        # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂ±•Ê≠¥„ÇÇÂâäÈô§
        if model_key in self.performance_history:
            del self.performance_history[model_key]

        self._save_model_registry()
        self._save_performance_history()

        logger.info(f"„É¢„Éá„É´ÂâäÈô§ÂÆå‰∫Ü: {model_key}")

    def _generate_model_id(self, model_name: str, algorithm: str) -> str:
        """„É¢„Éá„É´ID„ÇíÁîüÊàê"""
        base_id = f"{model_name}_{algorithm}"
        # ÁâπÊÆäÊñáÂ≠ó„ÇíÈô§Âéª
        model_id = "".join(c for c in base_id if c.isalnum() or c in "_-")
        return model_id

    def _generate_version(self, model_id: str) -> str:
        """„Éê„Éº„Ç∏„Éß„É≥„ÇíÁîüÊàê"""
        existing_versions = [
            key.split(":")[1]
            for key in self.model_registry.keys()
            if key.startswith(f"{model_id}:")
        ]

        if not existing_versions:
            return "v1.0.0"

        # ÊúÄÊñ∞„Éê„Éº„Ç∏„Éß„É≥„ÇíÂèñÂæó„Åó„Å¶„Ç§„É≥„ÇØ„É™„É°„É≥„Éà
        latest_version = max(existing_versions)
        try:
            major, minor, patch = map(int, latest_version[1:].split("."))
            return f"v{major}.{minor}.{patch + 1}"
        except ValueError:
            # ÊúüÂæÖ„Åô„Çã„Éï„Ç©„Éº„Éû„ÉÉ„Éà 'vX.Y.Z' „ÇíÊ∫Ä„Åü„Åï„Å™„ÅÑÂ†¥Âêà„ÅØÂæåÊñπ‰∫íÊèõ„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
            return f"v1.0.{len(existing_versions)}"

    def _calculate_data_hash(self, data: pd.DataFrame) -> str:
        """„Éá„Éº„Çø„Éè„ÉÉ„Ç∑„É•„ÇíË®àÁÆó"""
        try:
            data_str = data.to_string()
            return hashlib.md5(data_str.encode()).hexdigest()
        except Exception:
            # DataFrame „Åß„Å™„ÅÑ/„Ç∑„É™„Ç¢„É©„Ç§„Ç∫Â§±Êïó„Å™„Å©„ÅÆÂ†¥Âêà„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
            return "unknown"

    def _save_metadata(self, metadata: ModelMetadata):
        """„É°„Çø„Éá„Éº„Çø„Çí‰øùÂ≠ò"""
        try:
            with open(metadata.metadata_path, "w", encoding="utf-8") as f:
                json.dump(
                    asdict(metadata), f, indent=2, default=str, ensure_ascii=False
                )
        except Exception as e:
            logger.error(f"„É°„Çø„Éá„Éº„Çø‰øùÂ≠ò„Ç®„É©„Éº: {e}")

    def _load_model_registry(self) -> Dict[str, Any]:
        """„É¢„Éá„É´„É¨„Ç∏„Çπ„Éà„É™„Çí„É≠„Éº„Éâ"""
        try:
            if self.metadata_file.exists():
                with open(self.metadata_file, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"„É¨„Ç∏„Çπ„Éà„É™„É≠„Éº„Éâ„Ç®„É©„Éº: {e}")

        return {}

    def _save_model_registry(self):
        """„É¢„Éá„É´„É¨„Ç∏„Çπ„Éà„É™„Çí‰øùÂ≠ò"""
        try:
            with open(self.metadata_file, "w", encoding="utf-8") as f:
                json.dump(
                    self.model_registry, f, indent=2, default=str, ensure_ascii=False
                )
        except Exception as e:
            logger.error(f"„É¨„Ç∏„Çπ„Éà„É™‰øùÂ≠ò„Ç®„É©„Éº: {e}")

    def _load_performance_history(self) -> Dict[str, List[Dict]]:
        """„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂ±•Ê≠¥„Çí„É≠„Éº„Éâ"""
        try:
            if self.performance_history_file.exists():
                with open(self.performance_history_file, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"Â±•Ê≠¥„É≠„Éº„Éâ„Ç®„É©„Éº: {e}")

        return {}

    def _save_performance_history(self):
        """„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂ±•Ê≠¥„Çí‰øùÂ≠ò"""
        try:
            with open(self.performance_history_file, "w", encoding="utf-8") as f:
                json.dump(
                    self.performance_history,
                    f,
                    indent=2,
                    default=str,
                    ensure_ascii=False,
                )
        except Exception as e:
            logger.error(f"Â±•Ê≠¥‰øùÂ≠ò„Ç®„É©„Éº: {e}")


# „Ç∞„É≠„Éº„Éê„É´„Ç§„É≥„Çπ„Çø„É≥„Çπ
enhanced_model_manager = EnhancedModelManager()
